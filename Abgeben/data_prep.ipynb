{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66adbb1d-937b-4098-9e0e-9e4fba70c410",
   "metadata": {},
   "source": [
    "# Aufbereitung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772e326-d818-4530-b12e-50bcb78e42bc",
   "metadata": {},
   "source": [
    "Damit die Spektogramme überhaupt von Modellen genutzt werden können, müssen sie aufbereitet, (teilweise) bearbeitet und in einem gut verwendbarem Format abgespeichert werden.\n",
    "In unserem Fall besteht das aus den folgenden Schritten:\n",
    "1. Die Skala/Rahmen der Bilder entfernen\n",
    "2. Die Bilder runterskalieren\n",
    "3. (Optional) Filter oder andere Bildbearbeitungen anwenden\n",
    "4. Daten nach Klasse aussortieren\n",
    "5. Fledermausarten in Numerische Klassen umwandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699a1d3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375aaf9-6fe1-46b0-85ca-8573ed88db4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours as ENN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9453b12b-8514-4b45-944f-96d692be6755",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dateneinlesen, ggf. bearbeiten und abspeichern"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba1093-7cef-4b8f-ab34-3ddf7af31a92",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Funktion zum entfernen eines übergebenen Randes.\n",
    "Wird dafür genutzt die Skala und leeren Pixel um das Spektrogramm zu entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91a1c2b-e309-4876-8e4b-d85b86f631e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_frame(img, frame):\n",
    "    return img[frame[0]:frame[1], frame[2]:frame[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ede773-3c8c-4bd3-b8e3-8476dd4ef41b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Bilder einlesen\n",
    "Hier wird eine Liste aller Bilder im Ordner `../Bat_Orientation_Calls` erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa166b2-107b-4cb7-b681-ac11fdb3bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(\"../Bat_Orientation_Calls\")\n",
    "\n",
    "image_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".png\")]\n",
    "\n",
    "print(f\"Loaded {len(image_paths)} image paths.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd25f2c-56d2-4180-93c5-0384b87ac586",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Bildbearbeitung\n",
    "Hier werden die eingelesenen Bilder zugeschnitten, runterskaliert und optional mit Filtern o.ä. bearbeitet und anschließend in `./compressed_pictures/` gespeichert.\n",
    "\n",
    "**Hinweis:** Zur Darstellung des Fortschritts wird tqdm genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fdf4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_path in tqdm(image_paths):\n",
    "    # Bild für OpenCV2 einlesen\n",
    "    img = cv2.imread(image_path, -1)\n",
    "\n",
    "    # Den Rand inkl. Skala entfernen\n",
    "    # Has to be hardcoded, cause not every image has the exact pixel perfect border/scale, so an automatic detection\n",
    "    #   trips up and the images would end with different sizes.\n",
    "    frame = (36, 251, 55, 388)\n",
    "    img = remove_frame(img, frame)\n",
    "\n",
    "    # Größe des neuen Bildes berechnen und skalieren\n",
    "    base_width = 128 # finale Breite des Bildes\n",
    "    height, width, channels = img.shape\n",
    "    aspect_ratio = width / height  # Width / Height\n",
    "    new_width = base_width\n",
    "    new_height = int(new_width / aspect_ratio)\n",
    "    img = cv2.resize(img, (new_width, new_height)) \n",
    "\n",
    "    # Alpha Channel entfernen, da dieser keine Informationen beinhaltet\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "    # Bild abspeichern\n",
    "    image_name = Path(image_path).name\n",
    "    cv2.imwrite(f\"./compressed_pictures/{image_name}\",img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a358be5-4040-4f0b-8f44-468a2c5469fa",
   "metadata": {},
   "source": [
    "### Klassen aus CSV einlesen und anpassen\n",
    "Hier werden die Klassen aus den CSV's eingelesen und anschließend Unerwünschte entfernt.\n",
    "Kriterien für die Unerwünschtheit sind folgende:\n",
    "- Klasse ist 'Fledermaus nicht bestimmbar' oder 'Schwarzbild', da diese keine sinnvollen Informationen sind\n",
    "- Klasse (bzw. Bild-ID) ist ein Duplikat\n",
    "- Klasse hat weniger als 60 Samples, da diese schlichweg ungeeignet für das Trainieren eines Modells sind\n",
    "\n",
    "Anschließend liegt ein DataFrame mit den ID-Klassen Paaren vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff50f2f3-f3df-44f6-9514-3150188d142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads in classes from all csv's, and edits it\n",
    "def classes_csv_to_df(file_paths: list, delimiter=\";\") -> pd.DataFrame:\n",
    "    df_all = pd.DataFrame()\n",
    "    for file_path in file_paths:\n",
    "        df_current = pd.read_csv(file_path, delimiter=delimiter)\n",
    "        df_all = pd.concat([df_all, df_current])\n",
    "\n",
    "    df_all = df_all.reset_index(drop=True)\n",
    "    df_all = df_all.drop_duplicates()\n",
    "    df_all.drop(\"Filename\", axis=1, inplace=True)\n",
    "    df_all = remove_unwanted_datapoints(df_all)\n",
    "    df_all.replace(\"&Mausohr \", \"Mausohr\", inplace=True)\n",
    "    df_all = remove_less_sample_classes(df_all, 60)\n",
    "    \n",
    "    #df_all.drop(\"Species\", axis=1, inplace=True)\n",
    "    return df_all\n",
    "\n",
    "# Removes all duplicate entries and entries of type 'Fledermaus nicht bestimmbar' or 'Schwarzbild'\n",
    "def remove_unwanted_datapoints(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ids = set()\n",
    "    duplicated_ids = set()\n",
    "    for row in df.iterrows():\n",
    "        id = row[1][\"ID\"]\n",
    "        if id in ids:\n",
    "            duplicated_ids.add(id)\n",
    "        ids.add(id)\n",
    "\n",
    "    # print how many were droppes\n",
    "    print(f\"Dropped {len(duplicated_ids)} entries cause they were duplicates.\")\n",
    "    print(f\"Dropped {len(df[df['Species'] == 'Fledermaus nicht bestimmbar']) } entries cause they were of class 'Fledermaus nicht bestimmbar'.\")\n",
    "    print(f\"Dropped {len(df[df['Species'] == 'Schwarzbild'])} entries cause they were of class 'Schwarzbild'.\")\n",
    "    print(f\"Dropped {len(duplicated_ids)+len(df[df['Species'] == 'Fledermaus nicht bestimmbar'])+len(df[df['Species'] == 'Schwarzbild'])} in total.\")\n",
    "    \n",
    "    return df[~((df['ID'].isin(duplicated_ids)) |\n",
    "            (df['Species'].isin(['Fledermaus nicht bestimmbar', 'Schwarzbild'])))]\\\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "# Remove all classes with less than 60 entries\n",
    "def remove_less_sample_classes(df: pd.DataFrame, min_samples: int) -> pd.DataFrame:\n",
    "    class_distribution = df['Species'].value_counts()\n",
    "    valid_classes = class_distribution[class_distribution >= min_samples].index\n",
    "    return df[df['Species'].isin(valid_classes)]\n",
    "\n",
    "## execute functions\n",
    "\n",
    "df = classes_csv_to_df([\"../Auswertung_20220524.csv\",\"../LMU_20180326_class.csv\", \"../LMU_20180505_classified.csv\"])\n",
    "\n",
    "print(f\"DataFrame as {len(df)} different entries (Images).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e03463-eb8e-4bbe-a954-b3eac563df6d",
   "metadata": {},
   "source": [
    "#### Klassen konvertieren\n",
    "Hier werden die Labels der Klassen von den Fledermausnamen in numerische Label konvertiert.  \n",
    "\n",
    "**Hinweis:** Funktionen für kategorische und one-hot-encoded Labels haben wir erstellt aber letzendlich waren sie nicht nötig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79abd20-5d7f-4dbf-8075-997094a355a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_classes(df: pd.DataFrame, column_name: str):\n",
    "    # df passed as call by reference\n",
    "    df_copy = df.copy()\n",
    "    df_copy[column_name] = df_copy[column_name].astype('category')\n",
    "    return df_copy\n",
    "\n",
    "def numerical_classes(df: pd.DataFrame, column_name: str) -> tuple[pd.DataFrame, dict[int|str]]:\n",
    "    df_copy = df.copy()\n",
    "    df_copy[column_name] = df_copy[column_name].astype('category')\n",
    "    class_mapping = dict(enumerate(df_copy[column_name].cat.categories))\n",
    "    df_copy[column_name] = df_copy[column_name].cat.codes\n",
    "    return df_copy, class_mapping\n",
    "\n",
    "def encode_classes(df: pd.DataFrame, column_name: str):\n",
    "    encoded_classes = pd.get_dummies(df[column_name])\n",
    "    df = df.join(encoded_classes)\n",
    "    return df\n",
    "\n",
    "df_numerical, class_mapping = numerical_classes(df, \"Species\")\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf42251-535f-4e21-a39b-84e642b6908a",
   "metadata": {},
   "source": [
    "#### Klassen Mapping speichern\n",
    "Damit später die numerischen Klassen noch zuordenbar sind, wird das Mapping von Wertv und Klasse in der Datei `./data/class_mapping.csv` gespeichert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f1f1d4-eace-40f6-989a-efad084e7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_mapping_to_csv(class_mapping_dict: dict) -> None: \n",
    "    with open('./data/class_mapping.csv', 'w') as class_mapping_csv:  \n",
    "        writer = csv.writer(class_mapping_csv)\n",
    "        for key, value in class_mapping_dict.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "class_mapping_to_csv(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a160234-f81c-42e3-9220-4d770289740d",
   "metadata": {},
   "source": [
    "#### Klassenverteilung plotten\n",
    "Hier wird die Verteilung der Klassen geplottet und unter `./data/class_distribution` gespeichert. Wie zu sehen ist, sind sie stark ungleichmäßig verteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1e66e-93e1-4dcd-9f31-bb1ec8f578a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class_distribution(df: pd.DataFrame):\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    df['Species'].value_counts().plot.bar()\n",
    "    print(df['Species'].value_counts())\n",
    "    plt.title('Count Distribution')\n",
    "    plt.savefig(\"./data/class_distribution\")\n",
    "    plt.show(fig)\n",
    "    \n",
    "plot_class_distribution(df_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6aa44a-ec8c-44a7-87c4-e7f5b2a2992e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Bilder aus ordner lesen, Klasse zuordnen und in DataFrame wandeln\n",
    "In diesen Funktionen werden die bearbeiten Bilder geladen, ihnen die richtige Klasse zugeordnet und anschließend in einem DataFrame gespeichert.\n",
    "\n",
    "**Hinweis:** Hier wird erneut tqdm verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b8d4bf-f679-4e88-acbf-082cd4f6834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given an ID return the according class\n",
    "def get_classes_from_id(id: int, df: pd.DataFrame) -> pd.Series:\n",
    "    for row in df.iterrows():\n",
    "        if id == row[1][\"ID\"]:\n",
    "            return row[1].drop(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc0b485-194e-43e3-8df8-6a4f8e80204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_folder(folder_path: str, df_categorical=pd.DataFrame(), df_numerical=pd.DataFrame(), df_encoded=pd.DataFrame()) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    # make sure at least one df is set\n",
    "    if not df_categorical.empty and not df_numerical.empty and not df_encoded.empty:\n",
    "        raise ValueError(\"You have to define at least one Dataframe.\")\n",
    "        \n",
    "    images_categorical = list()\n",
    "    images_numerical = list()\n",
    "    images_encoded = list()\n",
    "    column_names = [\"data\", \"Species\"]\n",
    "    column_names_encoded = [\"data\"] + list(df_encoded.columns)[1:]\n",
    "    \n",
    "    for i, filename in enumerate(tqdm(os.listdir(folder_path))):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(img_path):  \n",
    "            # read in image\n",
    "            img = cv2.imread(img_path, -1)\n",
    "            # some images are broken\n",
    "            if img is not None:\n",
    "                # for the first image, save the size of the image to a seperate file, which makes it easier for other\n",
    "                # notebooks to get the image size & channels\n",
    "                if i == 0:\n",
    "                    print(\"The final image shape/size[hwc] is:\",img.shape)\n",
    "                    # store image shape in file\n",
    "                    with open(\"./data/meta.json\",\"w+\") as file:\n",
    "                        file.write(json.dumps({\"h\":img.shape[0],\"w\":img.shape[1],\"c\":img.shape[2]}))\n",
    "                \n",
    "                # find the class for the image based on ID and type of given class representation\n",
    "                class_categorical = None\n",
    "                class_numerical = None\n",
    "                class_encoded = None\n",
    "                if not df_categorical.empty:\n",
    "                    class_categorical = get_classes_from_id(int(filename[:-4]), df_categorical)\n",
    "                if not df_numerical.empty:\n",
    "                    class_numerical = get_classes_from_id(int(filename[:-4]), df_numerical)\n",
    "                if not df_encoded.empty:\n",
    "                    class_encoded = get_classes_from_id(int(filename[:-4]), df_encoded)\n",
    "                    \n",
    "                # need to check, if the class of the image is not null [aka. image would be one of the unwanted\n",
    "                # datapoints (e. g. class Schwarzbild)]\n",
    "                if (class_categorical is not None) or (class_numerical is not None) or (class_encoded is not None):\n",
    "                    if not df_categorical.empty:\n",
    "                        images_categorical.append([img.flatten(), *class_categorical.values])\n",
    "                    if not df_numerical.empty:\n",
    "                        images_numerical.append([img.flatten(), *class_numerical.values])\n",
    "                    if not df_encoded.empty:\n",
    "                        images_encoded.append([img.flatten(), *class_encoded.values])\n",
    "\n",
    "    return_dfs = dict()\n",
    "\n",
    "    if not df_categorical.empty:\n",
    "        return_dfs[\"df_categorical\"] = pd.DataFrame(np.array(images_categorical, dtype=object), columns=column_names)\n",
    "    if not df_numerical.empty:\n",
    "        return_dfs[\"df_numerical\"] = pd.DataFrame(np.array(images_numerical, dtype=object), columns=column_names)\n",
    "    if not df_encoded.empty:\n",
    "        return_dfs[\"df_encoded\"] = pd.DataFrame(np.array(images_encoded, dtype=object), columns=column_names_encoded).drop(\"Species\", axis=1)\n",
    "\n",
    "    return return_dfs\n",
    "\n",
    "dfs = load_images_from_folder(\"./compressed_pictures/\", df_numerical=df_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f6936",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Normalisierung der Daten\n",
    "\n",
    "Daten werden auf in ein Intervall von [0, 1] gebracht, dadurch wird die Konvergenzgeschwindigkeit verbessert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e89c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in dfs.items():\n",
    "    df['data'] = df['data'] / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9ac98-0acb-4f49-89ae-599a8ee3882e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### DataFrame als Pickles Datei speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176f3da1-7452-4642-abfa-f443937208dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in dfs.items():\n",
    "    if df_name == \"df_categorical\":\n",
    "        dfs[\"df_categorical\"].to_pickle(\"./data/images_df_categorical.pkl\")\n",
    "    if df_name == \"df_numerical\":\n",
    "        dfs[\"df_numerical\"].to_pickle(\"./data/images_df_numerical.pkl\")\n",
    "    if df_name == \"df_encoded\":\n",
    "        dfs[\"df_encoded\"].to_pickle(\"./data/images_df_encoded.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed1899c-83a9-4355-a62e-9dd0b6e3020f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Umgang mit den Daten\n",
    "Wie man die Daten einlesen und ausgeben kann."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731c0b4",
   "metadata": {},
   "source": [
    "#### Daten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80ee6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = dfs[\"df_numerical\"]\n",
    "data = pd.read_pickle('./data/images_df_numerical.pkl')\n",
    "X, y = data['data'], data['Species']\n",
    "X, y = np.stack(X).astype(np.float16), y.to_numpy().astype(np.uint8)\n",
    "with open(\"./data/meta.json\",\"r\") as file:\n",
    "    image_meta = json.load(file) \n",
    "image_shape = (image_meta['h'],image_meta['w'],image_meta['c'])\n",
    "X = X.reshape((-1,) + image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e736e823",
   "metadata": {},
   "source": [
    "#### Bilder ausgeben\n",
    "\n",
    "Man sieht, dass die Bilder sehr ähnlich sind und für uns kaum unterscheidbar. Deshalb war es oft recht schwierig, wie man die Bilder verbessern kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pictures(X, y, title, n=20):\n",
    "    fig = plt.figure(figsize=(20, 4))\n",
    "    fig.suptitle(title, fontsize=20)\n",
    "    for i in range(n):\n",
    "        # Display original\n",
    "        ax = plt.subplot(2, n, i + 1)\n",
    "        if image_shape[2] == 1:\n",
    "            plt.imshow((X[i]*255.).astype('uint8'), cmap='inferno', vmin=0, vmax=255) # for grayscale images\n",
    "            plt.set_suptitle(f\"{y[i]}\")\n",
    "        elif image_shape[2] == 4 or image_shape[2] == 3:\n",
    "            plt.imshow(cv2.cvtColor((X[i]*255.).astype('uint8'),cv2.COLOR_BGR2RGB)) # for rgb images\n",
    "            ax.title.set_text(f\"{y[i]}\")\n",
    "        #plt.color()\n",
    "        ax.get_xaxis().set_visible(False)\n",
    "        ax.get_yaxis().set_visible(False)\n",
    "        plt.savefig(\"./pics.png\")\n",
    "    \n",
    "print_pictures(X, y, title=\"orginal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4429a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## PCA\n",
    "\n",
    "Wir haben uns Anfangs Gedanken über PCA gemacht um die Klassen besser trennbar zu machen und ggf. die Daten komprimieren zu können, jedoch schnell verworfen. Es macht bei Bildern einfach wenig Sinn, da man fast genauso viele Hauptkomponenten bräuchte, wie ursprüngliche Features um ein gutes Ergebnis zu erhalten.\n",
    "\n",
    "**Hinweis:** Wir haben uns überlegt, ob wir die Daten zu Audio-Dateien umwandeln können, jedoch ist das nicht möglich ohne nähere Informationen über die Spektogramme. Auf die Audiodateien wäre die PCA ggf. ein guter Ansatz gewesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7131077",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%script false --no-raise-error\n",
    "def bestComponentValue(data):\n",
    "    for i in range(2, len(X), 25_000):\n",
    "        pca = PCA(n_components = i)\n",
    "        pca.fit_transform(X)\n",
    "        print(f'components = {i}, {pca.explained_variance_ratio_.sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47f6e4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Resampling\n",
    "\n",
    "Aufgrund der sehr ungleich verteilten Klassen haben wir mit Oversampling (Adasyn) und einer Kombination von Under- und Oversampling (Smoteen) experimentiert. Alleiniges undersampling haben wir als nicht sinnvoll erachtet, da selbst die größten Klassen recht wenig Datenpunkte enthalten.\n",
    "\n",
    "Ẁir sind aber ohne Expertenwissen an unsere Grenzen gestoßen\n",
    "\n",
    "Desweiteren sind wir z.B. mit SMOTEEN an Grenzen gestoßen, die durch das ENN verursacht wurden. ENN (EditedNearestNeighbours) versucht nach dem resamplen, durch SMOTE, mithilfe von einer kNN Klassifikation nur Daten beizubehalten die sich klar von den anderen abgrenzen, sprich alle Nachbarn oder eine Mehrheit übereinstimmen. Wenn aber alle Nachbarn übereinstimmen müssen, dann haben wir dadurch teilweise ganze Klassen verloren. Wenn allerdings nur eine Mehrheit übereinstimme muss, droppen wir gar keine Klassen mehr und es bleibt beim Oversampling.\n",
    "Dies zeigt nochmals wie ähnlich die Daten sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2581b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernt Klassen >= 60 Samples\n",
    "#data = data[data['Species'].isin(counts[counts>=60].index)]\n",
    "# Entfernt Klassen >= 200 Samples\n",
    "#data = data[data['Species'].isin(counts[counts>=200].index)]\n",
    "# Entfernt Klassen >= 500 Samples\n",
    "#data = data[data['Species'].isin(counts[counts>=500].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e41b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(resampler, return_syn_indezes=False, value_range_end=1) -> tuple[np.array, np.array]:\n",
    "    X, y = data['data'], data['Species']\n",
    "    X, y = np.stack(X).astype(np.float16), y.to_numpy().astype(np.uint8)\n",
    "    _, dist = np.unique(y, return_counts=True)\n",
    "    print(f\"{resampler} [0, {value_range_end}] before res:\",dist)\n",
    "\n",
    "    X_resampled, y_resampled = resampler.fit_resample(X*value_range_end, y)\n",
    "\n",
    "    _, dist = np.unique(y_resampled, return_counts=True)\n",
    "    print(f\"{resampler} [0, {value_range_end}] after res:\", dist)\n",
    "\n",
    "    # zeros am Anfang hinzufügen um sie gleich shufflen zu können\n",
    "    synthetic_indices = np.concatenate((np.zeros(len(X)), np.arange(len(X), len(X_resampled))), axis=0)\n",
    "\n",
    "    # shuffle ist nötig, da die synthetischen Daten nach Labels geordnet sind (random_state für bessere nachvollziehbarkeit)\n",
    "    X_resampled, y_resampled, synthetic_indices = shuffle(X_resampled, y_resampled, synthetic_indices, random_state=1)\n",
    "\n",
    "    if return_syn_indezes:\n",
    "        # zeros wieder aus den indizes droppen\n",
    "        return X_resampled.reshape((-1,) + image_shape), y_resampled, synthetic_indices[synthetic_indices != 0].astype(np.uint16)\n",
    "    else:\n",
    "        return X_resampled.reshape((-1,) + image_shape), y_resampled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522df056",
   "metadata": {},
   "source": [
    "**Hinweis:** Die resample_after_split-Funktion wurde benutzt um die Mengen nach dem KFold-Split einzeln zu resamplen, also z. B. nur train. Es ist der vollständigkeitshalber an dieser Stelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c6522",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "def resample_after_split(X,y, resampler) -> tuple[np.array, np.array]:\n",
    "    n, h, w, _ = X.shape\n",
    "    X = X.reshape((n, h * w * 3))\n",
    "    _, dist = np.unique(y, return_counts=True)\n",
    "    print(f\"{resampler} before res:\",dist)\n",
    "\n",
    "    X_resampled, y_resampled = resampler.fit_resample(X, y)\n",
    "\n",
    "    _, dist = np.unique(y_resampled, return_counts=True)\n",
    "    print(f\"{resampler} after res:\", dist)\n",
    "\n",
    "    X_resampled = X_resampled.reshape((-1,) + image_shape)\n",
    "    # shuffle ist nötig, da die synthetischen Daten nach Labels geordnet sind (random_state für bessere nachvollziehbarkeit)\n",
    "    X_resampled, y_resampled = shuffle(X_resampled, y_resampled, random_state=1)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# resample_after_split = SMOTEENN(sampling_strategy='all',enn=ENN(kind_sel='mode',n_neighbors=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0945db",
   "metadata": {},
   "source": [
    "Verschiedene Wertebereiche der X Daten vor dem Resampling haben für uns einen sehr viel besseres Ergebnis der geresampelten Daten gemacht.\n",
    "\n",
    "Erklärungsversuche:  \n",
    "[0, 1]: zu nah an den Orginaldaten (scheinen Kopien zu sein)  \n",
    "[0, 4]: für uns sehen die Bilder sehr gut geeignet aus, aber das Model hat keinen Vorteil dadurch. Uns fehlt das Domänenwissen warum dies der Fall ist.  \n",
    "[0, 20] und [0, 255]: zu weit weg von den Orginaldaten  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf76f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversampling\n",
    "adasyn = ADASYN()\n",
    "print_pictures(X, y, title=\"orginal\")\n",
    "# Range [0,1]\n",
    "X_adasyn_0_1, y_adasyn_0_1, synthetic_indices_0_1 = resample(adasyn, return_syn_indezes=True)\n",
    "print_pictures(X_adasyn_0_1[synthetic_indices_0_1], y_adasyn_0_1, title=\"resample [0,1]\")\n",
    "# Range [0,4]\n",
    "X_adasyn_0_4, y_adasyn_0_4, synthetic_indices_0_4 = resample(adasyn, return_syn_indezes=True)\n",
    "print_pictures(X_adasyn_0_4[synthetic_indices_0_4], y_adasyn_0_4, title=\"resample [0,4]\")\n",
    "# Range [0,20]\n",
    "X_adasyn_0_20, y_adasyn_0_20, synthetic_indices_0_20 = resample(adasyn, return_syn_indezes=True, value_range_end=20)\n",
    "print_pictures(X_adasyn_0_20[synthetic_indices_0_20], y_adasyn_0_20, title=\"resample [0,20]\")\n",
    "# Range [0,255]\n",
    "X_adasyn_0_255, y_adasyn_0_255, synthetic_indices_0_255 = resample(adasyn, return_syn_indezes=True, value_range_end=255)\n",
    "print_pictures(X_adasyn_0_255[synthetic_indices_0_255], y_adasyn_0_255, title=\"resample [0,255]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1048c4c",
   "metadata": {},
   "source": [
    "**Hinweis:** Wir denken, dass das Problem beim resampeln mit Adasyn ist das wir in manchen Klassen aus sehr wenig sehr viele zu machen (z. B. 60->2000) Die Klassenverteilung von SMOTETomek hat sehr stark Adasyn geähnelte. Daher haben wir SMOTETomek haben wir nicht weiter beachtet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bcfe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# under- und oversampling\n",
    "smotetomek = SMOTETomek()\n",
    "X_smotetomek, y_smotetomek = resample(smotetomek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# under- und oversampling\n",
    "somteenn = SMOTEENN()\n",
    "print_pictures(X, y, title=\"orginal\")\n",
    "# range [0,1]\n",
    "X_somteenn_0_1, y_somteenn_0_1, synthetic_indices_0_1 = resample(somteenn, return_syn_indezes=True)\n",
    "print_pictures(X_somteenn_0_1[synthetic_indices_0_1], y_somteenn_0_1, title=\"resample [0,1]\")\n",
    "# range [0,4]\n",
    "X_somteenn_0_4, y_somteenn_0_4, synthetic_indices_0_4 = resample(somteenn, return_syn_indezes=True)\n",
    "print_pictures(X_somteenn_0_4[synthetic_indices_0_4], y_somteenn_0_4, title=\"resample [0,4]\")\n",
    "# range [0,255]\n",
    "X_somteenn_0_255, y_somteenn_0_255, synthetic_indices_0_255 = resample(somteenn, return_syn_indezes=True, value_range_end=255)\n",
    "print_pictures(X_somteenn_0_255[synthetic_indices_0_255], y_somteenn_0_255, title=\"resample [0,255]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0013c3",
   "metadata": {},
   "source": [
    "**Hinweis:** Die Tests wurden auf unserem Hauptmodel CNN durchgeführt (s. CNN-Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe768cc",
   "metadata": {},
   "source": [
    "|Durchführung|Avg. Test Acc|\n",
    "|:-|-:|\n",
    "| Kein Resample, top 6 classes (> 60), cnn  | 79% |\n",
    "| Kein Resample, top 4 classes (>200), cnn  | 83% |\n",
    "| Kein Resample, top 2 classes (>500), cnn  | 87% |\n",
    "|---------------|\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=3)), top 6 classes (> 60), cnn   | 63% |\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=3)), top 4 classes (>200), cnn   | 80% |\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=3)), top 2 classes (>500), cnn   | 87% |\n",
    "|---------------|\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=7)), top 6 classes (> 60), cnn   | 68% |\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=7)), top 4 classes (>200), cnn   | 79% |\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=7)), top 2 classes (>500), cnn   | 86% |\n",
    "|---------------|\n",
    "| Train Resample (Smotteen(all,enn=Mode,n=3)), top 6 classes (> 60), cnn   | 73% |\n",
    "| Train Resample (Smotteen(all,enn=Mode,n=3)), top 4 classes (>200), cnn   | 81% |\n",
    "| Train Resample (Smotteen(all,enn=Mode,n=3)), top 2 classes (>500), cnn   | 88% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31063780",
   "metadata": {},
   "source": [
    "**Ergebis:** Man sieht, dass das Resampling leider in der Form keinen Vorteil bringt daher haben wir die Idee verworfen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9dda23",
   "metadata": {},
   "source": [
    "**Hinweis:** Wir halten die ungleiche Klassenverteilung bzw. den Mangel an Samples einiger Klassen für den Hebel mit dem meisten Verbesserungspotential. (Wenn wir die Testdaten mit resampeln erhalten wir einen Score von bis zu 92% s. unten)\n",
    "Mit mehr Zeit/Domänenwissen wären folgende Dinge denkbar:\n",
    "* Echte Daten vor allem der unterrepräsentierten Daten sammeln\n",
    "* Synthetische Daten erzeugen, welche sehr nah den der Orginaldaten sind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd70bd7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chunks um RAM zu sparen\n",
    "\n",
    "Da wir extrem große RAM Probleme hatten haben wir mit Aufteilung der Daten in Chunks experimentiert, aber die Idee dann wieder verworfen, da nach der Verkleinerung der Bilder es auch ohne geklappt hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abae3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "def split_df_equal_class_distribution(df, batch_size):\n",
    "    \n",
    "    df['temp_id'] = range(len(df))\n",
    "    \n",
    "    num_batches = int(np.ceil(len(df) / batch_size))\n",
    "    \n",
    "    grouped = df.groupby('Species', group_keys=False)\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        chunk = pd.DataFrame(columns=df.columns)\n",
    "        for _, group in grouped:\n",
    "            num_samples = int(batch_size * len(group) / len(df))\n",
    "            sample_indices = np.random.choice(group['temp_id'], size=num_samples, replace=False)\n",
    "            chunk = pd.concat([chunk, df[df['temp_id'].isin(sample_indices)]])\n",
    "        chunk = chunk.drop('temp_id', axis=1)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "chunks_with_same_dist = split_df_equal_class_distribution(data, batch_size)\n",
    "del data\n",
    "classes = chunks_with_same_dist[0][\"Species\"].unique()\n",
    "number_of_classes = classes.size\n",
    "most_x_in_one_class = chunks_with_same_dist[0][\"Species\"].value_counts().iloc[0]\n",
    "\n",
    "def resample(resampler) -> tuple[np.array, np.array]:\n",
    "    # 0.3 als Puffer, weil sich der Output der verschiedenen Resampler ja unterscheidet\n",
    "    array_size = int(most_x_in_one_class * number_of_classes * (len(chunks_with_same_dist) + 0.3))\n",
    "    # 216432 war die Länge des Vektors der nicht verkleinerten Bilder\n",
    "    X = np.empty((array_size, 216432), dtype=np.uint8)\n",
    "    y = np.empty((array_size), dtype=np.uint8)\n",
    "\n",
    "    current_index = 0\n",
    "    for chunk in chunks_with_same_dist:\n",
    "        X_batch, y_batch = chunk['data'], chunk['Species']\n",
    "        X_batch, y_batch = np.stack(X_batch).astype(np.uint8), y_batch.astype(np.uint8)\n",
    "        X_resampled, y_resampled = resampler.fit_resample(X_batch, y_batch)\n",
    "        num_samples = X_resampled.shape[0]\n",
    "        X[current_index:current_index + num_samples] = X_resampled.astype(np.uint8)\n",
    "        y[current_index:current_index + num_samples] = y_resampled.astype(np.uint8)\n",
    "        current_index += num_samples\n",
    "\n",
    "    X.resize((current_index, X.shape[1]))\n",
    "    y.resize(current_index)\n",
    "    print(f\"{resampler}: \", pd.Series(y, dtype=pd.UInt8Dtype()).value_counts())\n",
    "\n",
    "    return X, y\n",
    "\n",
    "num_samples = (len(X) * 0.8)\n",
    "chunk_size = 1000\n",
    "for i in range(0, num_samples, chunk_size):\n",
    "    X[i:i+chunk_size], y[i:i+chunk_size] # Hier ist der Code abgeschnitten, da nur die Idee erläutert werden soll "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df94c352-b92d-41f3-a256-42273b771a9f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Anmerkungen zu getesteten Bildbearbeitungen u.ä.\n",
    "Für den ersten Teil der Bildaufbereitung werden hier keine Filter ö.ä. Bildbearbeitungen verwendet.\n",
    "Allerdings haben wir im Laufe der Bearbeitung verschiedene Filter u.ä. ausprobiert, welche inkl. ihres Test Accuracy Wertes gelistet sind."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d0e33-0ba1-4f52-b967-327d9e732c90",
   "metadata": {},
   "source": [
    "**Hinweis:** Die Test Accuracy Werte entsprechen denen unseres Modells, auf Basis der mit Adasyn vollständig geresampleten (auch Testdaten) Bilder. Dies hat den Grund, dass die Tests durchgeführt wurden bevor wir den Fehler entdeckt haben und die Zeit für neue Tests nicht mehr ausgereicht hat. Somit sind sie ca 10-30% höher, als auf den ungresampleten Bildern. Mehr dazu im `cnn.ipynb`.\n",
    "\n",
    "\n",
    "|Bearbeitung (In Anwendungsreihenfolge)|Test Accuracy|\n",
    "|:-|-:|\n",
    "|Turn Pictrues into single channel grayscale|78%|\n",
    "|Turn Pictrues into single channel grayscale + fastNLMeansDenoising|78%|\n",
    "|fastNLMeansDenoising after downscaling|80%|\n",
    "|Bilateral Filter|82%|\n",
    "|Color Histogram Equalization + Contrast(*1.7) + Brightness(-150) + Bilateral Filter|82%\n",
    "|fastNLMeansDenoising before downscaling|83%|\n",
    "|Outlinedetection Conv2D Filter + Bilateral Filter + fastNLMeansDenoising + Contrast(*1.7) + Brightness(-150) + Sobel Conv2D Filter| 88%|\n",
    "|Contrast(*1.5) + Brightness(-150)|88%|\n",
    "|Contrast(*1.7) + Brightness(-100)|89%|\n",
    "|Contrast(*1.7) + Brightness(-150)|89%|\n",
    "|Sobel Conv2D Filter + Bilateral Filter + fastNLMeansDenoising + Bilateral Filter + Contrast(*1.7) + Brightness(-150)| 89%|\n",
    "|Bilateral Filter + Contrast(*1.7) + Brightness(-100)|90%|\n",
    "|Bilateral Filter + Contrast(*1.7) + Brightness(-150)|90%|\n",
    "|Outlinedetection Conv2D Filter + Bilateral Filter + fastNLMeansDenoising + Bilateral Filter + Contrast(*1.7) + Brightness(-150)| 91%|\n",
    "|Outlinedetection Conv2D Filter + Bilateral Filter + fastNLMeansDenoising + Bilateral Filter + Contrast(*1.7) + Brightness(-150) + Saturation=255| 91%|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b855d-ad57-4094-b2e9-797886fd8756",
   "metadata": {},
   "source": [
    "Eine Veranschaulichung dieser und weitere Filter/Bildbearbeitungen ist im folgenden zu sehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c1b8e-236d-4253-a985-a261d1520a34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##### spektrogramme anzeigen\n",
    "def show_images(images, titles):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, (img, title) in enumerate(zip(images, titles), 1):\n",
    "        plt.subplot(8, 4, i)\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.savefig(\"denoise_comaprison.png\")\n",
    "    plt.show()\n",
    "\n",
    "# color distribution to show differences in intesities etc\n",
    "def show_hist(images, titles):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, (image, title) in enumerate(zip(images, titles), 1):\n",
    "        plt.subplot(8, 4, i)\n",
    "        plt.title(title)\n",
    "        #plt.axis('off')\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        hist_r = cv2.calcHist([image_rgb], [0], None, [256], [0, 256]) / (image_size/3)\n",
    "        hist_g = cv2.calcHist([image_rgb], [1], None, [256], [0, 256]) / (image_size/3)\n",
    "        hist_b = cv2.calcHist([image_rgb], [2], None, [256], [0, 256]) / (image_size/3)\n",
    "\n",
    "        plt.plot(hist_r, color='red', label='Red')\n",
    "        plt.plot(hist_g, color='green', label='Green')\n",
    "        plt.plot(hist_b, color='blue', label='Blue')\n",
    "        plt.xlabel('Pixel Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.xlim([0, 255])\n",
    "        plt.ylim([0, 0.3])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"denoise_comaprison_hist.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Laden eines Beispielbildes\n",
    "image = dfs['df_numerical']['data'][0]\n",
    "image_size = image.size\n",
    "with open(\"./data/meta.json\",\"r\") as file:  # get metadata for images from file generated from import nb\n",
    "    image_meta = json.load(file) \n",
    "image_shape = (image_meta['h'],image_meta['w'],image_meta['c']) # hwc\n",
    "image= image.reshape(image_shape)\n",
    "image = cv2.cvtColor(image,cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "# Gaußscher Weichzeichner\n",
    "gaussian_blur = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "# Mittelwertfilter\n",
    "mean_blur = cv2.blur(image, (5, 5))\n",
    "\n",
    "# Medianfilter\n",
    "median_blur = cv2.medianBlur(image, 5)\n",
    "\n",
    "# Bilateralfilter\n",
    "bilateral_blur = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "\n",
    "# Bewegungsunschärfe\n",
    "kernel_motion_blur = np.zeros((5, 5))\n",
    "kernel_motion_blur[:, int((5-1)/2)] = 1\n",
    "kernel_motion_blur /= 5\n",
    "motion_blur = cv2.filter2D(image, -1, kernel_motion_blur)\n",
    "\n",
    "# predcit based on autoencoder\n",
    "#autoencoder = keras.saving.load_model('../autoencoder_files/autoencoder_cnn_denoise_v7.keras')\n",
    "#imagelist = np.asarray([image]).astype('float32')/255.\n",
    "#autoencoder_img = (autoencoder.predict(imagelist)[0]*255.).astype('uint8')\n",
    "\n",
    "# nl means denosing gray\n",
    "fast_nl_grey = cv2.fastNlMeansDenoising(image, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "# nl means denosing color\n",
    "fast_nl_color = cv2.fastNlMeansDenoisingColored(image, None, 15,15, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "# contrast/birghness\n",
    "brightness = 1 \n",
    "contrast = 1.5\n",
    "c_b = cv2.addWeighted(image, contrast, np.zeros(image.shape, image.dtype), 0, brightness)\n",
    "\n",
    "\n",
    "# cb + nlmeans\n",
    "brightness = 0 \n",
    "contrast = 1.5\n",
    "cb_nl = cv2.fastNlMeansDenoising(cv2.addWeighted(image, contrast, np.zeros(image.shape, image.dtype), 0, brightness), None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "\n",
    "# + cont\n",
    "cont1 = cv2.addWeighted(image, 1.5, np.zeros(image.shape, image.dtype), 0, -150)\n",
    "\n",
    "cont2 = cv2.addWeighted(image, 1.7, np.zeros(image.shape, image.dtype), 0, -150)\n",
    "\n",
    "cont3 = cv2.addWeighted(image, 2, np.zeros(image.shape, image.dtype), 0, -150)\n",
    "\n",
    "# bil + cont\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "bil_cont1 = cv2.addWeighted(img, 1.5, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "bil_cont2 = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "bil_cont3 = cv2.addWeighted(img, 2, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "# bil+c2+b-150+bil+c2+b-200\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 2, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "bl_cb_bil_cb1 = cv2.addWeighted(img, 2, np.zeros(img.shape, img.dtype), 0, -200)\n",
    "\n",
    "#sharpen kernel\n",
    "kernel = np.array([[-1, -1, -1],\n",
    "                       [-1, 9, -1],\n",
    "                       [-1, -1, -1]])\n",
    "sharp_img = cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "#sharpen laplace\n",
    "sharp_lapla = cv2.Laplacian(image, cv2.CV_8U) \n",
    "\n",
    "# equ hist\n",
    "img = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "equ_hist1 = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# equ hist + contrast\n",
    "img = cv2.addWeighted(image, 1.7, np.zeros(image.shape, image.dtype), 0, -150)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "equ_hist2 = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# equ hist + contrast + bilateral\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "equ_hist3 = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# erode\n",
    "kernel = np.array([[1, 9, 1],\n",
    "                   [1, 9, 1],\n",
    "                   [1, 9, 1]])\n",
    "eroded1 = cv2.erode(image, kernel)\n",
    "\n",
    "# c/b + equ hsit + bil\n",
    "img = cv2.addWeighted(img, 2, np.zeros(img.shape, img.dtype), 0, -100)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "eroded2 = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "\n",
    "# filter 2d sharp\n",
    "kernel = np.array([[0, -1, 0],\n",
    "                  [-1, 5, -1],\n",
    "                  [0, -1, 0]])\n",
    "filter2d_1 = cv2.filter2D(image,-1,kernel)\n",
    "\n",
    "# filter 2d outline\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 8, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "filter2d_2 = cv2.filter2D(image,-1,kernel)\n",
    "\n",
    "# filter 2d emboss\n",
    "kernel = np.array([\n",
    "  [-2, -1, 0],\n",
    "  [-1, 1, 1],\n",
    "  [0, 1, 2]\n",
    "])\n",
    "filter2d_3 = cv2.filter2D(image,-1,kernel)\n",
    "\n",
    "#filter2d outline + bil\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 8, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "img = cv2.filter2D(image,-1,kernel)\n",
    "fil_bil1 = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "\n",
    "#filter2d (-1/7)+ bil + nl + bil + c1.7/b-150  (best thing yet)\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 7, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "#img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "img = cv2.filter2D(image,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "fil_nl1 = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "#filter2d (-1/7)+ bil + nl + c1.7/b-150 (okay)\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 7, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "img = cv2.filter2D(image,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "fil_nl2 = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "#filter2d (outline)+ bil + nl + c1.7/b-150 + f2d (sobel)\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 7, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "img = cv2.filter2D(image,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "kernel = np.array([\n",
    "  [-1, 0, 1],\n",
    "  [-2, 0, 2],\n",
    "  [-1, 0, 1]\n",
    "])\n",
    "img = cv2.filter2D(img,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "fil_nl3 = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# best thing + emboss\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 7, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "img = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n",
    "img = cv2.filter2D(img,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "kernel = np.array([\n",
    "  [-2, -1, 0],\n",
    "  [-1, 1, 1],\n",
    "  [0, 1, 2]\n",
    "])\n",
    "fil_nl4 = cv2.filter2D(img,-1,kernel)\n",
    "\n",
    "list = ([image, gaussian_blur, mean_blur, median_blur, bilateral_blur, motion_blur,fast_nl_grey,fast_nl_color,\n",
    "         c_b,cb_nl,cont1,cont2,cont3,bil_cont1,bil_cont2,bil_cont3,sharp_img,sharp_lapla,equ_hist1,equ_hist2,\n",
    "         equ_hist3,bl_cb_bil_cb1,eroded1, eroded2,filter2d_1,filter2d_2,filter2d_3,fil_bil1,fil_nl1,fil_nl2,fil_nl3,\n",
    "         fil_nl4\n",
    "        ],\n",
    "        ['Original', 'Gaussian Blur', 'Mean Blur', 'Median Blur', 'Bilateral Filter', 'Motion Blur','NL Means Grey',\n",
    "         'NL Means Color','Color/Brightness','CB/NL','C/B (1.5,-150)','C/B (1.7,-150)','C/B (2,-150)',\n",
    "         'BF/Cont/Br (1.5,-150)', 'BF/Cont/Br (1.7,-150)', 'BF/Cont/Br (2,-150)', 'Sharp Kernel', 'Sharp Laplace',\n",
    "         'Histrogramm Equ.', 'HE+Contrast', 'HE+Cont+Bil', \"bil+c2+b-150+bil+c2+b-200\", \"Eroded\", \"+C/B\", 'Filter2D-Sharp',\n",
    "         \"Filter2D-Outline\", \"Filter2D-Emboss\", \"Fil2dOutline+Bil\", \"F2D+bil+nl+bil+cb (best)\", \"F2D+bil+nl+bil+cb+F2D\",\n",
    "         \"F2D+bil+nl+bil+cb\", \"best thing + emboss\"])\n",
    "\n",
    "# Anzeigen der Ergebnisse\n",
    "show_images(*list)\n",
    "show_hist(*list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91591da6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CAE\n",
    "Des weiteren haben wir Convolutional Autoencoder ausprobiert. (Da es ja um das Denoisen von Bildern geht.)\n",
    "\n",
    "Dafür muss zuerst Noise auf die Daten geleget werden. Wir nutzen v.a. Salt-and-Pepper noise, da diese der Noise in den Skeptrogrammen ähnlich ist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_salt_and_pepper_noise(image, noise_ratio=0.2):\n",
    "    noise_iamge = image.copy()\n",
    "    h, w, c = noise_iamge.shape\n",
    "    noisy_pixels = int(h * w * noise_ratio)\n",
    " \n",
    "    for _ in range(noisy_pixels):\n",
    "        row, col = np.random.randint(0, h), np.random.randint(0, w)\n",
    "        if np.random.rand() < 0.5:\n",
    "            noise_iamge[row, col] = [0, 0, 0] \n",
    "        else:\n",
    "            noise_iamge[row, col] = [255, 255, 255]\n",
    " \n",
    "    return (noise_iamge).astype(np.float16)\n",
    "\n",
    "def add_gaussian_noise(image, mean=0, std=0.3):\n",
    "    noise = np.random.normal(mean, std, image.shape)\n",
    "    noisy_image = np.clip(image + noise, 0, 1)\n",
    "    return (noisy_image).astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd7119b",
   "metadata": {},
   "source": [
    "Daten einlesen und splitten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22773641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image data, remove class and reshape \n",
    "data = pd.read_pickle('../data/images_df_numerical.pkl')\n",
    "X,y = data['data'], data['Species']\n",
    "X, y = np.stack(X).astype(np.float16), y.to_numpy().astype(np.uint8)\n",
    "print(X.shape, y.shape)\n",
    "image_size = X.size\n",
    "samples = X.size\n",
    "with open(\"../data/meta.json\",\"r\") as file:  # Shape der Bilder nach dem resizen aus dem data_prep notebook\n",
    "    image_meta = json.load(file) \n",
    "image_shape = (image_meta['h'],image_meta['w'],image_meta['c'])\n",
    "X = X.reshape((-1,) + image_shape)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,stratify=y, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2,stratify=y_train, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a94fd67",
   "metadata": {},
   "source": [
    "Noise auf die Bilder geben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678352f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_noise = [add_salt_and_pepper_noise(image) for image in X_train]\n",
    "X_test_noise = [add_salt_and_pepper_noise(image) for image in X_test]\n",
    "X_val_noise = [add_salt_and_pepper_noise(image) for image in X_val]\n",
    "\n",
    "y_train,X_train,y_test,X_test,y_val,X_val = np.asarray(X_train),np.asarray(X_train_noise),np.asarray(X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef3dab25",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image_shape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m weight_decay_alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m----> 2\u001b[0m shapes \u001b[38;5;241m=\u001b[39m \u001b[43mimage_shape\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39mimage_shape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Encoder\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'image_shape' is not defined"
     ]
    }
   ],
   "source": [
    "weight_decay_alpha = 0.01\n",
    "shapes = image_shape\n",
    "\n",
    "input = tf.keras.layers.Input(shape=image_shape)\n",
    "\n",
    "# Encoder\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(input)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "x = tf.keras.layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(x)\n",
    "x = tf.keras.layers.MaxPooling2D((2, 2), padding=\"same\")(x)\n",
    "\n",
    "# Decoder\n",
    "x = tf.keras.layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = tf.keras.layers.Conv2DTranspose(32, (3, 3), strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = tf.keras.layers.Conv2D(3, (3, 3), activation=\"sigmoid\", padding=\"same\")(x)\n",
    "x = tf.keras.layers.Cropping2D(((1,1),(0,0)))(x)\n",
    "\n",
    "# Autoencoder\n",
    "autoencoder = tf.keras.Model(input, x)\n",
    "autoencoder.compile(optimizer='adam', loss='binary_crossentropy',metrics=[\"accuracy\"])\n",
    "autoencoder.summary()\n",
    "\n",
    "# train on data\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=30, min_delta=0.01, start_from_epoch=15, restore_best_weights=True)\n",
    "autoencoder.fit(X_train,y_train,\n",
    "                epochs=200,\n",
    "                batch_size=30,\n",
    "                validation_data=[X_val,y_val],\n",
    "                callbacks=[early_stopping]\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669aa9b0",
   "metadata": {},
   "source": [
    "Bilder Predicten und ausgeben lassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0400e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "denoised_input = autoencoder.predict(X_test)\n",
    "denoised_target = autoencoder.predict(y_test)\n",
    "# display some of the images vs reconstruction\n",
    "n = 4  # How many digits we will display\n",
    "plt.figure(figsize=(20, 6))\n",
    "for i in range(n):\n",
    "    # Display original/target\n",
    "    ax = plt.subplot(4, n, i + 1)\n",
    "    plt.imshow(cv2.cvtColor((y_test[i].reshape(image_shape)*255.).astype(np.uint8), cv2.COLOR_BGR2RGB))\n",
    "    #plt.color()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # Display noise/input\n",
    "    ax = plt.subplot(4, n, i + 1+n)\n",
    "    plt.imshow(cv2.cvtColor((X_test[i].reshape(image_shape)*255.).astype(np.uint8), cv2.COLOR_BGR2RGB))\n",
    "    #plt.color()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # Display reconstruction/denoised on input\n",
    "    ax = plt.subplot(4, n, i + 1 + 2*n)\n",
    "    plt.imshow(cv2.cvtColor((denoised_input[i].reshape(image_shape)*255.).astype(np.uint8), cv2.COLOR_BGR2RGB))\n",
    "    #plt.color()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # Display reconstruction/denoised on target\n",
    "    ax = plt.subplot(4, n, i + 1 + 3*n)\n",
    "    plt.imshow(cv2.cvtColor((denoised_target[i].reshape(image_shape)*255.).astype(np.uint8), cv2.COLOR_BGR2RGB))\n",
    "    #plt.color()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.savefig(\"test_autoencoder_cnn_reconstruction.png\",dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8874e440",
   "metadata": {},
   "source": [
    "Zu sehen sind in der ersten Zeile die Originalen Spektogramme, die zweite die Originale mit der aufgelegten Noise, die dirtte Zeile den Output des CEAs auf den Originalen+Noise und die vierte Zeile den Output des CEAs auf den Originalen."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

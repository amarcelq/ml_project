{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66adbb1d-937b-4098-9e0e-9e4fba70c410",
   "metadata": {},
   "source": [
    "# Aufbereitung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772e326-d818-4530-b12e-50bcb78e42bc",
   "metadata": {},
   "source": [
    "Damit die Spektogramme überhaupt von Modellen genutzt werden können, müssen sie aufbereitet, (teilweise) bearbeitet und in einem gut verwendbarem Format abgespeichert werden.\n",
    "In unserem Fall besteht das aus den folgenden Schritten:\n",
    "1. Die Skala/Rahmen der Bilder entfernen\n",
    "2. Die Bilder runterskalieren\n",
    "3. (Optional) Filter oder andere Bildbearbeitungen anwenden\n",
    "4. Daten nach Klasse aussortieren\n",
    "5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3375aaf9-6fe1-46b0-85ca-8573ed88db4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337fed96-2c69-44be-ab39-5d001f61277b",
   "metadata": {},
   "source": [
    "#### Things tried:\n",
    "- fastNlMeansDenoising on big and small image ~ 83/80%\n",
    "- grayscale ~ 78%\n",
    "- grayscale + fastNlMeansDenoising on big and small image ~ 76%\n",
    "- bilateral Filter ~ 82%\n",
    "- bilateral filter + contrast(1.7) + brightness(-100) ~ 90% (max; avg: 89%) | avg90,best91\n",
    "- bilateral filter + contrast(1.7) + brightness(-150) ~ a:90%b:91%\n",
    "- contrast(1.7) + brightness(-100) ~ a:89%b:90%\n",
    "- contrast(1.7) + brightness(-150) ~ a:89%b:90%\n",
    "- contrast(1.5) + brightness(-150) ~ a:88%b:89%\n",
    "- contrast(2) + brightness(-150) ~ a:%b:%\n",
    "- hist equ + c/b (1.7/-150) + bil ~ a:82%b:82%\n",
    "- filter2d (-1/7)+ bil + nl + bil + c1.7/b-150 ~ a:91%b:91%\n",
    "- f2d (sobel) + bil + nl + bil + c1.7/b-150 ~ a:89%b:91%\n",
    "- filter2d (outline)+ bil + nl + c1.7/b-150 + f2d (sobel) ~a:88%b:88%\n",
    "- filter2d (-1/7)+ bil + nl + bil + c1.7/b-150 + sat=255 ~ a:91%b:92% aber stark schwankend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5fdf4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39c3032d8b3044b9a5d08ffea1e48807",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "folder_path = Path(\"../Bat_Orientation_Calls\")\n",
    "\n",
    "def remove_black_frame(img, frame):\n",
    "    return img[frame[0]:frame[1], frame[2]:frame[3]]\n",
    "\n",
    "image_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".png\")]\n",
    "\n",
    "for image_path in tqdm(image_paths):\n",
    "    img = cv2.imread(image_path, -1)\n",
    "    \n",
    "    #denoise\n",
    "    #img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "    # remove outer frame\n",
    "    frame = (36, 251, 55, 388)\n",
    "    img = remove_black_frame(img, frame)\n",
    "    \n",
    "    base_width = 128 # might change, since \n",
    "    height, width, channels = img.shape\n",
    "    aspect_ratio = width / height  # Width / Height\n",
    "\n",
    "    new_width = base_width\n",
    "    new_height = int(new_width / aspect_ratio)\n",
    "\n",
    "    img = cv2.resize(img, (new_width, new_height)) \n",
    "    # into grayscale\n",
    "    # img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # increase contrast and decrease brightness\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    #img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "    #img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "    # best yet\n",
    "    #kernel = np.array([\n",
    "    #  [-1, -1, -1],\n",
    "    #  [-1, 7, -1],\n",
    "    #  [-1, -1, -1]\n",
    "    #])\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    #img = cv2.filter2D(img,-1,kernel)\n",
    "    #img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "    #img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "    #img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "    #img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "    # ok\n",
    "    #kernel = np.array([\n",
    "    #  [-1, 0, 1],\n",
    "    #  [-2, 0, 2],\n",
    "    #  [-1, 0, 1]\n",
    "    #])\n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    #img = cv2.filter2D(img,-1,kernel)\n",
    "    #img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "    #img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "    #img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "    #img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "    # best thing + emboss\n",
    "    #kernel = np.array([\n",
    "    #    [-1, -1, -1],\n",
    "    #    [-1, 7, -1],\n",
    "    #    [-1, -1, -1]\n",
    "    #])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "    #img = cv2.filter2D(img,-1,kernel)\n",
    "    #img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "    #img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "    #img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "    #img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "    #kernel = np.array([\n",
    "   #     [-2, -1, 0],\n",
    "    #    [-1, 1, 1],\n",
    "    #    [0, 1, 2]\n",
    "    #])\n",
    "    #fil_nl4 = cv2.filter2D(img,-1,kernel)\n",
    "\n",
    "    \n",
    "    image_name = Path(image_path).name\n",
    "    cv2.imwrite(f\"./compressed_pictures/{image_name}\",img)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4658682-2b86-4040-ac3b-0017fdf8492d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d956a9ec7d4351b361ff6467648e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final image shape/size[hwc] is: (82, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "def classes_csv_to_df(file_paths: list, delimiter=\";\") -> pd.DataFrame:\n",
    "    df_all = pd.DataFrame()\n",
    "    for file_path in file_paths:\n",
    "        df_current = pd.read_csv(file_path, delimiter=delimiter)\n",
    "        df_all = pd.concat([df_all, df_current])\n",
    "\n",
    "    df_all = df_all.reset_index(drop=True)\n",
    "    df_all = df_all.drop_duplicates()\n",
    "    df_all.drop(\"Filename\", axis=1, inplace=True)\n",
    "    df_all = remove_unwanted_datapoints(df_all)\n",
    "    df_all.replace(\"&Mausohr \", \"Mausohr\", inplace=True)\n",
    "    df_all = remove_less_sample_classes(df_all, 60)\n",
    "    \n",
    "    #df_all.drop(\"Species\", axis=1, inplace=True)\n",
    "    return df_all\n",
    "\n",
    "def categorical_classes(df: pd.DataFrame, column_name: str):\n",
    "    # df passed as call by reference\n",
    "    df_copy = df.copy()\n",
    "    df_copy[column_name] = df_copy[column_name].astype('category')\n",
    "    return df_copy\n",
    "\n",
    "def numerical_classes(df: pd.DataFrame, column_name: str) -> tuple[pd.DataFrame, dict[int|str]]:\n",
    "    df_copy = df.copy()\n",
    "    df_copy[column_name] = df_copy[column_name].astype('category')\n",
    "    class_mapping = dict(enumerate(df_copy[column_name].cat.categories))\n",
    "    df_copy[column_name] = df_copy[column_name].cat.codes\n",
    "    return df_copy, class_mapping\n",
    "\n",
    "def encode_classes(df: pd.DataFrame, column_name: str):\n",
    "    encoded_classes = pd.get_dummies(df[column_name])\n",
    "    df = df.join(encoded_classes)\n",
    "    return df\n",
    "\n",
    "def remove_unwanted_datapoints(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ids = set()\n",
    "    duplicated_ids = set()\n",
    "    #duplicates = list()\n",
    "    for row in df.iterrows():\n",
    "        id = row[1][\"ID\"]\n",
    "        if id in ids:\n",
    "            duplicated_ids.add(id)\n",
    "            #duplicates.append(row)\n",
    "        ids.add(id)\n",
    "\n",
    "    # we drop than since they don't give us information in our trainingsprocess\n",
    "    #print(len(df[df[\"Schwarzbild\"] == 1]))\n",
    "    #print(len(df[df[\"Fledermaus nicht bestimmbar\"] == 1]))\n",
    "    # since this are just 9 we just drop them\n",
    "    # print(len(duplicates))\n",
    "    return df[~((df['ID'].isin(duplicated_ids)) |\n",
    "            (df['Species'].isin(['Fledermaus nicht bestimmbar', 'Schwarzbild'])))]\\\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "def remove_less_sample_classes(df: pd.DataFrame, min_samples: int) -> pd.DataFrame:\n",
    "    class_distribution = df['Species'].value_counts()\n",
    "    valid_classes = class_distribution[class_distribution >= min_samples].index\n",
    "    return df[df['Species'].isin(valid_classes)]\n",
    "\n",
    "def class_mapping_to_csv(class_mapping_dict: dict) -> None: \n",
    "    with open('data/class_mapping.csv', 'w') as class_mapping_csv:  \n",
    "        writer = csv.writer(class_mapping_csv)\n",
    "        for key, value in class_mapping_dict.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "def plot_class_distribution(df: pd.DataFrame):\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    df['Species'].value_counts().plot.bar()\n",
    "    plt.title('Count Distribution')\n",
    "    plt.savefig(\"data/class_distribution\")\n",
    "    plt.close(fig)\n",
    "\n",
    "df = classes_csv_to_df([\"../Auswertung_20220524.csv\",\"../LMU_20180326_class.csv\", \"../LMU_20180505_classified.csv\"])\n",
    "# RAM is cheaper than salary ;)\n",
    "#df_categorical = categorical_classes(df, \"Species\")\n",
    "df_numerical, class_mapping = numerical_classes(df, \"Species\")\n",
    "#df_encoded = encode_classes(df, \"Species\")\n",
    "class_mapping_to_csv(class_mapping)\n",
    "plot_class_distribution(df_numerical)\n",
    "\n",
    "def get_classes_from_id(id: int, df: pd.DataFrame) -> pd.Series:\n",
    "    for row in df.iterrows():\n",
    "        if id == row[1][\"ID\"]:\n",
    "            return row[1].drop(\"ID\")\n",
    "\n",
    "def calc_black_frame(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    _,thresh = cv2.threshold(gray,1,255,cv2.THRESH_BINARY)\n",
    "    contours,_ = cv2.findContours(thresh,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnt = contours[0]\n",
    "    x,y,w,h = cv2.boundingRect(cnt)\n",
    "    return (y, y+h, x, x+w)\n",
    "    \n",
    "def load_images_from_folder(folder_path: str, df_categorical=pd.DataFrame(), df_numerical=pd.DataFrame(), df_encoded=pd.DataFrame()) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    if not df_categorical.empty and not df_numerical.empty and not df_encoded.empty:\n",
    "        raise ValueError(\"You have to define at least one Dataframe.\")\n",
    "    images_categorical = list()\n",
    "    images_numerical = list()\n",
    "    images_encoded = list()\n",
    "    column_names = [\"data\", \"Species\"]\n",
    "    column_names_encoded = [\"data\"] + list(df_encoded.columns)[1:]\n",
    "    for i, filename in enumerate(tqdm(os.listdir(folder_path))):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(img_path):  \n",
    "            img = cv2.imread(img_path, -1)\n",
    "            # some images are broken\n",
    "            if img is not None:\n",
    "                if i == 0:\n",
    "                    print(\"The final image shape/size[hwc] is:\",img.shape)\n",
    "                    # store image shape in file\n",
    "                    with open(\"./data/meta.json\",\"w+\") as file:\n",
    "                        file.write(json.dumps({\"h\":img.shape[0],\"w\":img.shape[1],\"c\":img.shape[2]}))\n",
    "                class_categorical = None\n",
    "                class_numerical = None\n",
    "                class_encoded = None\n",
    "                if not df_categorical.empty:\n",
    "                    class_categorical = get_classes_from_id(int(filename[:-4]), df_categorical)\n",
    "                if not df_numerical.empty:\n",
    "                    class_numerical = get_classes_from_id(int(filename[:-4]), df_numerical)\n",
    "                if not df_encoded.empty:\n",
    "                    class_encoded = get_classes_from_id(int(filename[:-4]), df_encoded)\n",
    "                # need to check, if the class of the image is not null [aka. image would be one of the unwanted datapoints (e.g. class Schwarzbild)]\n",
    "                if (class_categorical is not None) or (class_numerical is not None) or (class_encoded is not None):\n",
    "                    if not df_categorical.empty:\n",
    "                        images_categorical.append([img.flatten(), *class_categorical.values])\n",
    "                    if not df_numerical.empty:\n",
    "                        images_numerical.append([img.flatten(), *class_numerical.values])\n",
    "                    if not df_encoded.empty:\n",
    "                        images_encoded.append([img.flatten(), *class_encoded.values])\n",
    "\n",
    "    #print(images_categorical)\n",
    "    #print(np.array(images_categorical, dtype=object).shape)\n",
    "\n",
    "    return_dfs = dict()\n",
    "\n",
    "    if not df_categorical.empty:\n",
    "        return_dfs[\"df_categorical\"] = pd.DataFrame(np.array(images_categorical, dtype=object), columns=column_names)\n",
    "    if not df_numerical.empty:\n",
    "        return_dfs[\"df_numerical\"] = pd.DataFrame(np.array(images_numerical, dtype=object), columns=column_names)\n",
    "    if not df_encoded.empty:\n",
    "        return_dfs[\"df_encoded\"] = pd.DataFrame(np.array(images_encoded, dtype=object), columns=column_names_encoded).drop(\"Species\", axis=1)\n",
    "\n",
    "    return return_dfs\n",
    "\n",
    "dfs = load_images_from_folder(\"./compressed_pictures/\", df_numerical=df_numerical)\n",
    "for df_name, df in dfs.items():\n",
    "    #print(images_categorical[\"data\"].to_numpy().shape)\n",
    "    if df_name == \"df_categorical\":\n",
    "        dfs[\"df_categorical\"].to_pickle(\"./data/images_df_categorical.pkl\")\n",
    "    if df_name == \"df_numerical\":\n",
    "        dfs[\"df_numerical\"].to_pickle(\"./data/images_df_numerical.pkl\")\n",
    "    if df_name == \"df_encoded\":\n",
    "        dfs[\"df_encoded\"].to_pickle(\"./data/images_df_encoded.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80c2e063-a871-4c4b-94e5-c00ed9195803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 1524994153.png -> (10, 75, 15, 116) to (65, 101, 3)\n",
    "# 2. 1521963463.png -> (10, 75, 16, 116) to (65, 100, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

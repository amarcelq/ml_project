{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66adbb1d-937b-4098-9e0e-9e4fba70c410",
   "metadata": {},
   "source": [
    "# Aufbereitung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772e326-d818-4530-b12e-50bcb78e42bc",
   "metadata": {},
   "source": [
    "Damit die Spektogramme überhaupt von Modellen genutzt werden können, müssen sie aufbereitet, (teilweise) bearbeitet und in einem gut verwendbarem Format abgespeichert werden.\n",
    "In unserem Fall besteht das aus den folgenden Schritten:\n",
    "1. Die Skala/Rahmen der Bilder entfernen\n",
    "2. Die Bilder runterskalieren\n",
    "3. (Optional) Filter oder andere Bildbearbeitungen anwenden\n",
    "4. Daten nach Klasse aussortieren\n",
    "5. Fledermausarten in Numerische Klassen umwandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699a1d3",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3375aaf9-6fe1-46b0-85ca-8573ed88db4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours as ENN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba1093-7cef-4b8f-ab34-3ddf7af31a92",
   "metadata": {},
   "source": [
    "#### Funktion zum entfernen eines übergebenen Randes.\n",
    "Wird letzendlich dafür genutzt die Skala und leeren Pixel um das Spektrogramm zu entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f91a1c2b-e309-4876-8e4b-d85b86f631e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_black_frame(img, frame):\n",
    "    return img[frame[0]:frame[1], frame[2]:frame[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ede773-3c8c-4bd3-b8e3-8476dd4ef41b",
   "metadata": {},
   "source": [
    "#### Bilder einlesen\n",
    "Hier wrid eine Lister aller Bilder im übergeordneten Ordner `../Bat_Orientation_Calls` erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ffa166b2-107b-4cb7-b681-ac11fdb3bf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4118 image paths.\n"
     ]
    }
   ],
   "source": [
    "folder_path = Path(\"../Bat_Orientation_Calls\")\n",
    "\n",
    "image_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".png\")]\n",
    "\n",
    "print(f\"Loaded {len(image_paths)} image paths.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd25f2c-56d2-4180-93c5-0384b87ac586",
   "metadata": {},
   "source": [
    "#### Bildbearbeitung\n",
    "Hier werden die eingelesenen Bilder zugeschnitten, runterskaliert und optional mit Filtern o.ä. bearbeitet und anschließend in `./compressed_pictures/` gespeichert.\n",
    "\n",
    "**Hinweis:** Zur Darstellung des Fortschritts wird tqdm genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5fdf4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb89321b8b74d499cad882779948e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image_path in tqdm(image_paths):\n",
    "    # Bild für OpenCV2 einlesen\n",
    "    img = cv2.imread(image_path, -1)\n",
    "\n",
    "    # Den Rand inkl. Skala entfernen\n",
    "    # Has to be hardcoded, cause not every image has the exact pixel perfect border/scale, so an automatic detection\n",
    "    #   trips up and the images would end with different sizes.\n",
    "    frame = (36, 251, 55, 388)\n",
    "    img = remove_black_frame(img, frame)\n",
    "\n",
    "    # Größe des neuen Bildes berechnen und skalieren\n",
    "    base_width = 128 # finale Breite des Bildes\n",
    "    height, width, channels = img.shape\n",
    "    aspect_ratio = width / height  # Width / Height\n",
    "    new_width = base_width\n",
    "    new_height = int(new_width / aspect_ratio)\n",
    "    img = cv2.resize(img, (new_width, new_height)) \n",
    "\n",
    "    # Alpha Channel entfernen, da dieser keine Informationen beinhaltet\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "    # Bild abspeichern\n",
    "    image_name = Path(image_path).name\n",
    "    cv2.imwrite(f\"./compressed_pictures/{image_name}\",img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a358be5-4040-4f0b-8f44-468a2c5469fa",
   "metadata": {},
   "source": [
    "#### Klassen aus CSV einlesen und anpassen\n",
    "Hier werden die Klassen aus den CSV's eingelesen und anschließend unerwünschte Klassen entfernt.\n",
    "Kriterien für die Unerwünschtheit sind folgende:\n",
    "- Klasse ist 'Fledermaus nicht bestimmbar' oder 'Schwarzbild', da diese keine sinnvollen Informationen sind\n",
    "- Klasse (bzw. Bild-ID) ist ein Duplikat\n",
    "- Klasse hat weniger als 60 Samples, da diese schlichweg ungeeignet für das Trainieren eines Modells sind\n",
    "\n",
    "Anschließend liegt ein DataFrame mit den ID-Klassen Paaren vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff50f2f3-f3df-44f6-9514-3150188d142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 40 entries cause they were duplicates.\n",
      "Dropped 180 entries cause they were of class 'Fledermaus nicht bestimmbar'.\n",
      "Dropped 180 entries cause they were of class 'Schwarzbild'.\n",
      "Dropped 400 in total.\n",
      "DataFrame as 3723 different entries (Images).\n"
     ]
    }
   ],
   "source": [
    "# Reads in classes from all csv's, and edits it\n",
    "def classes_csv_to_df(file_paths: list, delimiter=\";\") -> pd.DataFrame:\n",
    "    df_all = pd.DataFrame()\n",
    "    for file_path in file_paths:\n",
    "        df_current = pd.read_csv(file_path, delimiter=delimiter)\n",
    "        df_all = pd.concat([df_all, df_current])\n",
    "\n",
    "    df_all = df_all.reset_index(drop=True)\n",
    "    df_all = df_all.drop_duplicates()\n",
    "    df_all.drop(\"Filename\", axis=1, inplace=True)\n",
    "    df_all = remove_unwanted_datapoints(df_all)\n",
    "    df_all.replace(\"&Mausohr \", \"Mausohr\", inplace=True)\n",
    "    df_all = remove_less_sample_classes(df_all, 60)\n",
    "    \n",
    "    #df_all.drop(\"Species\", axis=1, inplace=True)\n",
    "    return df_all\n",
    "\n",
    "# Removes all duplicate entries and entries of type 'Fledermaus nicht bestimmbar' or 'Schwarzbild'\n",
    "def remove_unwanted_datapoints(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ids = set()\n",
    "    duplicated_ids = set()\n",
    "    for row in df.iterrows():\n",
    "        id = row[1][\"ID\"]\n",
    "        if id in ids:\n",
    "            duplicated_ids.add(id)\n",
    "        ids.add(id)\n",
    "\n",
    "    # print how many were droppes\n",
    "    print(f\"Dropped {len(duplicated_ids)} entries cause they were duplicates.\")\n",
    "    print(f\"Dropped {len(df[df['Species'] == 'Fledermaus nicht bestimmbar']) } entries cause they were of class 'Fledermaus nicht bestimmbar'.\")\n",
    "    print(f\"Dropped {len(df[df['Species'] == 'Schwarzbild'])} entries cause they were of class 'Schwarzbild'.\")\n",
    "    print(f\"Dropped {len(duplicated_ids)+len(df[df['Species'] == 'Fledermaus nicht bestimmbar'])+len(df[df['Species'] == 'Schwarzbild'])} in total.\")\n",
    "    \n",
    "    return df[~((df['ID'].isin(duplicated_ids)) |\n",
    "            (df['Species'].isin(['Fledermaus nicht bestimmbar', 'Schwarzbild'])))]\\\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "# Remove all classes with less than 60 entries\n",
    "def remove_less_sample_classes(df: pd.DataFrame, min_samples: int) -> pd.DataFrame:\n",
    "    class_distribution = df['Species'].value_counts()\n",
    "    valid_classes = class_distribution[class_distribution >= min_samples].index\n",
    "    return df[df['Species'].isin(valid_classes)]\n",
    "\n",
    "## execute functions\n",
    "\n",
    "df = classes_csv_to_df([\"../Auswertung_20220524.csv\",\"../LMU_20180326_class.csv\", \"../LMU_20180505_classified.csv\"])\n",
    "\n",
    "print(f\"DataFrame as {len(df)} different entries (Images).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e03463-eb8e-4bbe-a954-b3eac563df6d",
   "metadata": {},
   "source": [
    "#### Klassen konvertieren\n",
    "Hier werden die Labels der Klassen von den Fledermausnamen in numerische Label konvertiert. Funktionen für categorische und One-Hot Encodede Labels gibt es auch, werden von uns aber nicht genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a79abd20-5d7f-4dbf-8075-997094a355a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Bartfledermaus', 1: 'Bechsteinfledermaus', 2: 'Fransenfledermaus', 3: 'Langohrfledermaus', 4: 'Mausohr', 5: 'Wasserfledermaus'}\n"
     ]
    }
   ],
   "source": [
    "def categorical_classes(df: pd.DataFrame, column_name: str):\n",
    "    # df passed as call by reference\n",
    "    df_copy = df.copy()\n",
    "    df_copy[column_name] = df_copy[column_name].astype('category')\n",
    "    return df_copy\n",
    "\n",
    "def numerical_classes(df: pd.DataFrame, column_name: str) -> tuple[pd.DataFrame, dict[int|str]]:\n",
    "    df_copy = df.copy()\n",
    "    df_copy[column_name] = df_copy[column_name].astype('category')\n",
    "    class_mapping = dict(enumerate(df_copy[column_name].cat.categories))\n",
    "    df_copy[column_name] = df_copy[column_name].cat.codes\n",
    "    return df_copy, class_mapping\n",
    "\n",
    "def encode_classes(df: pd.DataFrame, column_name: str):\n",
    "    encoded_classes = pd.get_dummies(df[column_name])\n",
    "    df = df.join(encoded_classes)\n",
    "    return df\n",
    "\n",
    "df_numerical, class_mapping = numerical_classes(df, \"Species\")\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf42251-535f-4e21-a39b-84e642b6908a",
   "metadata": {},
   "source": [
    "#### Klassen Mapping speichern\n",
    "Damit die nun numerischen Klassen noch zuordbar sind, wird das Mapping von Wert-Klasse in der Datei `./data/class_mapping.csv` gespeichert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83f1f1d4-eace-40f6-989a-efad084e7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_mapping_to_csv(class_mapping_dict: dict) -> None: \n",
    "    with open('./data/class_mapping.csv', 'w') as class_mapping_csv:  \n",
    "        writer = csv.writer(class_mapping_csv)\n",
    "        for key, value in class_mapping_dict.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "class_mapping_to_csv(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a160234-f81c-42e3-9220-4d770289740d",
   "metadata": {},
   "source": [
    "#### Klassenverteilung plotten\n",
    "Hier wird die Verteilugn der Klassen geplottet und unter `./data/class_distribution` gespeichert. Wie zu sehen ist, sind sie stark ungleichmäßig verteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ae1e66e-93e1-4dcd-9f31-bb1ec8f578a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_class_distribution(df: pd.DataFrame):\n",
    "    fig = plt.figure(figsize=(3,3))\n",
    "    df['Species'].value_counts().plot.bar()\n",
    "    plt.title('Count Distribution')\n",
    "    plt.savefig(\"./data/class_distribution\")\n",
    "    plt.show(fig)\n",
    "    \n",
    "plot_class_distribution(df_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6aa44a-ec8c-44a7-87c4-e7f5b2a2992e",
   "metadata": {},
   "source": [
    "#### Bilder aus ordner lesen, Klasse zuordnen und in DataFrame wandeln\n",
    "In diesen Funktionen werden die bearbeiten Bilder geladen, ihnen die richtige Klasse zugeordnet und anschließend in einem DataFrame gespeichert.\n",
    "**Hinweis:** Hier wird erneut tqdm verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "41b8d4bf-f679-4e88-acbf-082cd4f6834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given an ID return the according class\n",
    "def get_classes_from_id(id: int, df: pd.DataFrame) -> pd.Series:\n",
    "    for row in df.iterrows():\n",
    "        if id == row[1][\"ID\"]:\n",
    "            return row[1].drop(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdc0b485-194e-43e3-8df8-6a4f8e80204c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5664451504774109a01a3ac7fc349345",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final image shape/size[hwc] is: (82, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_images_from_folder(folder_path: str, df_categorical=pd.DataFrame(), df_numerical=pd.DataFrame(), df_encoded=pd.DataFrame()) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    # Sicherstellen, dass mindestens ein Dataframe gegeben ist\n",
    "    if not df_categorical.empty and not df_numerical.empty and not df_encoded.empty:\n",
    "        raise ValueError(\"You have to define at least one Dataframe.\")\n",
    "        \n",
    "    images_categorical = list()\n",
    "    images_numerical = list()\n",
    "    images_encoded = list()\n",
    "    column_names = [\"data\", \"Species\"]\n",
    "    column_names_encoded = [\"data\"] + list(df_encoded.columns)[1:]\n",
    "    \n",
    "    for i, filename in enumerate(tqdm(os.listdir(folder_path))):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(img_path):  \n",
    "            # read in image\n",
    "            img = cv2.imread(img_path, -1)\n",
    "            # some images are broken\n",
    "            if img is not None:\n",
    "                # for the first image, save the size of the image to a seperate file, which makes it easier for other\n",
    "                #   notebooks to get the image size & channels\n",
    "                if i == 0:\n",
    "                    print(\"The final image shape/size[hwc] is:\",img.shape)\n",
    "                    # store image shape in file\n",
    "                    with open(\"./data/meta.json\",\"w+\") as file:\n",
    "                        file.write(json.dumps({\"h\":img.shape[0],\"w\":img.shape[1],\"c\":img.shape[2]}))\n",
    "                \n",
    "                # Find the Class for the Image based on ID and type of given Class representation\n",
    "                class_categorical = None\n",
    "                class_numerical = None\n",
    "                class_encoded = None\n",
    "                if not df_categorical.empty:\n",
    "                    class_categorical = get_classes_from_id(int(filename[:-4]), df_categorical)\n",
    "                if not df_numerical.empty:\n",
    "                    class_numerical = get_classes_from_id(int(filename[:-4]), df_numerical)\n",
    "                if not df_encoded.empty:\n",
    "                    class_encoded = get_classes_from_id(int(filename[:-4]), df_encoded)\n",
    "                    \n",
    "                # need to check, if the class of the image is not null [aka. image would be one of the unwanted\n",
    "                #   datapoints (e.g. class Schwarzbild)]\n",
    "                if (class_categorical is not None) or (class_numerical is not None) or (class_encoded is not None):\n",
    "                    if not df_categorical.empty:\n",
    "                        images_categorical.append([img.flatten(), *class_categorical.values])\n",
    "                    if not df_numerical.empty:\n",
    "                        images_numerical.append([img.flatten(), *class_numerical.values])\n",
    "                    if not df_encoded.empty:\n",
    "                        images_encoded.append([img.flatten(), *class_encoded.values])\n",
    "\n",
    "    return_dfs = dict()\n",
    "\n",
    "    if not df_categorical.empty:\n",
    "        return_dfs[\"df_categorical\"] = pd.DataFrame(np.array(images_categorical, dtype=object), columns=column_names)\n",
    "    if not df_numerical.empty:\n",
    "        return_dfs[\"df_numerical\"] = pd.DataFrame(np.array(images_numerical, dtype=object), columns=column_names)\n",
    "    if not df_encoded.empty:\n",
    "        return_dfs[\"df_encoded\"] = pd.DataFrame(np.array(images_encoded, dtype=object), columns=column_names_encoded).drop(\"Species\", axis=1)\n",
    "\n",
    "    return return_dfs\n",
    "\n",
    "# use function\n",
    "dfs = load_images_from_folder(\"./compressed_pictures/\", df_numerical=df_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9ac98-0acb-4f49-89ae-599a8ee3882e",
   "metadata": {},
   "source": [
    "#### DataFrame als Pickles Datei speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "176f3da1-7452-4642-abfa-f443937208dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in dfs.items():\n",
    "    if df_name == \"df_categorical\":\n",
    "        dfs[\"df_categorical\"].to_pickle(\"./data/images_df_categorical.pkl\")\n",
    "    if df_name == \"df_numerical\":\n",
    "        dfs[\"df_numerical\"].to_pickle(\"./data/images_df_numerical.pkl\")\n",
    "    if df_name == \"df_encoded\":\n",
    "        dfs[\"df_encoded\"].to_pickle(\"./data/images_df_encoded.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731c0b4",
   "metadata": {},
   "source": [
    "#### Daten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b80ee6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dfs[\"df_numerical\"]\n",
    "X, y = data['data'], data['Species']\n",
    "X, y = np.stack(X).astype(np.uint8), y.to_numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47f6e4",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "Aufgrund der sehr ungleich verteilten Klassen haben wir mit Oversampling (Adasyn) und einer Kombination von Under- und Oversampling (Smoteen) experimentiert. Alleiniges undersampling haben wir als nicht sinnvoll erachtet, da selbst die größten Klassen recht wenig Datenpunkte enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2581b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernt Klassen >= 60 Samples\n",
    "#data = data[data['Species'].isin(counts[counts>=60].index)]\n",
    "# Entfernt Klassen >= 200 Samples\n",
    "#data = data[data['Species'].isin(counts[counts>=200].index)]\n",
    "# Entfernt Klassen >= 500 Samples\n",
    "#data = data[data['Species'].isin(counts[counts>=500].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e41b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample(resampler) -> tuple[np.array, np.array]:\n",
    "    X, y = data['data'], data['Species']\n",
    "    X, y = np.stack(X).astype(np.uint8), y.to_numpy().astype(np.uint8)\n",
    "\n",
    "    _, dist = np.unique(y, return_counts=True)\n",
    "    print(f\"{resampler} before res:\",dist)\n",
    "\n",
    "    X_resampled, y_resampled = resampler.fit_resample(X, y)\n",
    "\n",
    "    _, dist = np.unique(y_resampled, return_counts=True)\n",
    "    print(f\"{resampler} after res:\", dist)\n",
    "\n",
    "    # shuffle ist nötig, da die synthetischen Daten nach Labels geordnet sind (random_state für bessere nachvollziehbarkeit)\n",
    "    X_resampled, y_resampled = shuffle(X_resampled, y_resampled, random_state=1)\n",
    "\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79c564ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/meta.json\",\"r\") as file:  # get metadata for images from file generated from import nb\n",
    "    image_meta = json.load(file)\n",
    "image_shape = (image_meta['h'],image_meta['w'],image_meta['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522df056",
   "metadata": {},
   "source": [
    "**Hinweis:** Die resample_after_split-Funktion wurde benutzt um die Mengen nach dem KFold-Split einzeln zu resamplen, also z. B. nur train. Es ist der vollständigkeitshalber an dieser Stelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac1c6522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_after_split(X,y, resampler) -> tuple[np.array, np.array]:\n",
    "    n, h, w, _ = X.shape\n",
    "    X = X.reshape((n, h * w * 3))\n",
    "    _, dist = np.unique(y, return_counts=True)\n",
    "    print(f\"{resampler} before res:\",dist)\n",
    "\n",
    "    X_resampled, y_resampled = resampler.fit_resample(X, y)\n",
    "\n",
    "    _, dist = np.unique(y_resampled, return_counts=True)\n",
    "    print(f\"{resampler} after res:\", dist)\n",
    "\n",
    "    X_resampled = X_resampled.reshape((-1,) + image_shape)\n",
    "\n",
    "    # shuffle ist nötig, da die synthetischen Daten nach Labels geordnet sind (random_state für bessere nachvollziehbarkeit)\n",
    "    X_resampled, y_resampled = shuffle(X_resampled, y_resampled, random_state=1)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# resample_after_split = SMOTEENN(sampling_strategy='all',enn=ENN(kind_sel='mode',n_neighbors=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcf76f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADASYN() before res: [2104  270  866  290  123   69]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# oversampling\u001b[39;00m\n\u001b[1;32m      2\u001b[0m adasyn \u001b[38;5;241m=\u001b[39m ADASYN()\n\u001b[0;32m----> 3\u001b[0m X_adasyn, y_adasyn \u001b[38;5;241m=\u001b[39m \u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43madasyn\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m, in \u001b[0;36mresample\u001b[0;34m(resampler)\u001b[0m\n\u001b[1;32m      5\u001b[0m _, dist \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresampler\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m before res:\u001b[39m\u001b[38;5;124m\"\u001b[39m,dist)\n\u001b[0;32m----> 8\u001b[0m X_resampled, y_resampled \u001b[38;5;241m=\u001b[39m \u001b[43mresampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m _, dist \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(y_resampled, return_counts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresampler\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m after res:\u001b[39m\u001b[38;5;124m\"\u001b[39m, dist)\n",
      "File \u001b[0;32m~/Documents/main_data_cloud/oth/s3/ml/prj/ml_project/venv/lib/python3.11/site-packages/imblearn/base.py:208\u001b[0m, in \u001b[0;36mBaseSampler.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Resample the dataset.\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    The corresponding label of `X_resampled`.\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m--> 208\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/main_data_cloud/oth/s3/ml/prj/ml_project/venv/lib/python3.11/site-packages/imblearn/base.py:112\u001b[0m, in \u001b[0;36mSamplerMixin.fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    106\u001b[0m X, y, binarize_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_X_y(X, y)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy_ \u001b[38;5;241m=\u001b[39m check_sampling_strategy(\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msampling_strategy, y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampling_type\n\u001b[1;32m    110\u001b[0m )\n\u001b[0;32m--> 112\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_resample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m y_ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m     label_binarize(output[\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y)) \u001b[38;5;28;01mif\u001b[39;00m binarize_y \u001b[38;5;28;01melse\u001b[39;00m output[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    116\u001b[0m )\n\u001b[1;32m    118\u001b[0m X_, y_ \u001b[38;5;241m=\u001b[39m arrays_transformer\u001b[38;5;241m.\u001b[39mtransform(output[\u001b[38;5;241m0\u001b[39m], y_)\n",
      "File \u001b[0;32m~/Documents/main_data_cloud/oth/s3/ml/prj/ml_project/venv/lib/python3.11/site-packages/imblearn/over_sampling/_adasyn.py:176\u001b[0m, in \u001b[0;36mADASYN._fit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    173\u001b[0m X_class \u001b[38;5;241m=\u001b[39m _safe_indexing(X, target_class_indices)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[0;32m--> 176\u001b[0m nns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m[:, \u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# The ratio is computed using a one-vs-rest manner. Using majority\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# in multi-class would lead to slightly different results at the\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;66;03m# cost of introducing a new parameter.\u001b[39;00m\n\u001b[1;32m    180\u001b[0m n_neighbors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_\u001b[38;5;241m.\u001b[39mn_neighbors \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Documents/main_data_cloud/oth/s3/ml/prj/ml_project/venv/lib/python3.11/site-packages/sklearn/neighbors/_base.py:859\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    857\u001b[0m         kwds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meffective_metric_params_\n\u001b[0;32m--> 859\u001b[0m     chunked_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpairwise_distances_chunked\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_X\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreduce_func\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreduce_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meffective_metric_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mball_tree\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkd_tree\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n",
      "File \u001b[0;32m~/Documents/main_data_cloud/oth/s3/ml/prj/ml_project/venv/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:2018\u001b[0m, in \u001b[0;36mpairwise_distances_chunked\u001b[0;34m(X, Y, reduce_func, metric, n_jobs, working_memory, **kwds)\u001b[0m\n\u001b[1;32m   2016\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2017\u001b[0m     X_chunk \u001b[38;5;241m=\u001b[39m X[sl]\n\u001b[0;32m-> 2018\u001b[0m D_chunk \u001b[38;5;241m=\u001b[39m \u001b[43mpairwise_distances\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (X \u001b[38;5;129;01mis\u001b[39;00m Y \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m PAIRWISE_DISTANCE_FUNCTIONS\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   2020\u001b[0m     metric, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2021\u001b[0m ) \u001b[38;5;129;01mis\u001b[39;00m euclidean_distances:\n\u001b[1;32m   2022\u001b[0m     \u001b[38;5;66;03m# zeroing diagonal, taking care of aliases of \"euclidean\",\u001b[39;00m\n\u001b[1;32m   2023\u001b[0m     \u001b[38;5;66;03m# i.e. \"l2\"\u001b[39;00m\n\u001b[1;32m   2024\u001b[0m     D_chunk\u001b[38;5;241m.\u001b[39mflat[sl\u001b[38;5;241m.\u001b[39mstart :: _num_samples(X) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/main_data_cloud/oth/s3/ml/prj/ml_project/venv/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:2196\u001b[0m, in \u001b[0;36mpairwise_distances\u001b[0;34m(X, Y, metric, n_jobs, force_all_finite, **kwds)\u001b[0m\n\u001b[1;32m   2193\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m distance\u001b[38;5;241m.\u001b[39msquareform(distance\u001b[38;5;241m.\u001b[39mpdist(X, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[1;32m   2194\u001b[0m     func \u001b[38;5;241m=\u001b[39m partial(distance\u001b[38;5;241m.\u001b[39mcdist, metric\u001b[38;5;241m=\u001b[39mmetric, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m-> 2196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_parallel_pairwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/main_data_cloud/oth/s3/ml/prj/ml_project/venv/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:1766\u001b[0m, in \u001b[0;36m_parallel_pairwise\u001b[0;34m(X, Y, func, n_jobs, **kwds)\u001b[0m\n\u001b[1;32m   1763\u001b[0m X, Y, dtype \u001b[38;5;241m=\u001b[39m _return_float_dtype(X, Y)\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(n_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;66;03m# enforce a threading backend to prevent data communication overhead\u001b[39;00m\n\u001b[1;32m   1769\u001b[0m fd \u001b[38;5;241m=\u001b[39m delayed(_dist_wrapper)\n",
      "File \u001b[0;32m~/Documents/main_data_cloud/oth/s3/ml/prj/ml_project/venv/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:310\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[0;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meuclidean_distances\u001b[39m(\n\u001b[1;32m    235\u001b[0m     X, Y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, Y_norm_squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, X_norm_squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    236\u001b[0m ):\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03m    Compute the distance matrix between each pair from a vector array X and Y.\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;03m           [1.41421356]])\u001b[39;00m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 310\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X_norm_squared \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    313\u001b[0m         X_norm_squared \u001b[38;5;241m=\u001b[39m check_array(X_norm_squared, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/main_data_cloud/oth/s3/ml/prj/ml_project/venv/lib/python3.11/site-packages/sklearn/metrics/pairwise.py:173\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, copy)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[1;32m    166\u001b[0m         X,\n\u001b[1;32m    167\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[1;32m    172\u001b[0m     )\n\u001b[0;32m--> 173\u001b[0m     Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precomputed:\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/Documents/main_data_cloud/oth/s3/ml/prj/ml_project/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:957\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    951\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    952\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    953\u001b[0m             \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m    954\u001b[0m         )\n\u001b[1;32m    956\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m--> 957\u001b[0m         \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m            \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_samples \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    965\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n",
      "File \u001b[0;32m~/Documents/main_data_cloud/oth/s3/ml/prj/ml_project/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:118\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# First try an O(n) time, O(1) space solution for the common case that\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;66;03m# everything is finite; fall back to O(n) space `np.isinf/isnan` or custom\u001b[39;00m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;66;03m# Cython implementation to prevent false positives and provide a detailed\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# error message.\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 118\u001b[0m     first_pass_isfinite \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39misfinite(\u001b[43mxp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/main_data_cloud/oth/s3/ml/prj/ml_project/venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:2313\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2310\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2314\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/main_data_cloud/oth/s3/ml/prj/ml_project/venv/lib/python3.11/site-packages/numpy/core/fromnumeric.py:88\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# oversampling\n",
    "adasyn = ADASYN()\n",
    "X_adasyn, y_adasyn = resample(adasyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1048c4c",
   "metadata": {},
   "source": [
    "**Hinweis:** SMOTETomek haben wir nicht weiter beachtet, da die Klassenverteilung sehr stark Adasyn ähnelte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d6478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# under- und oversampling\n",
    "somteenn = SMOTEENN()\n",
    "X_somteenn, y_somteenn = resample(somteenn)\n",
    "smotetomek = SMOTETomek()\n",
    "X_smotetomek, y_smotetomek = resample(smotetomek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0013c3",
   "metadata": {},
   "source": [
    "**Hinweis:** Die Tests wurden auf unserem Hauptmodel CNN durchgeführt (s. CNN-Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe768cc",
   "metadata": {},
   "source": [
    "|Durchführung|Avg. Test Acc|\n",
    "|:-|-:|\n",
    "| Kein Resample, top 6 classes (> 60), cnn  | 79% |\n",
    "| Kein Resample, top 4 classes (>200), cnn  | 83% |\n",
    "| Kein Resample, top 2 classes (>500), cnn  | 87% |\n",
    "|---------------|\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=3)), top 6 classes (> 60), cnn   | 63% |\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=3)), top 4 classes (>200), cnn   | 80% |\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=3)), top 2 classes (>500), cnn   | 87% |\n",
    "|---------------|\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=7)), top 6 classes (> 60), cnn   | 68% |\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=7)), top 4 classes (>200), cnn   | 79% |\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=7)), top 2 classes (>500), cnn   | 86% |\n",
    "|---------------|\n",
    "| Train Resample (Smotteen(all,enn=Mode,n=3)), top 6 classes (> 60), cnn   | 73% |\n",
    "| Train Resample (Smotteen(all,enn=Mode,n=3)), top 4 classes (>200), cnn   | 81% |\n",
    "| Train Resample (Smotteen(all,enn=Mode,n=3)), top 2 classes (>500), cnn   | 88% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31063780",
   "metadata": {},
   "source": [
    "**Erbenis:** Man sieht, dass das Resampling leider in der Form keinen Vorteil bringt daher haben wir die Idee verworfen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9dda23",
   "metadata": {},
   "source": [
    "**Hinweis:** Wir halten die ungleiche Klassenverteilung bzw. den Mangel an Samples einiger Klassen für den Hebel mit dem meisten Verbesserungspotential. (Wenn wir die Testdaten mit resampeln erhalten wir einen Score von bis zu 92% s. unten)\n",
    "Mit mehr Zeit/Domänenwissen wären folgende Dinge denkbar:\n",
    "* Echte Daten vor allem der unterrepräsentierten Daten sammeln\n",
    "* Synthetische Daten erzeugen, welche sehr nah den den Orginaldaten sind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd70bd7",
   "metadata": {},
   "source": [
    "#### Chunks um RAM zu sparen\n",
    "\n",
    "Da wir extrem große RAM Probleme hatten haben wir mit Aufteilung der Daten in Chunks experimentiert, aber die Idee dann wieder verworfen, da nach der Verkleinerung der Bilder es auch ohne geklappt hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abae3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df_equal_class_distribution(df, batch_size):\n",
    "    \n",
    "    df['temp_id'] = range(len(df))\n",
    "    \n",
    "    num_batches = int(np.ceil(len(df) / batch_size))\n",
    "    \n",
    "    grouped = df.groupby('Species', group_keys=False)\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        chunk = pd.DataFrame(columns=df.columns)\n",
    "        for _, group in grouped:\n",
    "            num_samples = int(batch_size * len(group) / len(df))\n",
    "            sample_indices = np.random.choice(group['temp_id'], size=num_samples, replace=False)\n",
    "            chunk = pd.concat([chunk, df[df['temp_id'].isin(sample_indices)]])\n",
    "        chunk = chunk.drop('temp_id', axis=1)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "chunks_with_same_dist = split_df_equal_class_distribution(data, batch_size)\n",
    "del data\n",
    "classes = chunks_with_same_dist[0][\"Species\"].unique()\n",
    "number_of_classes = classes.size\n",
    "most_x_in_one_class = chunks_with_same_dist[0][\"Species\"].value_counts().iloc[0]\n",
    "\n",
    "def resample(resampler) -> tuple[np.array, np.array]:\n",
    "    # 0.3 als Puffer, weil sich der Output der verschiedenen Resampler ja unterscheidet\n",
    "    array_size = int(most_x_in_one_class * number_of_classes * (len(chunks_with_same_dist) + 0.3))\n",
    "    # 216432 war die Länge des Vektors der nicht verkleinerten Bilder\n",
    "    X = np.empty((array_size, 216432), dtype=np.uint8)\n",
    "    y = np.empty((array_size), dtype=np.uint8)\n",
    "\n",
    "    current_index = 0\n",
    "    for chunk in chunks_with_same_dist:\n",
    "        X_batch, y_batch = chunk['data'], chunk['Species']\n",
    "        X_batch, y_batch = np.stack(X_batch).astype(np.uint8), y_batch.astype(np.uint8)\n",
    "        X_resampled, y_resampled = resampler.fit_resample(X_batch, y_batch)\n",
    "        num_samples = X_resampled.shape[0]\n",
    "        X[current_index:current_index + num_samples] = X_resampled.astype(np.uint8)\n",
    "        y[current_index:current_index + num_samples] = y_resampled.astype(np.uint8)\n",
    "        current_index += num_samples\n",
    "\n",
    "    X.resize((current_index, X.shape[1]))\n",
    "    y.resize(current_index)\n",
    "    print(f\"{resampler}: \", pd.Series(y, dtype=pd.UInt8Dtype()).value_counts())\n",
    "\n",
    "    return X, y\n",
    "\n",
    "num_samples = (len(X) * 0.8)\n",
    "chunk_size = 1000\n",
    "for i in range(0, num_samples, chunk_size):\n",
    "    X[i:i+chunk_size], y[i:i+chunk_size] # Hier ist der Code abgeschnitten, da nur die Idee erläutert werden soll "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f9f50",
   "metadata": {},
   "source": [
    "#### Bilder ausgeben\n",
    "\n",
    "Für uns waren die Unterscheidung zwischen den Bildern der meisten Klassen nicht möglich, weshalb Domänenwissen ein sehr starken Vorteil (und in unserem Projekt auch teilweise Problem) darstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab95a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2016888",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20  # Anzahl der gezeigten Bilder\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    if image_shape[2] == 1:\n",
    "        plt.imshow((X[i]*255.).astype('uint8'), cmap='inferno', vmin=0, vmax=255) # for grayscale images\n",
    "        plt.set_suptitle(f\"{y[i]}\")\n",
    "    elif image_shape[2] == 4 or image_shape[2] == 3:\n",
    "        plt.imshow(cv2.cvtColor((X[i]).astype('uint8'),cv2.COLOR_BGR2RGB)) # for rgb images\n",
    "        ax.title.set_text(f\"{y[i]}\")\n",
    "    #plt.color()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.savefig(\"./pics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d0e33-0ba1-4f52-b967-327d9e732c90",
   "metadata": {},
   "source": [
    "# Anmerkungen zu getesteten Bildbearbeitungen\n",
    "FÜr den ersten Teil der Bildaufbereitung werden hier keine Filter ö.ä. Bildbearbeitungen verwendet.\n",
    "Allerdings haben wir im Laufe der Bearbeitung verschiedene Filter u.ä. ausprobiert, welche inkl. ihres Test Accuracy Wertes gelistet sind.\n",
    "\n",
    "**Hinweis:** Die Test Accuracy Werte entsprechen denen unseres Modells, auf Basis der mit Adasyn vollständig geresampleten (auch Testdaten) Bilder. Dies hat den Grund, dass die Tests durchgeführt wurden bevor wir den Fehler entdeckt haben und die Zeit für neue Tests nicht mehr ausgereicht hat. Somit sind sie ca 10-30% höher, als auf den ungresampleten Bildern. Mehr dazu im `cnn.ipynb`.\n",
    "\n",
    "\n",
    "|Bearbeitung (In Anwendungsreihenfolge)|Test Accuracy|\n",
    "|:-|-:|\n",
    "|Turn Pictrues into single channel grayscale|78%|\n",
    "|Turn Pictrues into single channel grayscale + fastNLMeansDenoising|78%|\n",
    "|fastNLMeansDenoising after downscaling|80%|\n",
    "|Bilateral Filter|82%|\n",
    "|Color Histogram Equalization + Contrast(*1.7) + Brightness(-150) + Bilateral Filter|82%\n",
    "|fastNLMeansDenoising before downscaling|83%|\n",
    "|Outlinedetection Conv2D Filter + Bilateral Filter + fastNLMeansDenoising + Contrast(*1.7) + Brightness(-150) + Sobel Conv2D Filter| 88%|\n",
    "|Contrast(*1.5) + Brightness(-150)|88%|\n",
    "|Contrast(*1.7) + Brightness(-100)|89%|\n",
    "|Contrast(*1.7) + Brightness(-150)|89%|\n",
    "|Sobel Conv2D Filter + Bilateral Filter + fastNLMeansDenoising + Bilateral Filter + Contrast(*1.7) + Brightness(-150)| 89%|\n",
    "|Bilateral Filter + Contrast(*1.7) + Brightness(-100)|90%|\n",
    "|Bilateral Filter + Contrast(*1.7) + Brightness(-150)|90%|\n",
    "|Outlinedetection Conv2D Filter + Bilateral Filter + fastNLMeansDenoising + Bilateral Filter + Contrast(*1.7) + Brightness(-150)| 91%|\n",
    "|Outlinedetection Conv2D Filter + Bilateral Filter + fastNLMeansDenoising + Bilateral Filter + Contrast(*1.7) + Brightness(-150) + Saturation=255| 91%|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b855d-ad57-4094-b2e9-797886fd8756",
   "metadata": {},
   "source": [
    "Eine Veranschaulichung dieser und weitere Filter/Bildbearbeitungen ist im folgenden zu sehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c1b8e-236d-4253-a985-a261d1520a34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "##### spektrogramme anzeigen\n",
    "def show_images(images, titles):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, (img, title) in enumerate(zip(images, titles), 1):\n",
    "        plt.subplot(8, 4, i)\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.savefig(\"denoise_comaprison.png\")\n",
    "    plt.show()\n",
    "\n",
    "# color distribution to show differences in intesities etc\n",
    "def show_hist(images, titles):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, (image, title) in enumerate(zip(images, titles), 1):\n",
    "        plt.subplot(8, 4, i)\n",
    "        plt.title(title)\n",
    "        #plt.axis('off')\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        hist_r = cv2.calcHist([image_rgb], [0], None, [256], [0, 256]) / (image_size/3)\n",
    "        hist_g = cv2.calcHist([image_rgb], [1], None, [256], [0, 256]) / (image_size/3)\n",
    "        hist_b = cv2.calcHist([image_rgb], [2], None, [256], [0, 256]) / (image_size/3)\n",
    "\n",
    "        plt.plot(hist_r, color='red', label='Red')\n",
    "        plt.plot(hist_g, color='green', label='Green')\n",
    "        plt.plot(hist_b, color='blue', label='Blue')\n",
    "        plt.xlabel('Pixel Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.xlim([0, 255])\n",
    "        plt.ylim([0, 0.3])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"denoise_comaprison_hist.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Laden eines Beispielbildes\n",
    "image = dfs['df_numerical']['data'][0]\n",
    "image_size = image.size\n",
    "with open(\"./data/meta.json\",\"r\") as file:  # get metadata for images from file generated from import nb\n",
    "    image_meta = json.load(file) \n",
    "image_shape = (image_meta['h'],image_meta['w'],image_meta['c']) # hwc\n",
    "image= image.reshape(image_shape)\n",
    "image = cv2.cvtColor(image,cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "# Gaußscher Weichzeichner\n",
    "gaussian_blur = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "# Mittelwertfilter\n",
    "mean_blur = cv2.blur(image, (5, 5))\n",
    "\n",
    "# Medianfilter\n",
    "median_blur = cv2.medianBlur(image, 5)\n",
    "\n",
    "# Bilateralfilter\n",
    "bilateral_blur = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "\n",
    "# Bewegungsunschärfe\n",
    "kernel_motion_blur = np.zeros((5, 5))\n",
    "kernel_motion_blur[:, int((5-1)/2)] = 1\n",
    "kernel_motion_blur /= 5\n",
    "motion_blur = cv2.filter2D(image, -1, kernel_motion_blur)\n",
    "\n",
    "# predcit based on autoencoder\n",
    "#autoencoder = keras.saving.load_model('../autoencoder_files/autoencoder_cnn_denoise_v7.keras')\n",
    "#imagelist = np.asarray([image]).astype('float32')/255.\n",
    "#autoencoder_img = (autoencoder.predict(imagelist)[0]*255.).astype('uint8')\n",
    "\n",
    "# nl means denosing gray\n",
    "fast_nl_grey = cv2.fastNlMeansDenoising(image, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "# nl means denosing color\n",
    "fast_nl_color = cv2.fastNlMeansDenoisingColored(image, None, 15,15, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "# contrast/birghness\n",
    "brightness = 1 \n",
    "contrast = 1.5\n",
    "c_b = cv2.addWeighted(image, contrast, np.zeros(image.shape, image.dtype), 0, brightness)\n",
    "\n",
    "\n",
    "# cb + nlmeans\n",
    "brightness = 0 \n",
    "contrast = 1.5\n",
    "cb_nl = cv2.fastNlMeansDenoising(cv2.addWeighted(image, contrast, np.zeros(image.shape, image.dtype), 0, brightness), None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "\n",
    "# + cont\n",
    "cont1 = cv2.addWeighted(image, 1.5, np.zeros(image.shape, image.dtype), 0, -150)\n",
    "\n",
    "cont2 = cv2.addWeighted(image, 1.7, np.zeros(image.shape, image.dtype), 0, -150)\n",
    "\n",
    "cont3 = cv2.addWeighted(image, 2, np.zeros(image.shape, image.dtype), 0, -150)\n",
    "\n",
    "# bil + cont\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "bil_cont1 = cv2.addWeighted(img, 1.5, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "bil_cont2 = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "bil_cont3 = cv2.addWeighted(img, 2, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "# bil+c2+b-150+bil+c2+b-200\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 2, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "bl_cb_bil_cb1 = cv2.addWeighted(img, 2, np.zeros(img.shape, img.dtype), 0, -200)\n",
    "\n",
    "#sharpen kernel\n",
    "kernel = np.array([[-1, -1, -1],\n",
    "                       [-1, 9, -1],\n",
    "                       [-1, -1, -1]])\n",
    "sharp_img = cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "#sharpen laplace\n",
    "sharp_lapla = cv2.Laplacian(image, cv2.CV_8U) \n",
    "\n",
    "# equ hist\n",
    "img = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "equ_hist1 = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# equ hist + contrast\n",
    "img = cv2.addWeighted(image, 1.7, np.zeros(image.shape, image.dtype), 0, -150)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "equ_hist2 = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# equ hist + contrast + bilateral\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "equ_hist3 = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# erode\n",
    "kernel = np.array([[1, 9, 1],\n",
    "                   [1, 9, 1],\n",
    "                   [1, 9, 1]])\n",
    "eroded1 = cv2.erode(image, kernel)\n",
    "\n",
    "# c/b + equ hsit + bil\n",
    "img = cv2.addWeighted(img, 2, np.zeros(img.shape, img.dtype), 0, -100)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "eroded2 = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "\n",
    "# filter 2d sharp\n",
    "kernel = np.array([[0, -1, 0],\n",
    "                  [-1, 5, -1],\n",
    "                  [0, -1, 0]])\n",
    "filter2d_1 = cv2.filter2D(image,-1,kernel)\n",
    "\n",
    "# filter 2d outline\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 8, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "filter2d_2 = cv2.filter2D(image,-1,kernel)\n",
    "\n",
    "# filter 2d emboss\n",
    "kernel = np.array([\n",
    "  [-2, -1, 0],\n",
    "  [-1, 1, 1],\n",
    "  [0, 1, 2]\n",
    "])\n",
    "filter2d_3 = cv2.filter2D(image,-1,kernel)\n",
    "\n",
    "#filter2d outline + bil\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 8, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "img = cv2.filter2D(image,-1,kernel)\n",
    "fil_bil1 = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "\n",
    "#filter2d (-1/7)+ bil + nl + bil + c1.7/b-150  (best thing yet)\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 7, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "#img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "img = cv2.filter2D(image,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "fil_nl1 = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "#filter2d (-1/7)+ bil + nl + c1.7/b-150 (okay)\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 7, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "img = cv2.filter2D(image,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "fil_nl2 = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "#filter2d (outline)+ bil + nl + c1.7/b-150 + f2d (sobel)\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 7, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "img = cv2.filter2D(image,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "kernel = np.array([\n",
    "  [-1, 0, 1],\n",
    "  [-2, 0, 2],\n",
    "  [-1, 0, 1]\n",
    "])\n",
    "img = cv2.filter2D(img,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "fil_nl3 = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# best thing + emboss\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 7, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "img = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n",
    "img = cv2.filter2D(img,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "kernel = np.array([\n",
    "  [-2, -1, 0],\n",
    "  [-1, 1, 1],\n",
    "  [0, 1, 2]\n",
    "])\n",
    "fil_nl4 = cv2.filter2D(img,-1,kernel)\n",
    "\n",
    "list = ([image, gaussian_blur, mean_blur, median_blur, bilateral_blur, motion_blur,fast_nl_grey,fast_nl_color,\n",
    "         c_b,cb_nl,cont1,cont2,cont3,bil_cont1,bil_cont2,bil_cont3,sharp_img,sharp_lapla,equ_hist1,equ_hist2,\n",
    "         equ_hist3,bl_cb_bil_cb1,eroded1, eroded2,filter2d_1,filter2d_2,filter2d_3,fil_bil1,fil_nl1,fil_nl2,fil_nl3,\n",
    "         fil_nl4\n",
    "        ],\n",
    "        ['Original', 'Gaussian Blur', 'Mean Blur', 'Median Blur', 'Bilateral Filter', 'Motion Blur','NL Means Grey',\n",
    "         'NL Means Color','Color/Brightness','CB/NL','C/B (1.5,-150)','C/B (1.7,-150)','C/B (2,-150)',\n",
    "         'BF/Cont/Br (1.5,-150)', 'BF/Cont/Br (1.7,-150)', 'BF/Cont/Br (2,-150)', 'Sharp Kernel', 'Sharp Laplace',\n",
    "         'Histrogramm Equ.', 'HE+Contrast', 'HE+Cont+Bil', \"bil+c2+b-150+bil+c2+b-200\", \"Eroded\", \"+C/B\", 'Filter2D-Sharp',\n",
    "         \"Filter2D-Outline\", \"Filter2D-Emboss\", \"Fil2dOutline+Bil\", \"F2D+bil+nl+bil+cb (best)\", \"F2D+bil+nl+bil+cb+F2D\",\n",
    "         \"F2D+bil+nl+bil+cb\", \"best thing + emboss\"])\n",
    "\n",
    "# Anzeigen der Ergebnisse\n",
    "show_images(*list)\n",
    "show_hist(*list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

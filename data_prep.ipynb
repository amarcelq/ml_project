{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66adbb1d-937b-4098-9e0e-9e4fba70c410",
   "metadata": {},
   "source": [
    "# Aufbereitung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0772e326-d818-4530-b12e-50bcb78e42bc",
   "metadata": {},
   "source": [
    "Damit die Spektogramme überhaupt von Modellen genutzt werden können, müssen sie aufbereitet, (teilweise) bearbeitet und in einem gut verwendbarem Format abgespeichert werden.\n",
    "In unserem Fall besteht das aus den folgenden Schritten:\n",
    "1. Die Skala/Rahmen der Bilder entfernen\n",
    "2. Die Bilder runterskalieren\n",
    "3. (Optional) Filter oder andere Bildbearbeitungen anwenden\n",
    "4. Daten nach Klasse aussortieren\n",
    "5. Fledermausarten in Numerische Klassen umwandeln"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0699a1d3",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3375aaf9-6fe1-46b0-85ca-8573ed88db4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.under_sampling import EditedNearestNeighbours as ENN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ba1093-7cef-4b8f-ab34-3ddf7af31a92",
   "metadata": {},
   "source": [
    "#### Funktion zum entfernen eines übergebenen Randes.\n",
    "Wird letzendlich dafür genutzt die Skala und leeren Pixel um das Spektrogramm zu entfernen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f91a1c2b-e309-4876-8e4b-d85b86f631e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_frame(img, frame):\n",
    "    return img[frame[0]:frame[1], frame[2]:frame[3]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ede773-3c8c-4bd3-b8e3-8476dd4ef41b",
   "metadata": {},
   "source": [
    "#### Bilder einlesen\n",
    "Hier wrid eine Lister aller Bilder im übergeordneten Ordner `../Bat_Orientation_Calls` erstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ffa166b2-107b-4cb7-b681-ac11fdb3bf3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4118 image paths.\n"
     ]
    }
   ],
   "source": [
    "folder_path = Path(\"../Bat_Orientation_Calls\")\n",
    "\n",
    "image_paths = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith(\".png\")]\n",
    "\n",
    "print(f\"Loaded {len(image_paths)} image paths.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd25f2c-56d2-4180-93c5-0384b87ac586",
   "metadata": {},
   "source": [
    "#### Bildbearbeitung\n",
    "Hier werden die eingelesenen Bilder zugeschnitten, runterskaliert und optional mit Filtern o.ä. bearbeitet und anschließend in `./compressed_pictures/` gespeichert.\n",
    "\n",
    "**Hinweis:** Zur Darstellung des Fortschritts wird tqdm genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5fdf4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abc7dd93d3f4138b1303356442ae741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for image_path in tqdm(image_paths):\n",
    "    # Bild für OpenCV2 einlesen\n",
    "    img = cv2.imread(image_path, -1)\n",
    "\n",
    "    # Den Rand inkl. Skala entfernen\n",
    "    # Has to be hardcoded, cause not every image has the exact pixel perfect border/scale, so an automatic detection\n",
    "    #   trips up and the images would end with different sizes.\n",
    "    frame = (36, 251, 55, 388)\n",
    "    img = remove_frame(img, frame)\n",
    "\n",
    "    # Größe des neuen Bildes berechnen und skalieren\n",
    "    base_width = 128 # finale Breite des Bildes\n",
    "    height, width, channels = img.shape\n",
    "    aspect_ratio = width / height  # Width / Height\n",
    "    new_width = base_width\n",
    "    new_height = int(new_width / aspect_ratio)\n",
    "    img = cv2.resize(img, (new_width, new_height)) \n",
    "\n",
    "    # Alpha Channel entfernen, da dieser keine Informationen beinhaltet\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "    # Bild abspeichern\n",
    "    image_name = Path(image_path).name\n",
    "    cv2.imwrite(f\"./compressed_pictures/{image_name}\",img)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a358be5-4040-4f0b-8f44-468a2c5469fa",
   "metadata": {},
   "source": [
    "#### Klassen aus CSV einlesen und anpassen\n",
    "Hier werden die Klassen aus den CSV's eingelesen und anschließend unerwünschte Klassen entfernt.\n",
    "Kriterien für die Unerwünschtheit sind folgende:\n",
    "- Klasse ist 'Fledermaus nicht bestimmbar' oder 'Schwarzbild', da diese keine sinnvollen Informationen sind\n",
    "- Klasse (bzw. Bild-ID) ist ein Duplikat\n",
    "- Klasse hat weniger als 60 Samples, da diese schlichweg ungeeignet für das Trainieren eines Modells sind\n",
    "\n",
    "Anschließend liegt ein DataFrame mit den ID-Klassen Paaren vor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff50f2f3-f3df-44f6-9514-3150188d142a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 40 entries cause they were duplicates.\n",
      "Dropped 180 entries cause they were of class 'Fledermaus nicht bestimmbar'.\n",
      "Dropped 180 entries cause they were of class 'Schwarzbild'.\n",
      "Dropped 400 in total.\n",
      "DataFrame as 3723 different entries (Images).\n"
     ]
    }
   ],
   "source": [
    "# Reads in classes from all csv's, and edits it\n",
    "def classes_csv_to_df(file_paths: list, delimiter=\";\") -> pd.DataFrame:\n",
    "    df_all = pd.DataFrame()\n",
    "    for file_path in file_paths:\n",
    "        df_current = pd.read_csv(file_path, delimiter=delimiter)\n",
    "        df_all = pd.concat([df_all, df_current])\n",
    "\n",
    "    df_all = df_all.reset_index(drop=True)\n",
    "    df_all = df_all.drop_duplicates()\n",
    "    df_all.drop(\"Filename\", axis=1, inplace=True)\n",
    "    df_all = remove_unwanted_datapoints(df_all)\n",
    "    df_all.replace(\"&Mausohr \", \"Mausohr\", inplace=True)\n",
    "    df_all = remove_less_sample_classes(df_all, 60)\n",
    "    \n",
    "    #df_all.drop(\"Species\", axis=1, inplace=True)\n",
    "    return df_all\n",
    "\n",
    "# Removes all duplicate entries and entries of type 'Fledermaus nicht bestimmbar' or 'Schwarzbild'\n",
    "def remove_unwanted_datapoints(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    ids = set()\n",
    "    duplicated_ids = set()\n",
    "    for row in df.iterrows():\n",
    "        id = row[1][\"ID\"]\n",
    "        if id in ids:\n",
    "            duplicated_ids.add(id)\n",
    "        ids.add(id)\n",
    "\n",
    "    # print how many were droppes\n",
    "    print(f\"Dropped {len(duplicated_ids)} entries cause they were duplicates.\")\n",
    "    print(f\"Dropped {len(df[df['Species'] == 'Fledermaus nicht bestimmbar']) } entries cause they were of class 'Fledermaus nicht bestimmbar'.\")\n",
    "    print(f\"Dropped {len(df[df['Species'] == 'Schwarzbild'])} entries cause they were of class 'Schwarzbild'.\")\n",
    "    print(f\"Dropped {len(duplicated_ids)+len(df[df['Species'] == 'Fledermaus nicht bestimmbar'])+len(df[df['Species'] == 'Schwarzbild'])} in total.\")\n",
    "    \n",
    "    return df[~((df['ID'].isin(duplicated_ids)) |\n",
    "            (df['Species'].isin(['Fledermaus nicht bestimmbar', 'Schwarzbild'])))]\\\n",
    "            .reset_index(drop=True)\n",
    "\n",
    "# Remove all classes with less than 60 entries\n",
    "def remove_less_sample_classes(df: pd.DataFrame, min_samples: int) -> pd.DataFrame:\n",
    "    class_distribution = df['Species'].value_counts()\n",
    "    valid_classes = class_distribution[class_distribution >= min_samples].index\n",
    "    return df[df['Species'].isin(valid_classes)]\n",
    "\n",
    "## execute functions\n",
    "\n",
    "df = classes_csv_to_df([\"../Auswertung_20220524.csv\",\"../LMU_20180326_class.csv\", \"../LMU_20180505_classified.csv\"])\n",
    "\n",
    "print(f\"DataFrame as {len(df)} different entries (Images).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e03463-eb8e-4bbe-a954-b3eac563df6d",
   "metadata": {},
   "source": [
    "#### Klassen konvertieren\n",
    "Hier werden die Labels der Klassen von den Fledermausnamen in numerische Label konvertiert. Funktionen für categorische und One-Hot Encodede Labels gibt es auch, werden von uns aber nicht genutzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a79abd20-5d7f-4dbf-8075-997094a355a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Bartfledermaus', 1: 'Bechsteinfledermaus', 2: 'Fransenfledermaus', 3: 'Langohrfledermaus', 4: 'Mausohr', 5: 'Wasserfledermaus'}\n"
     ]
    }
   ],
   "source": [
    "def categorical_classes(df: pd.DataFrame, column_name: str):\n",
    "    # df passed as call by reference\n",
    "    df_copy = df.copy()\n",
    "    df_copy[column_name] = df_copy[column_name].astype('category')\n",
    "    return df_copy\n",
    "\n",
    "def numerical_classes(df: pd.DataFrame, column_name: str) -> tuple[pd.DataFrame, dict[int|str]]:\n",
    "    df_copy = df.copy()\n",
    "    df_copy[column_name] = df_copy[column_name].astype('category')\n",
    "    class_mapping = dict(enumerate(df_copy[column_name].cat.categories))\n",
    "    df_copy[column_name] = df_copy[column_name].cat.codes\n",
    "    return df_copy, class_mapping\n",
    "\n",
    "def encode_classes(df: pd.DataFrame, column_name: str):\n",
    "    encoded_classes = pd.get_dummies(df[column_name])\n",
    "    df = df.join(encoded_classes)\n",
    "    return df\n",
    "\n",
    "df_numerical, class_mapping = numerical_classes(df, \"Species\")\n",
    "print(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf42251-535f-4e21-a39b-84e642b6908a",
   "metadata": {},
   "source": [
    "#### Klassen Mapping speichern\n",
    "Damit die nun numerischen Klassen noch zuordbar sind, wird das Mapping von Wert-Klasse in der Datei `./data/class_mapping.csv` gespeichert. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83f1f1d4-eace-40f6-989a-efad084e7483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_mapping_to_csv(class_mapping_dict: dict) -> None: \n",
    "    with open('./data/class_mapping.csv', 'w') as class_mapping_csv:  \n",
    "        writer = csv.writer(class_mapping_csv)\n",
    "        for key, value in class_mapping_dict.items():\n",
    "            writer.writerow([key, value])\n",
    "\n",
    "class_mapping_to_csv(class_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a160234-f81c-42e3-9220-4d770289740d",
   "metadata": {},
   "source": [
    "#### Klassenverteilung plotten\n",
    "Hier wird die Verteilugn der Klassen geplottet und unter `./data/class_distribution` gespeichert. Wie zu sehen ist, sind sie stark ungleichmäßig verteilt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1ae1e66e-93e1-4dcd-9f31-bb1ec8f578a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    2104\n",
      "2     866\n",
      "3     291\n",
      "1     270\n",
      "4     123\n",
      "5      69\n",
      "Name: Species, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAFwCAYAAACsB1B4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuz0lEQVR4nO3de1RU5f4/8PcAzqDIDCIyAycExaWIFzL0EKWoaaCSl7KLV7DwGnpUTInf6ahZXzE8mXY0Xa6T4llHj5d11ArNxCsnxUsYoqh4CYWWzlgpM4E63PbvDxe7doAyOCPw+H6ttddiP8+z9/48UG+2D5uNSpIkCUREJAynhi6AiIjsi8FORCQYBjsRkWAY7EREgmGwExEJhsFORCQYBjsRkWAY7EREgmGwExEJhsFOVAdXr16FSqVCamqqw6+VmpoKlUqFq1evym0BAQF46aWXHH5tADh06BBUKhUOHTr0WK5H9sdgp4e6cuUKpkyZgvbt28PV1RVarRbPP/88VqxYgbt37zZ0eQCAzz77zKbQValU8ubi4gJPT0+EhoZi5syZOHfuXIPV9Tg15tro0aj4rhh6kF27duG1116DRqNBTEwMunbtitLSUnz77bf473//iwkTJmDt2rUNXSa6du0KLy+vOt9lqlQqvPjii4iJiYEkSTCbzTh9+jS2bduGkpISfPTRR0hISJDHS5IEq9WKZs2awdnZ2WF1AUBFRQXKysqg0WigUqkA3L9j79q1K9LS0up8nvrWVllZidLSUqjVajg58d6vKXJp6AKo8crPz8eoUaPg7++PAwcOwMfHR+6Lj4/H5cuXsWvXrgas8NF07NgR48aNU7QtWbIEQ4cOxZw5cxAUFIQhQ4YAuP+NwNXV1aH1lJSUwM3NDc7OzjZ987A3Jycnh8+VHEwiqsXUqVMlANKRI0fqNL6srExatGiR1L59e0mtVkv+/v5SUlKSdO/ePcU4ANKCBQuqHe/v7y/FxsbK++vXr5cASN9++600e/ZsycvLS2rRooU0YsQI6ebNm4rjACi2vn37PrBWAFJ8fHyNfdeuXZNcXFyk5557Tm7Lz8+XAEjr16+X227cuCFNmDBB+tOf/iSp1WrJYDBIw4YNk/Lz8x9aV9XcDh06JE2bNk1q06aN5OHhoeirOk/VuaKjo6VvvvlGCgkJkTQajdS5c2fpv//9r6L2BQsWSDX9b/3Hcz6otoMHD0oApIMHDyrOsXXrVumZZ56RXF1dpdatW0tjx46VfvzxR8WY2NhYyc3NTfrxxx+l4cOHS25ubpKXl5c0Z84cqby8vMbPN9kf79ipVl999RXat2+P5557rk7jJ06ciA0bNuDVV1/FnDlzcPz4cSQnJ+P8+fPYsWNHveuYMWMGWrVqhQULFuDq1atYvnw5pk+fji1btgAAli9fjhkzZqBly5b461//CgDQ6/X1vl7btm3Rt29fHDx4EBaLBVqttsZxI0eORG5uLmbMmIGAgADcvHkT6enpKCgoQEBAQJ3qevvtt9GmTRvMnz8fJSUlD6zr0qVLeOONNzB16lTExsZi/fr1eO2117Bnzx68+OKLNs3R1s9Zamoq3nzzTfTq1QvJyckwmUxYsWIFjhw5gu+//x4eHh7y2IqKCkRFRSEsLAx///vfsW/fPnz88ccIDAzEtGnTbKqT6qmhv7NQ42Q2myUA0vDhw+s0Pjs7WwIgTZw4UdH+zjvvSACkAwcOyG2w8Y594MCBUmVlpdw+e/ZsydnZWSoqKpLbunTp8tC79N/DA+7YJUmSZs6cKQGQTp8+LUlS9Tv227dvSwCkpUuXPvA6tdVVNbfevXtXu5Ot7Y4dgOIO3Ww2Sz4+PlKPHj3ktrresT+otj/esZeWlkre3t5S165dpbt378rj0tLSJADS/Pnz5bbY2FgJgLRo0SLFOXv06CGFhoZWuxY5Bn8yQjWyWCwAAHd39zqN3717NwAofuAIAHPmzAGAR1qLnzx5svxDRADo06cPKioqcO3atXqf82FatmwJAPj1119r7G/evDnUajUOHTqE27dv1/s6kyZNqvN6uq+vL15++WV5X6vVIiYmBt9//z2MRmO9a3iY7777Djdv3sTbb7+tWHuPjo5GUFBQjV/bqVOnKvb79OmDH374wWE1khKDnWpUtfxQW7D90bVr1+Dk5IQOHToo2g0GAzw8PB4phNu2bavYb9WqFQA8UqA+THFxMYDav7FpNBp89NFH+Prrr6HX6xEREYGUlBSbA7Zdu3Z1HtuhQwfFNzjg/g+AASieebe3qq9dp06dqvUFBQVV+9q6urqiTZs2irZWrVo59OtFSgx2qpFWq4Wvry/Onj1r03F/DB5bVFRU1Nhe2x2t5MAndc+ePQtnZ+cHBu+sWbNw8eJFJCcnw9XVFX/729/QuXNnfP/993W+TvPmze1Rrqy2z39tn1tHaMgneug+BjvV6qWXXsKVK1eQmZn50LH+/v6orKzEpUuXFO0mkwlFRUXw9/eX21q1aoWioiLFuNLSUty4caPetT7KN5Q/KigowOHDhxEeHv7QpajAwEDMmTMHe/fuxdmzZ1FaWoqPP/7YIXVdvny52jezixcvArj/nDvw279m/vj5relfTHWtreprl5eXV60vLy9P8bWlxoHBTrWaN28e3NzcMHHiRJhMpmr9V65cwYoVKwBAft57+fLlijHLli0DcH89tkpgYCAyMjIU49auXftId5Vubm7Vwqw+bt26hdGjR6OiokJ+WqQmd+7cwb179xRtgYGBcHd3h9VqtXtdAHD9+nXF00UWiwX/+te/8PTTT8NgMMg1AFB8fktKSrBhw4Zq56trbT179oS3tzfWrFmjmNvXX3+N8+fPK7621DjwcUeqVWBgIDZt2oQ33ngDnTt3Vvzm6dGjR7Ft2zZMmDABABASEoLY2FisXbsWRUVF6Nu3L06cOIENGzZgxIgR6N+/v3zeiRMnYurUqRg5ciRefPFFnD59Gt988w28vLzqXWtoaChWr16NDz/8EB06dIC3tzdeeOGFBx5z8eJF/Pvf/4YkSbBYLPJvnhYXF2PZsmUYNGjQA48dMGAAXn/9dQQHB8PFxQU7duyAyWTCqFGjHqmu2nTs2BFxcXE4efIk9Ho91q1bB5PJhPXr18tjIiMj0bZtW8TFxWHu3LlwdnbGunXr0KZNGxQUFNTrc9asWTN89NFHePPNN9G3b1+MHj1aftwxICAAs2fPrtd8yIEa+KkcagIuXrwoTZo0SQoICJDUarXk7u4uPf/889I//vEPxS8flZWVSe+//77Url07qVmzZpKfn1+Nv6BUUVEhJSYmyr9wFBUVJV2+fLnWxx1PnjypOL6mX6AxGo1SdHS05O7uXudfUKranJycJA8PD6lHjx7SzJkzpdzc3Grj//i4488//yzFx8dLQUFBkpubm6TT6aSwsDBp69atiuNqq6u2uf2+r7ZfUOrevbuk0WikoKAgadu2bdWOz8rKksLCwiS1Wi21bdtWWrZsWY3nrK222n5BacuWLVKPHj0kjUYjeXp6PvAXlP6otscwyTH4rhgiIsFwjZ2ISDAMdiIiwTDYiYgEw2AnIhIMg52ISDDCPsdeWVmJ69evw93d3a6//UdE1FAkScKvv/4KX1/fB/51K2GD/fr16/Dz82voMoiI7K6wsBBPPfVUrf3CBnvVOz4KCwtr/UMJRERNicVigZ+f30PfYSRssFctv2i1WgY7EQnlYcvL/OEpEZFgGOxERIJhsBMRCYbBTkQkGAY7EZFgGOxERIJhsBMRCYbBTkQkGAY7EZFgGOxERIJhsBMRCYbBTkQkGGFfAlZfAe/ueizXubok+rFch4iePLxjJyISDIOdiEgwDHYiIsEw2ImIBMNgJyISDIOdiEgwDHYiIsEw2ImIBMNgJyISDIOdiEgwNgV7cnIyevXqBXd3d3h7e2PEiBHIy8tTjLl37x7i4+PRunVrtGzZEiNHjoTJZFKMKSgoQHR0NFq0aAFvb2/MnTsX5eXlijGHDh3CM888A41Ggw4dOiA1NbV+MyQiesLYFOyHDx9GfHw8jh07hvT0dJSVlSEyMhIlJSXymNmzZ+Orr77Ctm3bcPjwYVy/fh2vvPKK3F9RUYHo6GiUlpbi6NGj2LBhA1JTUzF//nx5TH5+PqKjo9G/f39kZ2dj1qxZmDhxIr755hs7TJmISGwqSZKk+h78008/wdvbG4cPH0ZERATMZjPatGmDTZs24dVXXwUAXLhwAZ07d0ZmZiaeffZZfP3113jppZdw/fp16PV6AMCaNWuQmJiIn376CWq1GomJidi1axfOnj0rX2vUqFEoKirCnj176lSbxWKBTqeD2WyGVqut85z4EjAiaqzqmmuPtMZuNpsBAJ6engCArKwslJWVYeDAgfKYoKAgtG3bFpmZmQCAzMxMdOvWTQ51AIiKioLFYkFubq485vfnqBpTdY6aWK1WWCwWxUZE9CSqd7BXVlZi1qxZeP7559G1a1cAgNFohFqthoeHh2KsXq+H0WiUx/w+1Kv6q/oeNMZiseDu3bs11pOcnAydTidvfn5+9Z0aEVGTVu9gj4+Px9mzZ7F582Z71lNvSUlJMJvN8lZYWNjQJRERNYh6/aGN6dOnIy0tDRkZGXjqqafkdoPBgNLSUhQVFSnu2k0mEwwGgzzmxIkTivNVPTXz+zF/fJLGZDJBq9WiefPmNdak0Wig0WjqMx0iIqHYdMcuSRKmT5+OHTt24MCBA2jXrp2iPzQ0FM2aNcP+/fvltry8PBQUFCA8PBwAEB4ejjNnzuDmzZvymPT0dGi1WgQHB8tjfn+OqjFV5yAiotrZdMceHx+PTZs24YsvvoC7u7u8Jq7T6dC8eXPodDrExcUhISEBnp6e0Gq1mDFjBsLDw/Hss88CACIjIxEcHIzx48cjJSUFRqMR7733HuLj4+U77qlTp2LlypWYN28e3nrrLRw4cABbt27Frl2P54kVIqKmzKY79tWrV8NsNqNfv37w8fGRty1btshjPvnkE7z00ksYOXIkIiIiYDAYsH37drnf2dkZaWlpcHZ2Rnh4OMaNG4eYmBgsWrRIHtOuXTvs2rUL6enpCAkJwccff4x//vOfiIqKssOUiYjE9kjPsTdmfI6diETzWJ5jJyKixofBTkQkGAY7EZFgGOxERIJhsBMRCYbBTkQkGAY7EZFgGOxERIJhsBMRCYbBTkQkGAY7EZFgGOxERIJhsBMRCYbBTkQkGAY7EZFgGOxERIJhsBMRCYbBTkQkGAY7EZFgGOxERIJhsBMRCYbBTkQkGAY7EZFgGOxERIKxOdgzMjIwdOhQ+Pr6QqVSYefOnYp+lUpV47Z06VJ5TEBAQLX+JUuWKM6Tk5ODPn36wNXVFX5+fkhJSanfDImInjA2B3tJSQlCQkKwatWqGvtv3Lih2NatWweVSoWRI0cqxi1atEgxbsaMGXKfxWJBZGQk/P39kZWVhaVLl2LhwoVYu3atreUSET1xXGw9YPDgwRg8eHCt/QaDQbH/xRdfoH///mjfvr2i3d3dvdrYKhs3bkRpaSnWrVsHtVqNLl26IDs7G8uWLcPkyZNrPMZqtcJqtcr7FoulrlMiIhKKQ9fYTSYTdu3ahbi4uGp9S5YsQevWrdGjRw8sXboU5eXlcl9mZiYiIiKgVqvltqioKOTl5eH27ds1Xis5ORk6nU7e/Pz87D8hIqImwKHBvmHDBri7u+OVV15RtP/lL3/B5s2bcfDgQUyZMgWLFy/GvHnz5H6j0Qi9Xq84pmrfaDTWeK2kpCSYzWZ5KywstPNsiIiaBpuXYmyxbt06jB07Fq6uror2hIQE+ePu3btDrVZjypQpSE5Ohkajqde1NBpNvY8lIhKJw+7Y//e//yEvLw8TJ0586NiwsDCUl5fj6tWrAO6v05tMJsWYqv3a1uWJiOg+hwX7559/jtDQUISEhDx0bHZ2NpycnODt7Q0ACA8PR0ZGBsrKyuQx6enp6NSpE1q1auWokomIhGBzsBcXFyM7OxvZ2dkAgPz8fGRnZ6OgoEAeY7FYsG3bthrv1jMzM7F8+XKcPn0aP/zwAzZu3IjZs2dj3LhxcmiPGTMGarUacXFxyM3NxZYtW7BixQrFEg4REdXM5jX27777Dv3795f3q8I2NjYWqampAIDNmzdDkiSMHj262vEajQabN2/GwoULYbVa0a5dO8yePVsR2jqdDnv37kV8fDxCQ0Ph5eWF+fPn1/qoIxER/UYlSZLU0EU4gsVigU6ng9lshlarrfNxAe/ucmBVv7m6JPqxXIeIxFHXXOO7YoiIBMNgJyISDIOdiEgwDHYiIsEw2ImIBMNgJyISDIOdiEgwDHYiIsEw2ImIBMNgJyISDIOdiEgwDHYiIsEw2ImIBMNgJyISDIOdiEgwDHYiIsEw2ImIBMNgJyISDIOdiEgwDHYiIsEw2ImIBMNgJyISDIOdiEgwDHYiIsHYHOwZGRkYOnQofH19oVKpsHPnTkX/hAkToFKpFNugQYMUY27duoWxY8dCq9XCw8MDcXFxKC4uVozJyclBnz594OrqCj8/P6SkpNg+OyKiJ5DNwV5SUoKQkBCsWrWq1jGDBg3CjRs35O0///mPon/s2LHIzc1Feno60tLSkJGRgcmTJ8v9FosFkZGR8Pf3R1ZWFpYuXYqFCxdi7dq1tpZLRPTEcbH1gMGDB2Pw4MEPHKPRaGAwGGrsO3/+PPbs2YOTJ0+iZ8+eAIB//OMfGDJkCP7+97/D19cXGzduRGlpKdatWwe1Wo0uXbogOzsby5YtU3wD+D2r1Qqr1SrvWywWW6dGRCQEh6yxHzp0CN7e3ujUqROmTZuGX375Re7LzMyEh4eHHOoAMHDgQDg5OeH48ePymIiICKjVanlMVFQU8vLycPv27RqvmZycDJ1OJ29+fn6OmBoRUaNn92AfNGgQ/vWvf2H//v346KOPcPjwYQwePBgVFRUAAKPRCG9vb8UxLi4u8PT0hNFolMfo9XrFmKr9qjF/lJSUBLPZLG+FhYX2nhoRUZNg81LMw4waNUr+uFu3bujevTsCAwNx6NAhDBgwwN6Xk2k0Gmg0Goedn4ioqXD4447t27eHl5cXLl++DAAwGAy4efOmYkx5eTlu3bolr8sbDAaYTCbFmKr92tbuiYjoPocH+48//ohffvkFPj4+AIDw8HAUFRUhKytLHnPgwAFUVlYiLCxMHpORkYGysjJ5THp6Ojp16oRWrVo5umQioibN5mAvLi5GdnY2srOzAQD5+fnIzs5GQUEBiouLMXfuXBw7dgxXr17F/v37MXz4cHTo0AFRUVEAgM6dO2PQoEGYNGkSTpw4gSNHjmD69OkYNWoUfH19AQBjxoyBWq1GXFwccnNzsWXLFqxYsQIJCQn2mzkRkaBsDvbvvvsOPXr0QI8ePQAACQkJ6NGjB+bPnw9nZ2fk5ORg2LBh6NixI+Li4hAaGor//e9/ivXvjRs3IigoCAMGDMCQIUPQu3dvxTPqOp0Oe/fuRX5+PkJDQzFnzhzMnz+/1kcdiYjoNypJkqSGLsIRLBYLdDodzGYztFptnY8LeHeXA6v6zdUl0Y/lOkQkjrrmGt8VQ0QkGAY7EZFgGOxERIJhsBMRCYbBTkQkGAY7EZFgGOxERIJhsBMRCYbBTkQkGAY7EZFgGOxERIJhsBMRCYbBTkQkGAY7EZFgGOxERIJhsBMRCYbBTkQkGAY7EZFgGOxERIJhsBMRCYbBTkQkGAY7EZFgGOxERIJhsBMRCcbmYM/IyMDQoUPh6+sLlUqFnTt3yn1lZWVITExEt27d4ObmBl9fX8TExOD69euKcwQEBEClUim2JUuWKMbk5OSgT58+cHV1hZ+fH1JSUuo3QyKiJ4zNwV5SUoKQkBCsWrWqWt+dO3dw6tQp/O1vf8OpU6ewfft25OXlYdiwYdXGLlq0CDdu3JC3GTNmyH0WiwWRkZHw9/dHVlYWli5dioULF2Lt2rW2lktE9MRxsfWAwYMHY/DgwTX26XQ6pKenK9pWrlyJP//5zygoKEDbtm3ldnd3dxgMhhrPs3HjRpSWlmLdunVQq9Xo0qULsrOzsWzZMkyePLnGY6xWK6xWq7xvsVhsnRoRkRAcvsZuNpuhUqng4eGhaF+yZAlat26NHj16YOnSpSgvL5f7MjMzERERAbVaLbdFRUUhLy8Pt2/frvE6ycnJ0Ol08ubn5+eQ+RARNXYODfZ79+4hMTERo0ePhlarldv/8pe/YPPmzTh48CCmTJmCxYsXY968eXK/0WiEXq9XnKtq32g01nitpKQkmM1meSssLHTAjIiIGj+bl2LqqqysDK+//jokScLq1asVfQkJCfLH3bt3h1qtxpQpU5CcnAyNRlOv62k0mnofS0QkEofcsVeF+rVr15Cenq64W69JWFgYysvLcfXqVQCAwWCAyWRSjKnar21dnoiI7rN7sFeF+qVLl7Bv3z60bt36ocdkZ2fDyckJ3t7eAIDw8HBkZGSgrKxMHpOeno5OnTqhVatW9i6ZiEgoNi/FFBcX4/Lly/J+fn4+srOz4enpCR8fH7z66qs4deoU0tLSUFFRIa+Je3p6Qq1WIzMzE8ePH0f//v3h7u6OzMxMzJ49G+PGjZNDe8yYMXj//fcRFxeHxMREnD17FitWrMAnn3xip2kTEYlLJUmSZMsBhw4dQv/+/au1x8bGYuHChWjXrl2Nxx08eBD9+vXDqVOn8Pbbb+PChQuwWq1o164dxo8fj4SEBMUaeU5ODuLj43Hy5El4eXlhxowZSExMrHOdFosFOp0OZrP5oUtBvxfw7q46j30UV5dEP5brEJE46pprNgd7U8FgJyLR1DXX+K4YIiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiATDYCciEozNwZ6RkYGhQ4fC19cXKpUKO3fuVPRLkoT58+fDx8cHzZs3x8CBA3Hp0iXFmFu3bmHs2LHQarXw8PBAXFwciouLFWNycnLQp08fuLq6ws/PDykpKbbPjojoCWRzsJeUlCAkJASrVq2qsT8lJQWffvop1qxZg+PHj8PNzQ1RUVG4d++ePGbs2LHIzc1Feno60tLSkJGRgcmTJ8v9FosFkZGR8Pf3R1ZWFpYuXYqFCxdi7dq19ZgiEdGTRSVJklTvg1Uq7NixAyNGjABw/27d19cXc+bMwTvvvAMAMJvN0Ov1SE1NxahRo3D+/HkEBwfj5MmT6NmzJwBgz549GDJkCH788Uf4+vpi9erV+Otf/wqj0Qi1Wg0AePfdd7Fz505cuHChTrVZLBbodDqYzWZotdo6zyng3V02fAbq7+qS6MdyHSISR11zza5r7Pn5+TAajRg4cKDcptPpEBYWhszMTABAZmYmPDw85FAHgIEDB8LJyQnHjx+Xx0RERMihDgBRUVHIy8vD7du3a7y21WqFxWJRbERETyK7BrvRaAQA6PV6Rbter5f7jEYjvL29Ff0uLi7w9PRUjKnpHL+/xh8lJydDp9PJm5+f36NPiIioCRLmqZikpCSYzWZ5KywsbOiSiIgahF2D3WAwAABMJpOi3WQyyX0GgwE3b95U9JeXl+PWrVuKMTWd4/fX+CONRgOtVqvYiIieRHYN9nbt2sFgMGD//v1ym8ViwfHjxxEeHg4ACA8PR1FREbKysuQxBw4cQGVlJcLCwuQxGRkZKCsrk8ekp6ejU6dOaNWqlT1LJiISjs3BXlxcjOzsbGRnZwO4/wPT7OxsFBQUQKVSYdasWfjwww/x5Zdf4syZM4iJiYGvr6/85Eznzp0xaNAgTJo0CSdOnMCRI0cwffp0jBo1Cr6+vgCAMWPGQK1WIy4uDrm5udiyZQtWrFiBhIQEu02ciEhULrYe8N1336F///7yflXYxsbGIjU1FfPmzUNJSQkmT56MoqIi9O7dG3v27IGrq6t8zMaNGzF9+nQMGDAATk5OGDlyJD799FO5X6fTYe/evYiPj0doaCi8vLwwf/58xbPuRERUs0d6jr0x43PsRCSaBnmOnYiIGh6DnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiARj89sdqengC82Inky8YyciEgyDnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLB2D3YAwICoFKpqm3x8fEAgH79+lXrmzp1quIcBQUFiI6ORosWLeDt7Y25c+eivLzc3qUSEQnJ7i8BO3nyJCoqKuT9s2fP4sUXX8Rrr70mt02aNAmLFi2S91u0aCF/XFFRgejoaBgMBhw9ehQ3btxATEwMmjVrhsWLF9u7XCIi4dg92Nu0aaPYX7JkCQIDA9G3b1+5rUWLFjAYDDUev3fvXpw7dw779u2DXq/H008/jQ8++ACJiYlYuHAh1Gq1vUsmIhKKQ9fYS0tL8e9//xtvvfUWVCqV3L5x40Z4eXmha9euSEpKwp07d+S+zMxMdOvWDXq9Xm6LioqCxWJBbm5urdeyWq2wWCyKjYjoSeTQ97Hv3LkTRUVFmDBhgtw2ZswY+Pv7w9fXFzk5OUhMTEReXh62b98OADAajYpQByDvG43GWq+VnJyM999/3/6TICJqYhwa7J9//jkGDx4MX19fuW3y5Mnyx926dYOPjw8GDBiAK1euIDAwsN7XSkpKQkJCgrxvsVjg5+dX7/MRETVVDgv2a9euYd++ffKdeG3CwsIAAJcvX0ZgYCAMBgNOnDihGGMymQCg1nV5ANBoNNBoNI9YNRFR0+ewNfb169fD29sb0dEP/rNp2dnZAAAfHx8AQHh4OM6cOYObN2/KY9LT06HVahEcHOyocomIhOGQO/bKykqsX78esbGxcHH57RJXrlzBpk2bMGTIELRu3Ro5OTmYPXs2IiIi0L17dwBAZGQkgoODMX78eKSkpMBoNOK9995DfHw878iJiOrAIcG+b98+FBQU4K233lK0q9Vq7Nu3D8uXL0dJSQn8/PwwcuRIvPfee/IYZ2dnpKWlYdq0aQgPD4ebmxtiY2MVz70TEVHtHBLskZGRkCSpWrufnx8OHz780OP9/f2xe/duR5RGRCQ8viuGiEgwDHYiIsEw2ImIBMNgJyISDIOdiEgwDHYiIsEw2ImIBMNgJyISDIOdiEgwDHYiIsEw2ImIBMNgJyISDIOdiEgwDHYiIsEw2ImIBMNgJyISDIOdiEgwDHYiIsEw2ImIBMNgJyISDIOdiEgwDHYiIsEw2ImIBMNgJyISjN2DfeHChVCpVIotKChI7r937x7i4+PRunVrtGzZEiNHjoTJZFKco6CgANHR0WjRogW8vb0xd+5clJeX27tUIiIhuTjipF26dMG+fft+u4jLb5eZPXs2du3ahW3btkGn02H69Ol45ZVXcOTIEQBARUUFoqOjYTAYcPToUdy4cQMxMTFo1qwZFi9e7IhyiYiE4pBgd3FxgcFgqNZuNpvx+eefY9OmTXjhhRcAAOvXr0fnzp1x7NgxPPvss9i7dy/OnTuHffv2Qa/X4+mnn8YHH3yAxMRELFy4EGq12hElExEJwyFr7JcuXYKvry/at2+PsWPHoqCgAACQlZWFsrIyDBw4UB4bFBSEtm3bIjMzEwCQmZmJbt26Qa/Xy2OioqJgsViQm5tb6zWtVissFotiIyJ6Etk92MPCwpCamoo9e/Zg9erVyM/PR58+ffDrr7/CaDRCrVbDw8NDcYxer4fRaAQAGI1GRahX9Vf11SY5ORk6nU7e/Pz87DsxIqImwu5LMYMHD5Y/7t69O8LCwuDv74+tW7eiefPm9r6cLCkpCQkJCfK+xWJhuBPRE8nhjzt6eHigY8eOuHz5MgwGA0pLS1FUVKQYYzKZ5DV5g8FQ7SmZqv2a1u2raDQaaLVaxUZE9CRyeLAXFxfjypUr8PHxQWhoKJo1a4b9+/fL/Xl5eSgoKEB4eDgAIDw8HGfOnMHNmzflMenp6dBqtQgODnZ0uURETZ7dl2LeeecdDB06FP7+/rh+/ToWLFgAZ2dnjB49GjqdDnFxcUhISICnpye0Wi1mzJiB8PBwPPvsswCAyMhIBAcHY/z48UhJSYHRaMR7772H+Ph4aDQae5dLRCQcuwf7jz/+iNGjR+OXX35BmzZt0Lt3bxw7dgxt2rQBAHzyySdwcnLCyJEjYbVaERUVhc8++0w+3tnZGWlpaZg2bRrCw8Ph5uaG2NhYLFq0yN6lEhEJye7Bvnnz5gf2u7q6YtWqVVi1alWtY/z9/bF79257l0ZE9ETgu2KIiATDYCciEgyDnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiATDYCciEoxLQxdAVFcB7+56LNe5uiT6sVyHyFF4x05EJBi7B3tycjJ69eoFd3d3eHt7Y8SIEcjLy1OM6devH1QqlWKbOnWqYkxBQQGio6PRokULeHt7Y+7cuSgvL7d3uUREwrH7Uszhw4cRHx+PXr16oby8HP/v//0/REZG4ty5c3Bzc5PHTZo0CYsWLZL3W7RoIX9cUVGB6OhoGAwGHD16FDdu3EBMTAyaNWuGxYsX27tkogbBpSVyFLsH+549exT7qamp8Pb2RlZWFiIiIuT2Fi1awGAw1HiOvXv34ty5c9i3bx/0ej2efvppfPDBB0hMTMTChQuhVqvtXTYRkTAcvsZuNpsBAJ6enor2jRs3wsvLC127dkVSUhLu3Lkj92VmZqJbt27Q6/VyW1RUFCwWC3Jzc2u8jtVqhcViUWxERE8ihz4VU1lZiVmzZuH5559H165d5fYxY8bA398fvr6+yMnJQWJiIvLy8rB9+3YAgNFoVIQ6AHnfaDTWeK3k5GS8//77DpoJEVHT4dBgj4+Px9mzZ/Htt98q2idPnix/3K1bN/j4+GDAgAG4cuUKAgMD63WtpKQkJCQkyPsWiwV+fn71K5yIqAlz2FLM9OnTkZaWhoMHD+Kpp5564NiwsDAAwOXLlwEABoMBJpNJMaZqv7Z1eY1GA61Wq9iIiJ5Edg92SZIwffp07NixAwcOHEC7du0eekx2djYAwMfHBwAQHh6OM2fO4ObNm/KY9PR0aLVaBAcH27tkIiKh2H0pJj4+Hps2bcIXX3wBd3d3eU1cp9OhefPmuHLlCjZt2oQhQ4agdevWyMnJwezZsxEREYHu3bsDACIjIxEcHIzx48cjJSUFRqMR7733HuLj46HRaOxdMhGRUOx+x7569WqYzWb069cPPj4+8rZlyxYAgFqtxr59+xAZGYmgoCDMmTMHI0eOxFdffSWfw9nZGWlpaXB2dkZ4eDjGjRuHmJgYxXPvRERUM7vfsUuS9MB+Pz8/HD58+KHn8ff3x+7du+1VFhHRE4PviiEiEgyDnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMA59HzsRPRn491sbF96xExEJhsFORCQYBjsRkWAY7EREgmGwExEJhsFORCQYBjsRkWD4HDsR0R809efyecdORCQYBjsRkWAY7EREgmGwExEJhsFORCSYRh3sq1atQkBAAFxdXREWFoYTJ040dElERI1eow32LVu2ICEhAQsWLMCpU6cQEhKCqKgo3Lx5s6FLIyJq1Brtc+zLli3DpEmT8OabbwIA1qxZg127dmHdunV49913q423Wq2wWq3yvtlsBgBYLBabrltpvfMIVdedrXXVh0hzATif+uJ/a7ZrrPOpGi9J0oMHSo2Q1WqVnJ2dpR07dijaY2JipGHDhtV4zIIFCyQA3Lhx4yb8VlhY+MAMbZR37D///DMqKiqg1+sV7Xq9HhcuXKjxmKSkJCQkJMj7lZWVuHXrFlq3bg2VSuWwWi0WC/z8/FBYWAitVuuw6zwOIs0F4HwaM5HmAjy++UiShF9//RW+vr4PHNcog70+NBoNNBqNos3Dw+OxXV+r1QrxHygg1lwAzqcxE2kuwOOZj06ne+iYRvnDUy8vLzg7O8NkMinaTSYTDAZDA1VFRNQ0NMpgV6vVCA0Nxf79++W2yspK7N+/H+Hh4Q1YGRFR49dol2ISEhIQGxuLnj174s9//jOWL1+OkpIS+SmZxkKj0WDBggXVloGaIpHmAnA+jZlIcwEa33xUkvSw52YazsqVK7F06VIYjUY8/fTT+PTTTxEWFtbQZRERNWqNOtiJiMh2jXKNnYiI6o/BTkQkGAY7EZFgGOxERIJhsBNRg+BzG47TaJ9jb6x+/vlnrFu3DpmZmTAajQAAg8GA5557DhMmTECbNm0auEKipkGj0eD06dPo3LlzQ5ciHD7uaIOTJ08iKioKLVq0wMCBA+WXlJlMJuzfvx937tzBN998g549ezZwpXVz9+5dZGVlwdPTE8HBwYq+e/fuYevWrYiJiWmg6mx3/vx5HDt2DOHh4QgKCsKFCxewYsUKWK1WjBs3Di+88EJDl2g3hYWFWLBgAdatW9fQpTzU71/O93srVqzAuHHj0Lp1awD3X9XdFJWUlGDr1q24fPkyfHx8MHr0aHlODYXBboNnn30WISEhWLNmTbU3RkqShKlTpyInJweZmZkNVGHdXbx4EZGRkSgoKIBKpULv3r2xefNm+Pj4ALj/zcrX1xcVFRUNXGnd7NmzB8OHD0fLli1x584d7NixAzExMQgJCUFlZSUOHz6MvXv3ChPup0+fxjPPPNMkvj5OTk4ICQmp9lK+w4cPo2fPnnBzc4NKpcKBAwcapkAbBQcH49tvv4WnpycKCwsRERGB27dvo2PHjrhy5QpcXFxw7NgxtGvXrsFqZLDboHnz5vj+++8RFBRUY/+FCxfQo0cP3L179zFXZruXX34ZZWVlSE1NRVFREWbNmoVz587h0KFDaNu2bZML9ueeew4vvPACPvzwQ2zevBlvv/02pk2bhv/7v/8DcP+1zllZWdi7d28DV1o3X3755QP7f/jhB8yZM6dJfH2WLFmCtWvX4p///KfiG2uzZs1w+vTpav9abOycnJxgNBrh7e2NcePGIT8/H7t374ZOp0NxcTFefvlltGnTBps2bWq4Ih/h72E8cQICAqQNGzbU2r9hwwbJ39//8RX0CLy9vaWcnBx5v7KyUpo6darUtm1b6cqVK5LRaJScnJwasELbaLVa6dKlS5IkSVJFRYXk4uIinTp1Su4/c+aMpNfrG6o8m6lUKsnJyUlSqVS1bk3p63PixAmpY8eO0pw5c6TS0lJJkiTJxcVFys3NbeDKbKdSqSSTySRJkiS1b99e2rt3r6L/yJEjkp+fX0OUJuNTMTZ45513MHnyZMycORNffvkljh8/juPHj+PLL7/EzJkzMXXqVMybN6+hy6yTu3fvwsXlt5+dq1QqrF69GkOHDkXfvn1x8eLFBqyufqqWx5ycnODq6qp4b7W7u7v85xKbAh8fH2zfvh2VlZU1bqdOnWroEm3Sq1cvZGVl4aeffkLPnj1x9uxZh/4BHEerqv3evXvy8mWVP/3pT/jpp58aoiwZn4qxQXx8PLy8vPDJJ5/gs88+k/8Z7OzsjNDQUKSmpuL1119v4CrrJigoCN999121JxJWrlwJABg2bFhDlFVvAQEBuHTpEgIDAwEAmZmZaNu2rdxfUFBQ7X/Axiw0NBRZWVkYPnx4jf0qlarJPS7YsmVLbNiwAZs3b8bAgQObxDJSbQYMGAAXFxdYLBbk5eWha9euct+1a9ca/IenDHYbvfHGG3jjjTdQVlaGn3/+GcD9PwzSrFmzBq7MNi+//DL+85//YPz48dX6Vq5cicrKSqxZs6YBKqufadOmKYLi9/+jAcDXX3/dpH5wOnfuXJSUlNTa36FDBxw8ePAxVmQ/o0aNQu/evZGVlQV/f/+GLsdmCxYsUOy3bNlSsf/VV1+hT58+j7OkavjDUyIiwXCNnYhIMAx2IiLBMNiJiATDYCciEgyDnYhIMAx2IiLBMNiJiATz/wH+0hRI+7pVCAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_class_distribution(df: pd.DataFrame):\n",
    "    fig = plt.figure(figsize=(4,4))\n",
    "    df['Species'].value_counts().plot.bar()\n",
    "    print(df['Species'].value_counts())\n",
    "    plt.title('Count Distribution')\n",
    "    plt.savefig(\"./data/class_distribution\")\n",
    "    plt.show(fig)\n",
    "    \n",
    "plot_class_distribution(df_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6aa44a-ec8c-44a7-87c4-e7f5b2a2992e",
   "metadata": {},
   "source": [
    "#### Bilder aus ordner lesen, Klasse zuordnen und in DataFrame wandeln\n",
    "In diesen Funktionen werden die bearbeiten Bilder geladen, ihnen die richtige Klasse zugeordnet und anschließend in einem DataFrame gespeichert.\n",
    "**Hinweis:** Hier wird erneut tqdm verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "41b8d4bf-f679-4e88-acbf-082cd4f6834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given an ID return the according class\n",
    "def get_classes_from_id(id: int, df: pd.DataFrame) -> pd.Series:\n",
    "    for row in df.iterrows():\n",
    "        if id == row[1][\"ID\"]:\n",
    "            return row[1].drop(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdc0b485-194e-43e3-8df8-6a4f8e80204c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076452f1535546c6b1066830817d4b9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The final image shape/size[hwc] is: (82, 128, 3)\n"
     ]
    }
   ],
   "source": [
    "def load_images_from_folder(folder_path: str, df_categorical=pd.DataFrame(), df_numerical=pd.DataFrame(), df_encoded=pd.DataFrame()) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    # Sicherstellen, dass mindestens ein Dataframe gegeben ist\n",
    "    if not df_categorical.empty and not df_numerical.empty and not df_encoded.empty:\n",
    "        raise ValueError(\"You have to define at least one Dataframe.\")\n",
    "        \n",
    "    images_categorical = list()\n",
    "    images_numerical = list()\n",
    "    images_encoded = list()\n",
    "    column_names = [\"data\", \"Species\"]\n",
    "    column_names_encoded = [\"data\"] + list(df_encoded.columns)[1:]\n",
    "    \n",
    "    for i, filename in enumerate(tqdm(os.listdir(folder_path))):\n",
    "        img_path = os.path.join(folder_path, filename)\n",
    "        if os.path.isfile(img_path):  \n",
    "            # read in image\n",
    "            img = cv2.imread(img_path, -1)\n",
    "            # some images are broken\n",
    "            if img is not None:\n",
    "                # for the first image, save the size of the image to a seperate file, which makes it easier for other\n",
    "                #   notebooks to get the image size & channels\n",
    "                if i == 0:\n",
    "                    print(\"The final image shape/size[hwc] is:\",img.shape)\n",
    "                    # store image shape in file\n",
    "                    with open(\"./data/meta.json\",\"w+\") as file:\n",
    "                        file.write(json.dumps({\"h\":img.shape[0],\"w\":img.shape[1],\"c\":img.shape[2]}))\n",
    "                \n",
    "                # Find the Class for the Image based on ID and type of given Class representation\n",
    "                class_categorical = None\n",
    "                class_numerical = None\n",
    "                class_encoded = None\n",
    "                if not df_categorical.empty:\n",
    "                    class_categorical = get_classes_from_id(int(filename[:-4]), df_categorical)\n",
    "                if not df_numerical.empty:\n",
    "                    class_numerical = get_classes_from_id(int(filename[:-4]), df_numerical)\n",
    "                if not df_encoded.empty:\n",
    "                    class_encoded = get_classes_from_id(int(filename[:-4]), df_encoded)\n",
    "                    \n",
    "                # need to check, if the class of the image is not null [aka. image would be one of the unwanted\n",
    "                #   datapoints (e.g. class Schwarzbild)]\n",
    "                if (class_categorical is not None) or (class_numerical is not None) or (class_encoded is not None):\n",
    "                    if not df_categorical.empty:\n",
    "                        images_categorical.append([img.flatten(), *class_categorical.values])\n",
    "                    if not df_numerical.empty:\n",
    "                        images_numerical.append([img.flatten(), *class_numerical.values])\n",
    "                    if not df_encoded.empty:\n",
    "                        images_encoded.append([img.flatten(), *class_encoded.values])\n",
    "\n",
    "    return_dfs = dict()\n",
    "\n",
    "    if not df_categorical.empty:\n",
    "        return_dfs[\"df_categorical\"] = pd.DataFrame(np.array(images_categorical, dtype=object), columns=column_names)\n",
    "    if not df_numerical.empty:\n",
    "        return_dfs[\"df_numerical\"] = pd.DataFrame(np.array(images_numerical, dtype=object), columns=column_names)\n",
    "    if not df_encoded.empty:\n",
    "        return_dfs[\"df_encoded\"] = pd.DataFrame(np.array(images_encoded, dtype=object), columns=column_names_encoded).drop(\"Species\", axis=1)\n",
    "\n",
    "    return return_dfs\n",
    "\n",
    "# use function\n",
    "dfs = load_images_from_folder(\"./compressed_pictures/\", df_numerical=df_numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40f6936",
   "metadata": {},
   "source": [
    "#### Normalisierung der Daten\n",
    "\n",
    "Daten werden auf in ein Intervall von [0, 1] gebracht, dadurch wird die Konvergenzgeschwindigkeit verbessert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e89c4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in dfs.items():\n",
    "    df['data'] = df['data'] / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9ac98-0acb-4f49-89ae-599a8ee3882e",
   "metadata": {},
   "source": [
    "#### DataFrame als Pickles Datei speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "176f3da1-7452-4642-abfa-f443937208dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for df_name, df in dfs.items():\n",
    "    if df_name == \"df_categorical\":\n",
    "        dfs[\"df_categorical\"].to_pickle(\"./data/images_df_categorical.pkl\")\n",
    "    if df_name == \"df_numerical\":\n",
    "        dfs[\"df_numerical\"].to_pickle(\"./data/images_df_numerical.pkl\")\n",
    "    if df_name == \"df_encoded\":\n",
    "        dfs[\"df_encoded\"].to_pickle(\"./data/images_df_encoded.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d731c0b4",
   "metadata": {},
   "source": [
    "#### Daten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b80ee6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dfs[\"df_numerical\"]\n",
    "X, y = data['data'], data['Species']\n",
    "X, y = np.stack(X).astype(np.uint8), y.to_numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b47f6e4",
   "metadata": {},
   "source": [
    "## Resampling\n",
    "\n",
    "Aufgrund der sehr ungleich verteilten Klassen haben wir mit Oversampling (Adasyn) und einer Kombination von Under- und Oversampling (Smoteen) experimentiert. Alleiniges undersampling haben wir als nicht sinnvoll erachtet, da selbst die größten Klassen recht wenig Datenpunkte enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2581b44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entfernt Klassen >= 60 Samples\n",
    "#data = data[data['Species'].isin(counts[counts>=60].index)]\n",
    "# Entfernt Klassen >= 200 Samples\n",
    "#data = data[data['Species'].isin(counts[counts>=200].index)]\n",
    "# Entfernt Klassen >= 500 Samples\n",
    "#data = data[data['Species'].isin(counts[counts>=500].index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e41b5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "def resample(resampler) -> tuple[np.array, np.array]:\n",
    "    X, y = data['data'], data['Species']\n",
    "    X, y = np.stack(X).astype(np.uint8), y.to_numpy().astype(np.uint8)\n",
    "\n",
    "    _, dist = np.unique(y, return_counts=True)\n",
    "    print(f\"{resampler} before res:\",dist)\n",
    "\n",
    "    X_resampled, y_resampled = resampler.fit_resample(X, y)\n",
    "\n",
    "    _, dist = np.unique(y_resampled, return_counts=True)\n",
    "    print(f\"{resampler} after res:\", dist)\n",
    "\n",
    "    # shuffle ist nötig, da die synthetischen Daten nach Labels geordnet sind (random_state für bessere nachvollziehbarkeit)\n",
    "    X_resampled, y_resampled = shuffle(X_resampled, y_resampled, random_state=1)\n",
    "\n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79c564ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "with open(\"./data/meta.json\",\"r\") as file:  # get metadata for images from file generated from import nb\n",
    "    image_meta = json.load(file)\n",
    "image_shape = (image_meta['h'],image_meta['w'],image_meta['c'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522df056",
   "metadata": {},
   "source": [
    "**Hinweis:** Die resample_after_split-Funktion wurde benutzt um die Mengen nach dem KFold-Split einzeln zu resamplen, also z. B. nur train. Es ist der vollständigkeitshalber an dieser Stelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac1c6522",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "def resample_after_split(X,y, resampler) -> tuple[np.array, np.array]:\n",
    "    n, h, w, _ = X.shape\n",
    "    X = X.reshape((n, h * w * 3))\n",
    "    _, dist = np.unique(y, return_counts=True)\n",
    "    print(f\"{resampler} before res:\",dist)\n",
    "\n",
    "    X_resampled, y_resampled = resampler.fit_resample(X, y)\n",
    "\n",
    "    _, dist = np.unique(y_resampled, return_counts=True)\n",
    "    print(f\"{resampler} after res:\", dist)\n",
    "\n",
    "    X_resampled = X_resampled.reshape((-1,) + image_shape)\n",
    "\n",
    "    # shuffle ist nötig, da die synthetischen Daten nach Labels geordnet sind (random_state für bessere nachvollziehbarkeit)\n",
    "    X_resampled, y_resampled = shuffle(X_resampled, y_resampled, random_state=1)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# resample_after_split = SMOTEENN(sampling_strategy='all',enn=ENN(kind_sel='mode',n_neighbors=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcf76f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# oversampling\n",
    "adasyn = ADASYN()\n",
    "X_adasyn, y_adasyn = resample(adasyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1048c4c",
   "metadata": {},
   "source": [
    "**Hinweis:** SMOTETomek haben wir nicht weiter beachtet, da die Klassenverteilung sehr stark Adasyn ähnelte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14d6478f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "# under- und oversampling\n",
    "somteenn = SMOTEENN()\n",
    "X_somteenn, y_somteenn = resample(somteenn)\n",
    "smotetomek = SMOTETomek()\n",
    "X_smotetomek, y_smotetomek = resample(smotetomek)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0013c3",
   "metadata": {},
   "source": [
    "**Hinweis:** Die Tests wurden auf unserem Hauptmodel CNN durchgeführt (s. CNN-Notebook)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe768cc",
   "metadata": {},
   "source": [
    "|Durchführung|Avg. Test Acc|\n",
    "|:-|-:|\n",
    "| Kein Resample, top 6 classes (> 60), cnn  | 79% |\n",
    "| Kein Resample, top 4 classes (>200), cnn  | 83% |\n",
    "| Kein Resample, top 2 classes (>500), cnn  | 87% |\n",
    "|---------------|\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=3)), top 6 classes (> 60), cnn   | 63% |\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=3)), top 4 classes (>200), cnn   | 80% |\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=3)), top 2 classes (>500), cnn   | 87% |\n",
    "|---------------|\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=7)), top 6 classes (> 60), cnn   | 68% |\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=7)), top 4 classes (>200), cnn   | 79% |\n",
    "| Train & Validation Resample (Smotteen(all,enn=Mode,n=7)), top 2 classes (>500), cnn   | 86% |\n",
    "|---------------|\n",
    "| Train Resample (Smotteen(all,enn=Mode,n=3)), top 6 classes (> 60), cnn   | 73% |\n",
    "| Train Resample (Smotteen(all,enn=Mode,n=3)), top 4 classes (>200), cnn   | 81% |\n",
    "| Train Resample (Smotteen(all,enn=Mode,n=3)), top 2 classes (>500), cnn   | 88% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31063780",
   "metadata": {},
   "source": [
    "**Erbenis:** Man sieht, dass das Resampling leider in der Form keinen Vorteil bringt daher haben wir die Idee verworfen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9dda23",
   "metadata": {},
   "source": [
    "**Hinweis:** Wir halten die ungleiche Klassenverteilung bzw. den Mangel an Samples einiger Klassen für den Hebel mit dem meisten Verbesserungspotential. (Wenn wir die Testdaten mit resampeln erhalten wir einen Score von bis zu 92% s. unten)\n",
    "Mit mehr Zeit/Domänenwissen wären folgende Dinge denkbar:\n",
    "* Echte Daten vor allem der unterrepräsentierten Daten sammeln\n",
    "* Synthetische Daten erzeugen, welche sehr nah den der Orginaldaten sind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd70bd7",
   "metadata": {},
   "source": [
    "#### Chunks um RAM zu sparen\n",
    "\n",
    "Da wir extrem große RAM Probleme hatten haben wir mit Aufteilung der Daten in Chunks experimentiert, aber die Idee dann wieder verworfen, da nach der Verkleinerung der Bilder es auch ohne geklappt hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7abae3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "def split_df_equal_class_distribution(df, batch_size):\n",
    "    \n",
    "    df['temp_id'] = range(len(df))\n",
    "    \n",
    "    num_batches = int(np.ceil(len(df) / batch_size))\n",
    "    \n",
    "    grouped = df.groupby('Species', group_keys=False)\n",
    "    \n",
    "    chunks = []\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        chunk = pd.DataFrame(columns=df.columns)\n",
    "        for _, group in grouped:\n",
    "            num_samples = int(batch_size * len(group) / len(df))\n",
    "            sample_indices = np.random.choice(group['temp_id'], size=num_samples, replace=False)\n",
    "            chunk = pd.concat([chunk, df[df['temp_id'].isin(sample_indices)]])\n",
    "        chunk = chunk.drop('temp_id', axis=1)\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "batch_size = 1000\n",
    "\n",
    "chunks_with_same_dist = split_df_equal_class_distribution(data, batch_size)\n",
    "del data\n",
    "classes = chunks_with_same_dist[0][\"Species\"].unique()\n",
    "number_of_classes = classes.size\n",
    "most_x_in_one_class = chunks_with_same_dist[0][\"Species\"].value_counts().iloc[0]\n",
    "\n",
    "def resample(resampler) -> tuple[np.array, np.array]:\n",
    "    # 0.3 als Puffer, weil sich der Output der verschiedenen Resampler ja unterscheidet\n",
    "    array_size = int(most_x_in_one_class * number_of_classes * (len(chunks_with_same_dist) + 0.3))\n",
    "    # 216432 war die Länge des Vektors der nicht verkleinerten Bilder\n",
    "    X = np.empty((array_size, 216432), dtype=np.uint8)\n",
    "    y = np.empty((array_size), dtype=np.uint8)\n",
    "\n",
    "    current_index = 0\n",
    "    for chunk in chunks_with_same_dist:\n",
    "        X_batch, y_batch = chunk['data'], chunk['Species']\n",
    "        X_batch, y_batch = np.stack(X_batch).astype(np.uint8), y_batch.astype(np.uint8)\n",
    "        X_resampled, y_resampled = resampler.fit_resample(X_batch, y_batch)\n",
    "        num_samples = X_resampled.shape[0]\n",
    "        X[current_index:current_index + num_samples] = X_resampled.astype(np.uint8)\n",
    "        y[current_index:current_index + num_samples] = y_resampled.astype(np.uint8)\n",
    "        current_index += num_samples\n",
    "\n",
    "    X.resize((current_index, X.shape[1]))\n",
    "    y.resize(current_index)\n",
    "    print(f\"{resampler}: \", pd.Series(y, dtype=pd.UInt8Dtype()).value_counts())\n",
    "\n",
    "    return X, y\n",
    "\n",
    "num_samples = (len(X) * 0.8)\n",
    "chunk_size = 1000\n",
    "for i in range(0, num_samples, chunk_size):\n",
    "    X[i:i+chunk_size], y[i:i+chunk_size] # Hier ist der Code abgeschnitten, da nur die Idee erläutert werden soll "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059f9f50",
   "metadata": {},
   "source": [
    "#### Bilder ausgeben\n",
    "\n",
    "Für uns waren die Unterscheidung zwischen den Bildern der meisten Klassen nicht möglich, weshalb Domänenwissen ein sehr starken Vorteil (und in unserem Projekt auch teilweise Problem) darstellt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ab95a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "print(image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2016888",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script false --no-raise-error\n",
    "n = 20  # Anzahl der gezeigten Bilder\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    if image_shape[2] == 1:\n",
    "        plt.imshow((X[i]*255.).astype('uint8'), cmap='inferno', vmin=0, vmax=255) # for grayscale images\n",
    "        plt.set_suptitle(f\"{y[i]}\")\n",
    "    elif image_shape[2] == 4 or image_shape[2] == 3:\n",
    "        plt.imshow(cv2.cvtColor((X[i]).astype('uint8'),cv2.COLOR_BGR2RGB)) # for rgb images\n",
    "        ax.title.set_text(f\"{y[i]}\")\n",
    "    #plt.color()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.savefig(\"./pics.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661d0e33-0ba1-4f52-b967-327d9e732c90",
   "metadata": {},
   "source": [
    "# Anmerkungen zu getesteten Bildbearbeitungen\n",
    "FÜr den ersten Teil der Bildaufbereitung werden hier keine Filter ö.ä. Bildbearbeitungen verwendet.\n",
    "Allerdings haben wir im Laufe der Bearbeitung verschiedene Filter u.ä. ausprobiert, welche inkl. ihres Test Accuracy Wertes gelistet sind.\n",
    "\n",
    "**Hinweis:** Die Test Accuracy Werte entsprechen denen unseres Modells, auf Basis der mit Adasyn vollständig geresampleten (auch Testdaten) Bilder. Dies hat den Grund, dass die Tests durchgeführt wurden bevor wir den Fehler entdeckt haben und die Zeit für neue Tests nicht mehr ausgereicht hat. Somit sind sie ca 10-30% höher, als auf den ungresampleten Bildern. Mehr dazu im `cnn.ipynb`.\n",
    "\n",
    "\n",
    "|Bearbeitung (In Anwendungsreihenfolge)|Test Accuracy|\n",
    "|:-|-:|\n",
    "|Turn Pictrues into single channel grayscale|78%|\n",
    "|Turn Pictrues into single channel grayscale + fastNLMeansDenoising|78%|\n",
    "|fastNLMeansDenoising after downscaling|80%|\n",
    "|Bilateral Filter|82%|\n",
    "|Color Histogram Equalization + Contrast(*1.7) + Brightness(-150) + Bilateral Filter|82%\n",
    "|fastNLMeansDenoising before downscaling|83%|\n",
    "|Outlinedetection Conv2D Filter + Bilateral Filter + fastNLMeansDenoising + Contrast(*1.7) + Brightness(-150) + Sobel Conv2D Filter| 88%|\n",
    "|Contrast(*1.5) + Brightness(-150)|88%|\n",
    "|Contrast(*1.7) + Brightness(-100)|89%|\n",
    "|Contrast(*1.7) + Brightness(-150)|89%|\n",
    "|Sobel Conv2D Filter + Bilateral Filter + fastNLMeansDenoising + Bilateral Filter + Contrast(*1.7) + Brightness(-150)| 89%|\n",
    "|Bilateral Filter + Contrast(*1.7) + Brightness(-100)|90%|\n",
    "|Bilateral Filter + Contrast(*1.7) + Brightness(-150)|90%|\n",
    "|Outlinedetection Conv2D Filter + Bilateral Filter + fastNLMeansDenoising + Bilateral Filter + Contrast(*1.7) + Brightness(-150)| 91%|\n",
    "|Outlinedetection Conv2D Filter + Bilateral Filter + fastNLMeansDenoising + Bilateral Filter + Contrast(*1.7) + Brightness(-150) + Saturation=255| 91%|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652b855d-ad57-4094-b2e9-797886fd8756",
   "metadata": {},
   "source": [
    "Eine Veranschaulichung dieser und weitere Filter/Bildbearbeitungen ist im folgenden zu sehen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "793c1b8e-236d-4253-a985-a261d1520a34",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Future exception was never retrieved\n",
      "future: <Future finished exception=BrokenPipeError(32, 'Broken pipe')>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.10/asyncio/unix_events.py\", line 676, in write\n",
      "    n = os.write(self._fileno, data)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.1) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [34], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m image_shape \u001b[38;5;241m=\u001b[39m (image_meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m],image_meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m],image_meta[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m# hwc\u001b[39;00m\n\u001b[1;32m     42\u001b[0m image\u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mreshape(image_shape)\n\u001b[0;32m---> 43\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcvtColor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCOLOR_BGRA2BGR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Gaußscher Weichzeichner\u001b[39;00m\n\u001b[1;32m     46\u001b[0m gaussian_blur \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mGaussianBlur(image, (\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m), \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.1) /io/opencv/modules/imgproc/src/color.simd_helpers.hpp:94: error: (-2:Unspecified error) in function 'cv::impl::{anonymous}::CvtHelper<VScn, VDcn, VDepth, sizePolicy>::CvtHelper(cv::InputArray, cv::OutputArray, int) [with VScn = cv::impl::{anonymous}::Set<3, 4>; VDcn = cv::impl::{anonymous}::Set<3, 4>; VDepth = cv::impl::{anonymous}::Set<0, 2, 5>; cv::impl::{anonymous}::SizePolicy sizePolicy = cv::impl::<unnamed>::NONE; cv::InputArray = const cv::_InputArray&; cv::OutputArray = const cv::_OutputArray&]'\n> Unsupported depth of input image:\n>     'VDepth::contains(depth)'\n> where\n>     'depth' is 6 (CV_64F)\n"
     ]
    }
   ],
   "source": [
    "##### spektrogramme anzeigen\n",
    "def show_images(images, titles):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, (img, title) in enumerate(zip(images, titles), 1):\n",
    "        plt.subplot(8, 4, i)\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "    plt.savefig(\"denoise_comaprison.png\")\n",
    "    plt.show()\n",
    "\n",
    "# color distribution to show differences in intesities etc\n",
    "def show_hist(images, titles):\n",
    "    plt.figure(figsize=(20, 20))\n",
    "    for i, (image, title) in enumerate(zip(images, titles), 1):\n",
    "        plt.subplot(8, 4, i)\n",
    "        plt.title(title)\n",
    "        #plt.axis('off')\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        hist_r = cv2.calcHist([image_rgb], [0], None, [256], [0, 256]) / (image_size/3)\n",
    "        hist_g = cv2.calcHist([image_rgb], [1], None, [256], [0, 256]) / (image_size/3)\n",
    "        hist_b = cv2.calcHist([image_rgb], [2], None, [256], [0, 256]) / (image_size/3)\n",
    "\n",
    "        plt.plot(hist_r, color='red', label='Red')\n",
    "        plt.plot(hist_g, color='green', label='Green')\n",
    "        plt.plot(hist_b, color='blue', label='Blue')\n",
    "        plt.xlabel('Pixel Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.xlim([0, 255])\n",
    "        plt.ylim([0, 0.3])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"denoise_comaprison_hist.png\")\n",
    "    plt.show()\n",
    "\n",
    "# Laden eines Beispielbildes\n",
    "image = dfs['df_numerical']['data'][0]\n",
    "image_size = image.size\n",
    "with open(\"./data/meta.json\",\"r\") as file:  # get metadata for images from file generated from import nb\n",
    "    image_meta = json.load(file) \n",
    "image_shape = (image_meta['h'],image_meta['w'],image_meta['c']) # hwc\n",
    "image= image.reshape(image_shape)\n",
    "image = cv2.cvtColor(image,cv2.COLOR_BGRA2BGR)\n",
    "\n",
    "# Gaußscher Weichzeichner\n",
    "gaussian_blur = cv2.GaussianBlur(image, (5, 5), 0)\n",
    "\n",
    "# Mittelwertfilter\n",
    "mean_blur = cv2.blur(image, (5, 5))\n",
    "\n",
    "# Medianfilter\n",
    "median_blur = cv2.medianBlur(image, 5)\n",
    "\n",
    "# Bilateralfilter\n",
    "bilateral_blur = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "\n",
    "# Bewegungsunschärfe\n",
    "kernel_motion_blur = np.zeros((5, 5))\n",
    "kernel_motion_blur[:, int((5-1)/2)] = 1\n",
    "kernel_motion_blur /= 5\n",
    "motion_blur = cv2.filter2D(image, -1, kernel_motion_blur)\n",
    "\n",
    "# predcit based on autoencoder\n",
    "#autoencoder = keras.saving.load_model('../autoencoder_files/autoencoder_cnn_denoise_v7.keras')\n",
    "#imagelist = np.asarray([image]).astype('float32')/255.\n",
    "#autoencoder_img = (autoencoder.predict(imagelist)[0]*255.).astype('uint8')\n",
    "\n",
    "# nl means denosing gray\n",
    "fast_nl_grey = cv2.fastNlMeansDenoising(image, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "# nl means denosing color\n",
    "fast_nl_color = cv2.fastNlMeansDenoisingColored(image, None, 15,15, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "# contrast/birghness\n",
    "brightness = 1 \n",
    "contrast = 1.5\n",
    "c_b = cv2.addWeighted(image, contrast, np.zeros(image.shape, image.dtype), 0, brightness)\n",
    "\n",
    "\n",
    "# cb + nlmeans\n",
    "brightness = 0 \n",
    "contrast = 1.5\n",
    "cb_nl = cv2.fastNlMeansDenoising(cv2.addWeighted(image, contrast, np.zeros(image.shape, image.dtype), 0, brightness), None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "\n",
    "\n",
    "# + cont\n",
    "cont1 = cv2.addWeighted(image, 1.5, np.zeros(image.shape, image.dtype), 0, -150)\n",
    "\n",
    "cont2 = cv2.addWeighted(image, 1.7, np.zeros(image.shape, image.dtype), 0, -150)\n",
    "\n",
    "cont3 = cv2.addWeighted(image, 2, np.zeros(image.shape, image.dtype), 0, -150)\n",
    "\n",
    "# bil + cont\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "bil_cont1 = cv2.addWeighted(img, 1.5, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "bil_cont2 = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "bil_cont3 = cv2.addWeighted(img, 2, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "# bil+c2+b-150+bil+c2+b-200\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 2, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "bl_cb_bil_cb1 = cv2.addWeighted(img, 2, np.zeros(img.shape, img.dtype), 0, -200)\n",
    "\n",
    "#sharpen kernel\n",
    "kernel = np.array([[-1, -1, -1],\n",
    "                       [-1, 9, -1],\n",
    "                       [-1, -1, -1]])\n",
    "sharp_img = cv2.filter2D(image, -1, kernel)\n",
    "\n",
    "#sharpen laplace\n",
    "sharp_lapla = cv2.Laplacian(image, cv2.CV_8U) \n",
    "\n",
    "# equ hist\n",
    "img = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "equ_hist1 = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# equ hist + contrast\n",
    "img = cv2.addWeighted(image, 1.7, np.zeros(image.shape, image.dtype), 0, -150)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "equ_hist2 = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# equ hist + contrast + bilateral\n",
    "img = cv2.bilateralFilter(image, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "equ_hist3 = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# erode\n",
    "kernel = np.array([[1, 9, 1],\n",
    "                   [1, 9, 1],\n",
    "                   [1, 9, 1]])\n",
    "eroded1 = cv2.erode(image, kernel)\n",
    "\n",
    "# c/b + equ hsit + bil\n",
    "img = cv2.addWeighted(img, 2, np.zeros(img.shape, img.dtype), 0, -100)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "eroded2 = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "\n",
    "# filter 2d sharp\n",
    "kernel = np.array([[0, -1, 0],\n",
    "                  [-1, 5, -1],\n",
    "                  [0, -1, 0]])\n",
    "filter2d_1 = cv2.filter2D(image,-1,kernel)\n",
    "\n",
    "# filter 2d outline\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 8, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "filter2d_2 = cv2.filter2D(image,-1,kernel)\n",
    "\n",
    "# filter 2d emboss\n",
    "kernel = np.array([\n",
    "  [-2, -1, 0],\n",
    "  [-1, 1, 1],\n",
    "  [0, 1, 2]\n",
    "])\n",
    "filter2d_3 = cv2.filter2D(image,-1,kernel)\n",
    "\n",
    "#filter2d outline + bil\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 8, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "img = cv2.filter2D(image,-1,kernel)\n",
    "fil_bil1 = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "\n",
    "#filter2d (-1/7)+ bil + nl + bil + c1.7/b-150  (best thing yet)\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 7, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "#img = cv2.cvtColor(img, cv2.COLOR_BGRA2BGR)\n",
    "img = cv2.filter2D(image,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "fil_nl1 = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "#filter2d (-1/7)+ bil + nl + c1.7/b-150 (okay)\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 7, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "img = cv2.filter2D(image,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "fil_nl2 = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "\n",
    "#filter2d (outline)+ bil + nl + c1.7/b-150 + f2d (sobel)\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 7, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "img = cv2.filter2D(image,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "kernel = np.array([\n",
    "  [-1, 0, 1],\n",
    "  [-2, 0, 2],\n",
    "  [-1, 0, 1]\n",
    "])\n",
    "img = cv2.filter2D(img,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2YUV)\n",
    "img[:,:,0] = cv2.equalizeHist(img[:,:,0])\n",
    "fil_nl3 = cv2.cvtColor(img, cv2.COLOR_YUV2BGR)\n",
    "\n",
    "# best thing + emboss\n",
    "kernel = np.array([\n",
    "  [-1, -1, -1],\n",
    "  [-1, 7, -1],\n",
    "  [-1, -1, -1]\n",
    "])\n",
    "img = cv2.cvtColor(image, cv2.COLOR_BGRA2BGR)\n",
    "img = cv2.filter2D(img,-1,kernel)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.fastNlMeansDenoising(img, None, h=15, templateWindowSize=7, searchWindowSize=21)\n",
    "img = cv2.bilateralFilter(img, 9, 75, 75)\n",
    "img = cv2.addWeighted(img, 1.7, np.zeros(img.shape, img.dtype), 0, -150)\n",
    "kernel = np.array([\n",
    "  [-2, -1, 0],\n",
    "  [-1, 1, 1],\n",
    "  [0, 1, 2]\n",
    "])\n",
    "fil_nl4 = cv2.filter2D(img,-1,kernel)\n",
    "\n",
    "list = ([image, gaussian_blur, mean_blur, median_blur, bilateral_blur, motion_blur,fast_nl_grey,fast_nl_color,\n",
    "         c_b,cb_nl,cont1,cont2,cont3,bil_cont1,bil_cont2,bil_cont3,sharp_img,sharp_lapla,equ_hist1,equ_hist2,\n",
    "         equ_hist3,bl_cb_bil_cb1,eroded1, eroded2,filter2d_1,filter2d_2,filter2d_3,fil_bil1,fil_nl1,fil_nl2,fil_nl3,\n",
    "         fil_nl4\n",
    "        ],\n",
    "        ['Original', 'Gaussian Blur', 'Mean Blur', 'Median Blur', 'Bilateral Filter', 'Motion Blur','NL Means Grey',\n",
    "         'NL Means Color','Color/Brightness','CB/NL','C/B (1.5,-150)','C/B (1.7,-150)','C/B (2,-150)',\n",
    "         'BF/Cont/Br (1.5,-150)', 'BF/Cont/Br (1.7,-150)', 'BF/Cont/Br (2,-150)', 'Sharp Kernel', 'Sharp Laplace',\n",
    "         'Histrogramm Equ.', 'HE+Contrast', 'HE+Cont+Bil', \"bil+c2+b-150+bil+c2+b-200\", \"Eroded\", \"+C/B\", 'Filter2D-Sharp',\n",
    "         \"Filter2D-Outline\", \"Filter2D-Emboss\", \"Fil2dOutline+Bil\", \"F2D+bil+nl+bil+cb (best)\", \"F2D+bil+nl+bil+cb+F2D\",\n",
    "         \"F2D+bil+nl+bil+cb\", \"best thing + emboss\"])\n",
    "\n",
    "# Anzeigen der Ergebnisse\n",
    "show_images(*list)\n",
    "show_hist(*list)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main CNN model for bat call classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# dont use gpu [for M1/M2 Metal, cause it cant handle Dropout Layers too well], comment out if you want to use GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')  # this +\n",
    "tf.config.set_visible_devices(physical_devices[1:], 'GPU') # this fixes the above mentioned bug with the Droputlayers. idfk why\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import backend as K \n",
    "import gc\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import json\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./data/images_df_numerical.pkl')\n",
    "classes = data[\"Species\"].unique()\n",
    "number_of_classes = classes.size\n",
    "\n",
    "# Alleiniges undersampling wird keinen Sinn machen, da wir extrem wenig Datenpunkte overall haben\n",
    "def resample(resampler) -> tuple[np.array, np.array]:\n",
    "    X, y = data['data'], data['Species']\n",
    "    X, y = np.stack(X).astype(np.uint8), y.to_numpy().astype(np.uint8)\n",
    "    X_resampled, y_resampled = resampler.fit_resample(X, y)\n",
    "\n",
    "    print(f\"{resampler}: \", pd.Series(y_resampled, dtype=pd.UInt8Dtype()).value_counts())\n",
    "\n",
    "    X_resampled, y_resampled = shuffle(X_resampled, y_resampled, random_state=1)\n",
    "\n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "# oversampling\n",
    "adasyn = ADASYN()\n",
    "\n",
    "X, y = resample(adasyn)\n",
    "#X, y = data['data'], data['Species']\n",
    "#X, y = np.stack(X).astype(np.uint8), y.to_numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = X[0].size\n",
    "samples = X.size\n",
    "with open(\"./data/meta.json\",\"r\") as file:  # get metadata for images from file generated from import nb\n",
    "    image_meta = json.load(file) \n",
    "image_shape = (image_meta['h'],image_meta['w'],image_meta['c']) # hwc\n",
    "print(\"Image has shape\", image_shape)\n",
    "# normalize to 0-1\n",
    "\n",
    "\n",
    "X = X / 255.\n",
    "X = X.reshape((-1,) + image_shape)\n",
    "\n",
    "# denoise\n",
    "#autoencoder = tf.keras.saving.load_model('./ae_files/denoise_autoencoder_cnn.keras')\n",
    "#X = autoencoder.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show sample of generated images\n",
    "n = 10  # How many digits we will display\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(n):\n",
    "    # Display original\n",
    "    ax = plt.subplot(2, n, i + 1)\n",
    "    if image_shape[2]==1:\n",
    "        plt.imshow((X[i]*255.).astype('uint8'),cmap='inferno',vmin=0, vmax=255) # for grayscale images\n",
    "    elif image_shape[2]==4 or image_shape[2]==3:\n",
    "        plt.imshow(cv2.cvtColor((X[i]*255.).astype('uint8'),cv2.COLOR_BGR2RGB)) # for rgb images\n",
    "    \n",
    "    #plt.color()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=2, shuffle=True) # orig n=10 splits\n",
    "\n",
    "tf.keras.utils.set_random_seed(1)\n",
    "\n",
    "# If using TensorFlow, this will make GPU ops as deterministic as possible,\n",
    "# but it will affect the overall performance, so be mindful of that.\n",
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=30, min_delta=0.001, start_from_epoch=15, restore_best_weights=True)\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "dropout_rate = 0.2 #or 0.3 # https://www.kaggle.com/code/rafjaa/dealing-with-very-small-datasets interessant bzgl oberfitting\n",
    "weight_decay_alpha = 0.01 # or 0.015\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=image_shape,name=\"aaa\"))\n",
    "    model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    #model.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "    #model.add(tf.keras.layers.BatchNormalization())\n",
    "    #model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(number_of_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, X_train, y_train, worker=8):\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        workers=worker, # workers are number of cores\n",
    "        callbacks=[early_stopping, tfmot.sparsity.keras.UpdatePruningStep()],\n",
    "        validation_split=0.2,\n",
    "        verbose=1)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_step = np.ceil(X.shape[0] / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "pruning_params = {\n",
    "    # In this example, you start the model with 50% sparsity (50% zeros in weights) and end with 80% sparsity.\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                                final_sparsity=0.80,\n",
    "                                                                begin_step=0,\n",
    "                                                                end_step=end_step)}\n",
    "\n",
    "model = create_model()\n",
    "model = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for confusion_matrix\n",
    "true_labels = list()\n",
    "pred_labels = list()\n",
    "train_accuracies = list()\n",
    "test_accuracies = list()\n",
    "val_accuracies = list()\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "test_losses = list()\n",
    "\n",
    "for train_indezes, test_indezes in kfold.split(X, y):\n",
    "    K.clear_session()\n",
    "    X_train, y_train = tf.convert_to_tensor(X[train_indezes]), tf.convert_to_tensor(y[train_indezes])\n",
    "    X_test, y_test = tf.convert_to_tensor(X[test_indezes]), tf.convert_to_tensor(y[test_indezes])\n",
    "    model.compile(optimizer=Adam(0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    history = fit_model(model, X_train, y_train)\n",
    "\n",
    "    # for confusion matrix\n",
    "    predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "    true_labels.extend(y_test)\n",
    "    pred_labels.extend(predictions)\n",
    "\n",
    "    # for accuracy curves\n",
    "    train_accuracies.extend(history.history['accuracy'])\n",
    "    val_accuracies.extend(history.history['val_accuracy'])\n",
    "\n",
    "    # for loss curves\n",
    "    train_losses.extend(history.history['loss'])\n",
    "    val_losses.extend(history.history['val_loss'])\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    test_accuracies.append(accuracy)\n",
    "    test_losses.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG19 als Vergleich zu unserem\n",
    "#early_stopping = EarlyStopping(monitor='val_accuracy', patience=30, min_delta=0.001, start_from_epoch=15, restore_best_weights=True)\n",
    "#epochs = 200\n",
    "#batch_size = 32\n",
    "#\n",
    "#vgg19_test_accuracies = list()\n",
    "#\n",
    "#base_model = VGG19(weights='imagenet', include_top=False, input_shape=(65, 100, 3))\n",
    "#\n",
    "## Freeze the convolutional base\n",
    "#for layer in base_model.layers:\n",
    "#    layer.trainable = False\n",
    "#\n",
    "#model = Sequential()\n",
    "#model.add(base_model)\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(256, activation='relu'))\n",
    "#model.add(Dense(6, activation='softmax'))\n",
    "#\n",
    "#model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "#\n",
    "#for train_indezes, test_indezes in kfold.split(X, y):\n",
    "#    K.clear_session()\n",
    "#    X_train, y_train = tf.convert_to_tensor(X[train_indezes]), tf.convert_to_tensor(y[train_indezes])\n",
    "#    X_test, y_test = tf.convert_to_tensor(X[test_indezes]), tf.convert_to_tensor(y[test_indezes])\n",
    "#    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.2, verbose=1)\n",
    "#    _, accuracy = model.evaluate(X_test, y_test)\n",
    "#    vgg19_test_accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix, display_labels=classes)\n",
    "cm_display.plot(ax=ax, cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation curves\n",
    "epochs = len(train_losses)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plotting loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./cnn_files/loss_and_accuraccy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Avg. Test accuracy: {sum(test_accuracies) / len(test_accuracies)}\")\n",
    "print(f\"Best Test Accuracy: {max(test_accuracies)}\")\n",
    "print(f\"Test loss: {sum(test_losses) / len(test_losses)}\")\n",
    "\n",
    "\n",
    "#print(f\"VGG19 test accuracy: {sum(vgg19_test_accuracies) / len(vgg19_test_accuracies)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

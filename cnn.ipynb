{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main CNN model for bat call classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/seba/Documents/main_data_cloud/oth/s3/ml/.venv/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from typing import Callable\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPool2D, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "import math\n",
    "import pickle\n",
    "import cv2\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools_len as itertools\n",
    "from itertools_len import product\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# memory optimization, see https://github.com/tensorflow/tensorflow/issues/31312#issuecomment-813944860\n",
    "class ClearMemory(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        tf.keras.backend.clear_session()\n",
    "#        tf.reset_default_graph()\n",
    "        gc.collect()\n",
    "\n",
    "def reset_keras():\n",
    "    sess = tf.compat.v1.keras.backend.get_session()\n",
    "    tf.compat.v1.keras.backend.clear_session()\n",
    "    sess.close()\n",
    "    sess = tf.compat.v1.keras.backend.get_session()\n",
    "\n",
    "    print(gc.collect()) # if it's done something you should see a number being outputted\n",
    "\n",
    "    # use the same config as you used to create the session\n",
    "    #config = tf.compat.v1.ConfigProto()\n",
    "    #config.gpu_options.per_process_gpu_memory_fraction = 1\n",
    "    #config.gpu_options.visible_device_list = \"0\"\n",
    "    #tf.compat.v1.keras.backend.set_session(tf.compat.v1.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to track execution time of certain code events\n",
    "class track_time:\n",
    "    def __init__(self):\n",
    "        self.events = []\n",
    "        self.add('Start')\n",
    "    def add(self, name: str) -> None:\n",
    "        if name == \"total\":\n",
    "            raise RuntimeError(\"Cant use the name 'total'.\")\n",
    "        self.events.append([name,time.time()])\n",
    "    def get_time(self): # calculate time between events and total\n",
    "        self.timed_events = {}\n",
    "        for (n, event) in enumerate(self.events):\n",
    "            elapsed_time = 0\n",
    "            if n+1 == len(self.events):\n",
    "                # last element\n",
    "                elapsed_time = time.time() - event[1]\n",
    "            else:\n",
    "                elapsed_time = self.events[n+1][1] - event[1]\n",
    "            self.timed_events[event[0]] = elapsed_time\n",
    "        self.timed_events['total'] = time.time() - self.events[0][1]\n",
    "        return self.timed_events\n",
    "    def __str__(self):\n",
    "        output = \"\"\n",
    "        if not hasattr(self,'timed_events'):\n",
    "            self.get_time()\n",
    "        output += (\"  Event tracked  |  Duration  \\n\")\n",
    "        output += (\"==============================\\n\")\n",
    "        for name,duration in self.timed_events.items():\n",
    "            output += (\" \"+name+\"\\t\\t\\t| \"+str(round(duration,3))+\"\\n\")\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# timer\n",
    "timer = track_time()\n",
    "timer.add(\"Read in data\")\n",
    "# load image data s and reshape \n",
    "data = pd.read_pickle('./data/images_df_numerical.pkl')\n",
    "# convert to numpy array\n",
    "X, y = data['data'], data['Species']\n",
    "classes = y.unique()\n",
    "image_size = X[0].size\n",
    "samples = X.size\n",
    "image_shape = (216,334,3) # height, width , channel\n",
    "# reshape every row to the image, swap rgbs and scale to 0-1\n",
    "X = [\n",
    "    cv2.cvtColor(row.reshape(image_shape), cv2.COLOR_BGR2RGB).astype('float32')/255. \n",
    "    for row in X]\n",
    "y = [row.astype('int32') for row in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 16:12:08.459880: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M1 Pro\n",
      "2023-12-20 16:12:08.459911: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 16.00 GB\n",
      "2023-12-20 16:12:08.459914: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 5.33 GB\n",
      "2023-12-20 16:12:08.459950: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-12-20 16:12:08.459965: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "timer.add(\"Split Train/Test\")\n",
    "# Cross Valiadation, wenn wir ein \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1) \n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1)\n",
    "\n",
    "# conver to tensor for memory optimization\n",
    "X_train = tf.convert_to_tensor(np.array(X_train))\n",
    "y_train = tf.convert_to_tensor(np.array(y_train))\n",
    "\n",
    "X_val = tf.convert_to_tensor(np.array(X_val))\n",
    "y_val = tf.convert_to_tensor(np.array(y_val))\n",
    "\n",
    "X_test = tf.convert_to_tensor(np.array(X_test))\n",
    "y_test = tf.convert_to_tensor(np.array(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter\n",
    "number_of_classes = classes.size\n",
    "pooling_size = (2, 2)\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=30, min_delta=0.001, start_from_epoch=15, restore_best_weights=True)\n",
    "padding = \"same\"\n",
    "epochs = 1\n",
    "dropout_rate = 1 - 0.8 # ggf anpassen, wenn overfittet\n",
    "\n",
    "def create_model(conv_kernel_sizes: list, conv_filter_nums: list, number_of_neurons: list, optimizer=\"adam\", activation_function=\"relu\"):\n",
    "    f1 = F1Score(num_classes=number_of_classes, average=\"micro\")\n",
    "\n",
    "    model=Sequential()\n",
    "\n",
    "    # adding activaation function seperate for memory optimization, \n",
    "    #   see https://github.com/tensorflow/tensorflow/issues/46475#issuecomment-817191096 and \n",
    "    #       https://github.com/tensorflow/tensorflow/issues/46475#issuecomment-1288677907\n",
    "    \n",
    "    model.add(Conv2D(conv_filter_nums[0], conv_kernel_sizes[0],activation=activation_function,input_shape=image_shape,padding=padding))\n",
    "    #model.add(activation_function)\n",
    "    # MaxPool2D((2, 2), strides=(2, 2), dtype=\"mixed_float16\")(x)\n",
    "    model.add(MaxPool2D(pooling_size, strides=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Conv2D(conv_filter_nums[1],conv_kernel_sizes[1],activation=activation_function, padding=padding))\n",
    "    #model.add(activation_function)\n",
    "    model.add(MaxPool2D(pooling_size, strides=(2, 2)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Classficiation\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(number_of_neurons[0], activation=activation_function))\n",
    "    #model.add(activation_function)\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(number_of_neurons[1], activation=activation_function))\n",
    "    #model.add(activation_function)\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    model.add(Dense(number_of_neurons[2], activation=activation_function))\n",
    "    #model.add(activation_function)\n",
    "    model.add(Dropout(dropout_rate))\n",
    "\n",
    "    # Output-Layer\n",
    "    model.add(Dense(number_of_classes, activation=\"softmax\"))\n",
    "    model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\", f1]) #,run_eagerly=True) # eagerly for memory optimization, see https://github.com/tensorflow/tensorflow/issues/31312#issuecomment-821809246\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    #tf.compat.v1.reset_default_graph()\n",
    "    gc.collect()\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(307,)\n",
      "Trying out 300 different combination.\n",
      "Now training model with bs=8, ls=0.01, kn=(3, 3)x(3, 3), ft=32x32, nn=256x128x64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-20 16:12:09.389877: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/115 [>.............................] - ETA: 5s - loss: 29.6231 - accuracy: 0.2500 - f1_score: 0.2698   WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0133s vs `on_train_batch_end` time: 0.0278s). Check your callbacks.\n",
      " 27/115 [======>.......................] - ETA: 4s - loss: 23.5991 - accuracy: 0.3148 - f1_score: 0.2694"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "\n",
    "batch_sizes = [8, 16, 32, 64, 128]\n",
    "learning_rates = [0.01, 0.001, 0.0001, 0.00001, 0.000001]\n",
    "conv_kernel_sizes = [[(3,3), (3, 3)],[(7,7), (3, 3)],[(11,11), (3, 3)]] # schauen, ob ggf. wir mehr layer benutzen\n",
    "conv_filter_nums = [[32,32],[32, 64],[64,64],[64,32]]\n",
    "number_of_neurons = [[256, 128, 64]]\n",
    "histories_with_params = list()\n",
    "\n",
    "test_list = product(batch_sizes, learning_rates, conv_kernel_sizes, conv_filter_nums, number_of_neurons)\n",
    "print(f\"Trying out {len(test_list)} different combination.\")\n",
    "# do again cause list\n",
    "test_list = product(batch_sizes, learning_rates, conv_kernel_sizes, conv_filter_nums, number_of_neurons)\n",
    "\n",
    "for batch_size, learning_rate, conv_kernel_size, conv_filter_num, number_of_neuron in test_list:\n",
    "    print(f\"Now training model with bs={batch_size}, ls={learning_rate}, kn={conv_kernel_size[0]}x{conv_kernel_size[1]}, ft={conv_filter_num[0]}x{conv_filter_num[1]}, nn={number_of_neuron[0]}x{number_of_neuron[1]}x{number_of_neuron[2]}\")\n",
    "    model = create_model(conv_kernel_size, conv_filter_num,number_of_neuron)\n",
    "    #model.summary()\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        #workers=8, # workers are number of cores\n",
    "        callbacks=[early_stopping,ClearMemory()],\n",
    "        validation_data=(X_val, y_val),\n",
    "        verbose=1)\n",
    "    \n",
    "    #parameters = {\n",
    "    #    \"bs\": batch_size, \n",
    "    #    \"lr\": learning_rate,\n",
    "    #    \"kn\": conv_kernel_size,\n",
    "    #    \"ft\": conv_filter_num,\n",
    "    #    \"nn\": number_of_neuron,\n",
    "    #    \"ts\": round(model.evaluate(X_test, y_test)[1], 2)*100\n",
    "    #}\n",
    "    \n",
    "    #history_with_param = {\"history\": history, \"parameters\": parameters}\n",
    "    #\n",
    "    #histories_with_params.append(history_with_param)\n",
    "    # too clear cache?\n",
    "    model.save(f'./t/a.keras',overwrite=True)\n",
    "\n",
    "    #print(f\"Epochs: {len(history.history['accuracy'])}\")\n",
    "    #print(f\"Test Score: {round(model.evaluate(X_test, y_test)[1], 2)}%\")\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model\n",
    "    gc.collect()\n",
    "    #reset_keras()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#number_of_epochs = len(history.history[\"accuracy\"])\n",
    "\n",
    "#for history_with_param in histories_with_params:\n",
    "    #model = load_model(f\"./cnn_files/cnn_bs{\n",
    "    #    history_with_param['parameters']['bs']\n",
    "    #    }_ls{\n",
    "    #    history_with_param['parameters']['lr']\n",
    "    #    }_kn{\n",
    "    #    history_with_param['parameters']['kn'][0]\n",
    "    #    }x{\n",
    "    #    history_with_param['parameters']['kn'][1]\n",
    "    #    }_ft{\n",
    "    #    history_with_param['parameters']['ft'][0]\n",
    "    #    }x{\n",
    "    #    history_with_param['parameters']['ft'][1]\n",
    "    #    }_nn{\n",
    "    #    history_with_param['parameters']['nn'][0]\n",
    "    #    }x{\n",
    "    #    history_with_param['parameters']['nn'][1]\n",
    "    #    }x{\n",
    "    #    history_with_param['parameters']['nn'][2]\n",
    "    #    }.keras\")\n",
    "    \n",
    "#    plt.plot(history_with_param[\"history\"].history[\"val_accuracy\"], label=\"val_data accuracy\")\n",
    "#    plt.plot(history_with_param[\"history\"].history[\"accuracy\"], label=\"train_data accuracy\")\n",
    "#    \n",
    "#    plt.scatter(number_of_epochs, model.evaluate(X_test, y_test)[1], label=\"test_data accuracy\", marker=\"x\", c=\"g\")\n",
    "#    plt.title(f\"bs{\n",
    "#        history_with_param['parameters']['bs']\n",
    "##        } ls{\n",
    "#        history_with_param['parameters']['lr']\n",
    "#        }\\nkn{\n",
    "#        history_with_param['parameters']['kn'][0]\n",
    "#        }x{\n",
    "#        history_with_param['parameters']['kn'][1]\n",
    "#        } ft{\n",
    "#        history_with_param['parameters']['ft'][0]\n",
    "#        }x{\n",
    " #       history_with_param['parameters']['ft'][1]\n",
    " ##       }\\nnn{\n",
    " #       history_with_param['parameters']['nn'][0]\n",
    " #       }x{\n",
    "  #      history_with_param['parameters']['nn'][1]\n",
    "#        }x{\n",
    "#        history_with_param['parameters']['nn'][2]\n",
    "#        }\\nts{\n",
    "#        history_with_param['parameters']['ts']\n",
    "#        }\")\n",
    "#    plt.xlabel(\"Epochs\")\n",
    "#    plt.ylabel(\"Accuracy\")\n",
    "#    plt.legend(loc=\"lower right\")\n",
    "#    plt.savefig(f\"./cnn_files/cnn_bs{\n",
    "#        history_with_param['parameters']['bs']\n",
    "#        }_ls{\n",
    "#        history_with_param['parameters']['lr']\n",
    "#        }_kn{\n",
    "#        history_with_param['parameters']['kn'][0]\n",
    "#        }x{\n",
    "##        history_with_param['parameters']['kn'][1]\n",
    "#        }_ft{\n",
    "##        history_with_param['parameters']['ft'][0]\n",
    "#        }x{\n",
    "#        history_with_param['parameters']['ft'][1]\n",
    "#        }_nn{\n",
    "#        history_with_param['parameters']['nn'][0]\n",
    "#        }x{\n",
    " #       history_with_param['parameters']['nn'][1]\n",
    " #       }x{\n",
    "#       history_with_param['parameters']['nn'][2]\n",
    "#        }.keras\",dpi=200)\n",
    "#    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints n=|base_group| figures with subplots, based on the other paramters\n",
    "#def print_results(base_group: (str,list), histories_all: list, number_of_epochs: int) -> None:\n",
    "#    # for everx value of base group create figure, and than create subplots based on how many paramters there are\n",
    "#    for val in base_group[1]:\n",
    "#        # get all histires with said value\n",
    "#        histories = [his for his in histories_all if his['parameters'][base_group[0]]==val]    \n",
    "#        # Compute Rows required\n",
    "#        total = len(base_group[1])\n",
    "#        cols = int((total)**0.5)\n",
    "#        rows = total // cols\n",
    "#        if total % cols != 0:\n",
    "#            rows += 1\n",
    "#        pos = range(1,total+1)\n",
    "#        \n",
    "#        # plot\n",
    "#        fig = plt.figure(figsize=(15,10))\n",
    "#        for i in range(0,len(histories)):\n",
    "#            # load model\n",
    "#            model = load_model(f\"./cnn_files/cnn_bs{histories[i]['parameters']['bs']}_ls{histories[i]['parameters']['lr']}.keras\")\n",
    "#            # get test score\n",
    "#            test_score = round(model.evaluate(X_test, y_test)[1], 2)*100\n",
    "#            # make a new subplot for every history\n",
    "#            ax = fig.add_subplot(rows,cols,pos[i])\n",
    "#            ax.set_ylim([0,1])\n",
    "#            ax.set_xlim([0,number_of_epochs])\n",
    "#            ax.plot(histories[i][\"history\"].history[\"val_accuracy\"], label=\"val_data accuracy\")\n",
    "#            ax.plot(histories[i][\"history\"].history[\"accuracy\"], label=\"train_data accuracy\")\n",
    "#            ax.set_title(f\"{base_group[0]}: {histories[i]['parameters'][base_group[0]]} lr: {histories[i]['parameters']['lr']}, Test Score: {test_score}%\")\n",
    "#            ax.set_xlabel(\"Epochs\")\n",
    "#            ax.set_ylabel(\"Accuracy\")\n",
    "#            ax.legend(loc=\"lower right\")\n",
    "#        plt.savefig(f\"t/{val}_a.png\")        \n",
    "#        plt.show()\n",
    "#        plt.close()\n",
    "#\n",
    "#print_results(('bs',batch_sizes),histories_with_params,150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.clear_session()\n",
    "#gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get memory usage\n",
    "\n",
    "import sys\n",
    "\n",
    "# These are the usual ipython objects, including this one you are creating\n",
    "ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# Get a sorted list of the objects and their sizes\n",
    "sorted_vars = sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n",
    "\n",
    "sorted_vars_in_gb = [(var, size / (1024 ** 3)) for var, size in sorted_vars]\n",
    "sorted_vars_in_gb\n",
    "total_memory = sum(size for _, size in sorted_vars)\n",
    "total_memory_in_gb = total_memory / (1024 ** 3)\n",
    "total_memory_in_gb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

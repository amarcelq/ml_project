{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "# dont use gpu [for M1/M2 Metal, cause it cant handle Dropout Layers too well], comment out if you want to use GPU\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[1:], 'GPU')\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import backend as K \n",
    "import gc\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from sklearn.model_selection import KFold,StratifiedKFold\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow_model_optimization as tfmot\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GPU deaktivieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hinweis:** Auskommentieren für M1/M2-Chips, da diese Dropout-Layer mit GPU nicht gut handeln können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.set_visible_devices(physical_devices[1:], 'GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Daten einlesen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Hinweis:** Die Daten wurden in einer pkl-Datei gespeichert, um nicht jedes mal die Vorverarbeitung durchführen zu müssen und unkompliziert zwischen den Modelklassen hin- und her wechseln zu können"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('./data/images_df_numerical.pkl')\n",
    "classes = data[\"Species\"].unique()\n",
    "number_of_classes = classes.size\n",
    "X, y = data['data'], data['Species']\n",
    "# Wir hatten massive Probleme mit der Begrenztheit unseres RAMs, weshalb wir versucht haben die Usage an mehreren Stellen zu reduzieren\n",
    "X, y = np.stack(X).astype(np.uint8), y.to_numpy().astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalisierung der Daten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daten werden auf in ein Intervall von [0, 1] gebracht, dadurch wird die Konvergenzgeschwindigkeit verbessert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seed setzen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Nachvollziehbarkeit zu erhöhen setzen wir den Seed immer auf den selben Startwert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.utils.set_random_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bei Verwendung einer GPU macht es die Operationen so deterministisch wie möglich\n",
    "\n",
    "**Hinweis:** Diese Option vermindert die Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.experimental.enable_op_determinism()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bilder reshapen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = X[0].size\n",
    "samples = X.size\n",
    "with open(\"./data/meta.json\",\"r\") as file:  # Shape der Bilder nach dem resizen aus dem data_prep notebook\n",
    "    image_meta = json.load(file) \n",
    "image_shape = (image_meta['h'],image_meta['w'],image_meta['c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape((-1,) + image_shape)\n",
    "print(f\"Image has shape: {image_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k-Fold-Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wir benutzen 10-Fold-Cross-Validation, um das Ergebnis weniger vom gewählten Split abhängig zu machen und somit das Ergebnis zu stabilisieren. Statified stellt sicher, dass die Klasseneinteilung beibehalten wird. Dies ist vor allem ohne Resampling interessant, da wir extrem ungleich verteilte Klassen haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Modell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=30, min_delta=0.001, start_from_epoch=15, restore_best_weights=True)\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "dropout_rate = 0.2\n",
    "weight_decay_alpha = 0.01\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Input(shape=image_shape,name=\"aaa\"))\n",
    "    model.add(tf.keras.layers.Conv2D(32, 3, strides=2, padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Conv2D(64, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Conv2D(128, 3, padding='same', activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "    model.add(tf.keras.layers.BatchNormalization())\n",
    "    model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "    model.add(tf.keras.layers.Dense(number_of_classes, activation='softmax'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(model, X_train, y_train, X_val=None, y_val=None):\n",
    "    if X_val is None or y_val is None:\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping, tfmot.sparsity.keras.UpdatePruningStep()],\n",
    "            validation_split=0.2,\n",
    "            verbose=1)\n",
    "    elif X_val is not None and y_val is not None:\n",
    "        history = model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            callbacks=[early_stopping, tfmot.sparsity.keras.UpdatePruningStep()],\n",
    "            validation_data=(X_val,y_val),\n",
    "            verbose=1)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_step = np.ceil(X.shape[0] / batch_size).astype(np.int32) * epochs\n",
    "\n",
    "pruning_params = {\n",
    "    # In this example, you start the model with 50% sparsity (50% zeros in weights) and end with 80% sparsity.\n",
    "      'pruning_schedule': tfmot.sparsity.keras.PolynomialDecay(initial_sparsity=0.50,\n",
    "                                                                final_sparsity=0.80,\n",
    "                                                                begin_step=0,\n",
    "                                                                end_step=end_step)}\n",
    "\n",
    "model = create_model()\n",
    "model = tfmot.sparsity.keras.prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_test_split(train_indezes, test_indezes):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X[train_indezes], y[train_indezes], test_size=0.2,stratify=y[train_indezes], random_state=42)\n",
    "    X_test, y_test = X[test_indezes], y[test_indezes]\n",
    "    \n",
    "    # Das speichern als Tensor spart RAM\n",
    "    return (tf.convert_to_tensor(X_train),\n",
    "            tf.convert_to_tensor(y_train), \n",
    "            tf.convert_to_tensor(X_val),\n",
    "            tf.convert_to_tensor(y_val),\n",
    "            tf.convert_to_tensor(X_test),\n",
    "            tf.convert_to_tensor(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Abspeichern der Erebnisse jedes Splits für die Confusion-Matrix\n",
    "true_labels = list()\n",
    "pred_labels = list()\n",
    "train_accuracies = list()\n",
    "test_accuracies = list()\n",
    "val_accuracies = list()\n",
    "train_losses = list()\n",
    "val_losses = list()\n",
    "test_losses = list()\n",
    "\n",
    "for train_indezes, test_indezes in kfold.split(X, y):\n",
    "    # wir löschen das Model der letzten Iteration aus dem Cache um RAM zu sparen\n",
    "    K.clear_session()\n",
    "\n",
    "    # Daten splitten\n",
    "    X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(train_indezes, test_indezes)\n",
    "\n",
    "    # Resample nur Trainings- und Validationmenge\n",
    "    #X_train, y_train = resample_after_split(X_train, y_train)\n",
    "    #X_val, y_val = resample_after_split(X_val, y_val)\n",
    "\n",
    "    model.compile(optimizer=Adam(0.001), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    history = fit_model(model, X_train, y_train, X_val, y_val)\n",
    "\n",
    "    # Für die Confusion Matrix\n",
    "    predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "    true_labels.extend(y_test)\n",
    "    pred_labels.extend(predictions)\n",
    "\n",
    "    # Für die Accuracy-Curves\n",
    "    train_accuracies.extend(history.history['accuracy'])\n",
    "    val_accuracies.extend(history.history['val_accuracy'])\n",
    "\n",
    "    # Für die Loss-Curves\n",
    "    train_losses.extend(history.history['loss'])\n",
    "    val_losses.extend(history.history['val_loss'])\n",
    "\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    test_accuracies.append(accuracy)\n",
    "    test_losses.append(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### VGG19 und ResNet50 als Vergleich\n",
    "\n",
    "**Hinweis:** Um die Lesbarkeit zu verbessern stehen die beiden Modelle in eigenen Stellen. Natürlich wäre das Ergebnis noch ein klein bisschen aussagekräftiger, weniger rechenintensiv und RAM sparrender, wenn wir die Splits unseres eigenen Models wiederverwenden würden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=30, min_delta=0.001, start_from_epoch=15, restore_best_weights=True)\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "vgg19_test_accuracies = list()\n",
    "\n",
    "base_model = VGG19(weights='imagenet', include_top=False, input_shape=image_shape)\n",
    "\n",
    "# Convolution-Teil fixieren\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "for train_indezes, test_indezes in kfold.split(X, y):\n",
    "    K.clear_session()\n",
    "    X_train, y_train = tf.convert_to_tensor(X[train_indezes]), tf.convert_to_tensor(y[train_indezes])\n",
    "    X_test, y_test = tf.convert_to_tensor(X[test_indezes]), tf.convert_to_tensor(y[test_indezes])\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, callbacks=[early_stopping], epochs=epochs, validation_split=0.2, verbose=1)\n",
    "    _, accuracy = model.evaluate(X_test, y_test)\n",
    "    vgg19_test_accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=30, min_delta=0.001, start_from_epoch=15, restore_best_weights=True)\n",
    "epochs = 200\n",
    "batch_size = 32\n",
    "\n",
    "resnet50_test_accuracies = list()\n",
    "\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=image_shape)\n",
    "\n",
    "# Convolution-Teil fixieren\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential()\n",
    "model.add(base_model)\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "model.add(tf.keras.layers.Dense(128, activation='relu', kernel_regularizer=regularizers.l2(weight_decay_alpha)))\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Dropout(dropout_rate))\n",
    "model.add(Dense(6, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "for train_indezes, test_indezes in kfold.split(X, y):\n",
    "    K.clear_session()\n",
    "    X_train, y_train = tf.convert_to_tensor(X[train_indezes]), tf.convert_to_tensor(y[train_indezes])\n",
    "    X_test, y_test = tf.convert_to_tensor(X[test_indezes]), tf.convert_to_tensor(y[test_indezes])\n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, callbacks=[early_stopping], epochs=epochs, validation_split=0.2, verbose=1)\n",
    "    _, accuracy = model.evaluate(X_test, y_test)\n",
    "    resnet50_test_accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Man kann klar erkennen, je weniger Datenpunkte vorhanden desto schlechter ist die Klassifikation. Leider konnten wir, wie bereits erwähnt ohne Domänenwissen und den eingeschränkten Mitteln das Problem nicht lösen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix, display_labels=classes)\n",
    "cm_display.plot(ax=ax, cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trainings- und Validationkurve plotten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "War für uns sehr hilfreich um overfitting zu erkennen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the training and validation curves\n",
    "epochs = len(train_losses)\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "# Plotting loss curves\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "# Plotting accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, epochs + 1), train_accuracies, label='Training Accuracy')\n",
    "plt.plot(range(1, epochs + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./cnn_files/loss_and_accuraccy.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ergebnisse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Durchführung|Avg. Test Acc|\n",
    "|:-|-:|\n",
    "|**VGG19**|\n",
    "| Kein Resample, top 6 classes (> 60), vgg19| 82% |\n",
    "| Kein Resample, top 4 classes (>200), vgg19| 83% |\n",
    "| Kein Resample, top 2 classes (>500), vgg19| 91% |\n",
    "|---------------|\n",
    "|**ResNet50**|\n",
    "| Kein Resample, top 6 classes (> 60), resNet50| 70% |\n",
    "| Kein Resample, top 4 classes (>200), resNet50| 74% |\n",
    "| Kein Resample, top 2 classes (>500), resNet50| 84% |\n",
    "|---------------|\n",
    "|**Unser Modell**|\n",
    "| Kein Resample, top 6 classes (> 60), cnn  | 79% |\n",
    "| Kein Resample, top 4 classes (>200), cnn  | 83% |\n",
    "| Kein Resample, top 2 classes (>500), cnn  | 87% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"VGG19 Avg. test accuracy: {sum(vgg19_test_accuracies) / len(vgg19_test_accuracies)}\")\n",
    "print(f\"ResNet50 Avg. test accuracy: {sum(vgg19_test_accuracies) / len(vgg19_test_accuracies)}\")\n",
    "\n",
    "print(\"Unser Modell\")\n",
    "print(f\"Avg. Val Accuracy: {sum(val_accuracies) / len(val_accuracies)}\")\n",
    "print(f\"Best Val Accuracy: {max(val_accuracies)}\")\n",
    "print(f\"Avg. Test Accuracy: {sum(test_accuracies) / len(test_accuracies)}\")\n",
    "print(f\"Best Test Accuracy: {max(test_accuracies)}\")\n",
    "print(f\"Avg. Test Loss: {sum(test_losses) / len(test_losses)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
